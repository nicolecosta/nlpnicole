{"title": "Introduction to Python and Jupyter notebooks", "content": "If you think quantum mechanics sounds challenging, you are not alone. All of our intuitions are based on day-to-day experiences, and so are better at understanding the behavior of balls and bananas than atoms or electrons. Though quantum objects can seem random and chaotic at first, they just follow a different set of rules. Once we know what those rules are, we can use them to create new and powerful technology. Quantum computing will be the most revolutionary example of this. To get you started on your journey towards quantum computing, let's test what you already know.   Which of the following is the correct description of a bit? Actually, they are all correct: it's a very multi-purpose word! But if you chose \"the smallest unit of information\", it shows that you are already thinking along the right lines. The idea that information can be stored and processed as a series of 0s and 1s is quite a big conceptual hurdle, but it's something most people today know without even thinking about it. Taking this as a starting point, we can start to imagine bits that obey the rules of quantum mechanics. These quantum bits, or qubits, will then allow us to process information in new and different ways. We'll start diving deeper into the world of qubits. For this, we'll need some way of keeping track of what they are doing when we apply gates. The most powerful way to do this is to use the mathematical language of vectors and matrices. This chapter will be most effective for readers who are already familiar with vectors and matrices. Those who aren't familiar will likely be fine too, though it might be useful to consult our Introduction to Linear Algebra for Quantum Computing from time to time. Since we will be using Qiskit, our Python-based framework for quantum computing, it would also be useful to know the basics of Python. Those who need a primer can consult the Introduction to Python and Jupyter notebooks. If you think quantum mechanics sounds challenging, you are not alone. All of our intuitions are based on day-to-day experiences, and so are better at understanding the behavior of balls and bananas than atoms or electrons. Though quantum objects can seem random and chaotic at first, they just follow a different set of rules. Once we know what those rules are, we can use them to create new and powerful technology. Quantum computing will be the most revolutionary example of this. To get you started on your journey towards quantum computing, let's test what you already know.   Which of the following is the correct description of a bit? Actually, they are all correct: it's a very multi-purpose word! But if you chose \"the smallest unit of information\", it shows that you are already thinking along the right lines. The idea that information can be stored and processed as a series of 0s and 1s is quite a big conceptual hurdle, but it's something most people today know without even thinking about it. Taking this as a starting point, we can start to imagine bits that obey the rules of quantum mechanics. These quantum bits, or qubits, will then allow us to process information in new and different ways. We'll start diving deeper into the world of qubits. For this, we'll need some way of keeping track of what they are doing when we apply gates. The most powerful way to do this is to use the mathematical language of vectors and matrices. This chapter will be most effective for readers who are already familiar with vectors and matrices. Those who aren't familiar will likely be fine too, though it might be useful to consult our Introduction to Linear Algebra for Quantum Computing from time to time. Since we will be using Qiskit, our Python-based framework for quantum computing, it would also be useful to know the basics of Python. Those who need a primer can consult the Introduction to Python and Jupyter notebooks. Programming a quantum computer is now something that anyone can do in the comfort of their own home. But what to create? What is a quantum program anyway? In fact, what is a quantum computer? These questions can be answered by making comparisons to standard digital computers. Unfortunately, most people don\u2019t actually understand how digital computers work either. In this article, we\u2019ll look at the basics principles behind these devices. To help us transition over to quantum computing later on, we\u2019ll do it using the same tools as we'll use for quantum. Below is some Python code we'll need to run if we want to use the code in this page:   The first thing we need to know about is the idea of bits. These are designed to be the world\u2019s simplest alphabet. With only two characters, 0 and 1, we can represent any piece of information. One example is numbers. You are probably used to representing a number through a string of the ten digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In this string of digits, each digit represents how many times the number contains a certain power of ten. For example, when we write 9213, we mean \\begin{align*} 9000 + 200 + 10 + 3 \\end{align*} or, expressed in a way that emphasizes the powers of ten \\begin{align*} (9\\times10^3) + (2\\times10^2) + (1\\times10^1) + (3\\times10^0) \\end{align*} Though we usually use this system based on the number 10, we can just as easily use one based on any other number. The binary number system, for example, is based on the number two. This means using the two characters 0 and 1 to express numbers as multiples of powers of two. For example, 9213 becomes 10001111111101, since \\begin{align*}\\begin{aligned} 9213 &= (1 \\times 2^{13}) + (0 \\times 2^{12}) + (0 \\times 2^{11}) + (0 \\times 2^{10}) \\\\ &+ (1 \\times 2^9) + (1 \\times 2^8) + (1 \\times 2^7) + (1 \\times 2^6) \\\\ &+ (1 \\times 2^5) + (1 \\times 2^4) + (1 \\times 2^3) + (1 \\times 2^2) \\\\ &+ (0 \\times 2^1) + (1 \\times 2^0) \\end{aligned}\\end{align*} In this we are expressing numbers as multiples of 2, 4, 8, 16, 32, etc. instead of 10, 100, 1000, etc. These strings of bits, known as binary strings, can be used to represent more than just numbers. For example, there is a way to represent any text using bits. For any letter, number, or punctuation mark you want to use, you can find a corresponding string of at most eight bits using this table. Though these are quite arbitrary, this is a widely agreed-upon standard. In fact, it's what was used to transmit this article to you through the internet. This is how all information is represented in computers. Whether numbers, letters, images, or sound, it all exists in the form of binary strings. Like our standard digital computers, quantum computers are based on this same basic idea. The main difference is that they use qubits, an extension of the bit to quantum mechanics. In the rest of this textbook, we will explore what qubits are, what they can do, and how they do it. In this section, however, we are not talking about quantum at all. So, we just use qubits as if they were bits. Complete these sentences:   Whether we are using qubits or bits, we need to manipulate them in order to turn the inputs we have into the outputs we need. For the simplest programs with very few bits, it is useful to represent this process in a diagram known as a circuit diagram. These have inputs on the left, outputs on the right, and operations represented by arcane symbols in between. These operations are called 'gates', mostly for historical reasons. Here's an example of what a circuit looks like for standard, bit-based computers. You aren't expected to understand what it does. It should simply give you an idea of what these circuits look like. For quantum computers, we use the same basic idea but have different conventions for how to represent inputs, outputs, and the symbols used for operations. Here is the quantum circuit that represents the same process as above. In the rest of this section, we will explain how to build circuits. At the end, you'll know how to create the circuit above, what it does, and why it is useful.   In a circuit, we typically need to do three jobs: First, encode the input, then do some actual computation, and finally extract an output. For your first quantum circuit, we'll focus on the last of these jobs. We start by creating a circuit with eight qubits and eight outputs. This circuit, which we have called qc_output, is created by Qiskit using QuantumCircuit. The QuantumCircuit takes the number of qubits in the quantum circuit as an argument.  The extraction of outputs in a quantum circuit is done using an operation called measure_all(). Each measurement tells a specific qubit to give an output to a specific output bit. The command qc_output.measure_all() adds a measurement to each qubit in the circuit qc_output, and also adds some classical bits to write the output to. Now that our circuit has something in it, let's take a look at it. Qubits are always initialized to give the output 0. Since we don't do anything to our qubits in the circuit above, this is exactly the result we'll get when we measure them. We can see this by running the circuit many times and plotting the results in a histogram. We will find that the result is always 0: a 0 from each qubit. The reason for running many times and showing the result as a histogram is because quantum computers may have some randomness in their results. In this case, since we aren\u2019t doing anything quantum, we get just the 0 result with certainty. Note that this result comes from a quantum simulator, which is a standard computer calculating what an ideal quantum computer would do. Simulations are only possible for small numbers of qubits (~30 qubits), but they are nevertheless a very useful tool when designing your first quantum circuits. To run on a real device you simply need to replace Aer.getbackend\u2019aersimulator\u2019 with the backend object of the device you want to use.        Now let's look at how to encode a different binary string as an input. For this, we need what is known as a NOT gate. This is the most basic operation that you can do in a computer. It simply flips the bit value: 0 becomes 1 and 1 becomes 0. For qubits, it is an operation called x that does the job of the NOT. Below we create a new circuit dedicated to the job of encoding and call it qc_encode. For now, we only specify the number of qubits. Extracting results can be done using the circuit we have from before: qc_output. Now we can run the combined circuit and look at the results. Now our computer outputs the string 10000000 instead. The bit we flipped, which comes from qubit 7, lives on the far left of the string. This is because Qiskit numbers the bits in a string from right to left. Some prefer to number their bits the other way around, but Qiskit's system certainly has its advantages when we are using the bits to represent numbers. Specifically, it means that qubit 7 is telling us about how many 2^7s we have in our number. So by flipping this bit, we\u2019ve now written the number 1282566432 in our simple 8-bit computer. Now try out writing another number for yourself. You could do your age, for example. Just use a search engine to find out what the number looks like in binary (if it includes a \u20180b\u2019, just ignore it), and then add some 0s to the left side if you are younger than 128. Now we know how to encode information in a computer. The next step is to process it: To take an input that we have encoded, and turn it into an output that we need.   To look at turning inputs into outputs, we need a problem to solve. Let\u2019s do some basic maths. In primary school, you will have learned how to take large mathematical problems and break them down into manageable pieces. For example, how would you go about solving the following? One way is to do it digit by digit, from right to left. So we start with 3+4 And then 1+5 Then we have 2+8=10. Since this is a two digit answer, we need to carry the one over to the next column. Finally we have 9+1+1=11, and get our answer This may just be simple addition, but it demonstrates the principles behind all algorithms. Whether the algorithm is designed to solve mathematical problems or process text or images, we always break big tasks down into small and simple steps. To run on a computer, algorithms need to be compiled down to the smallest and simplest steps possible. To see what these look like, let\u2019s do the above addition problem again but in binary. Note that the second number has a bunch of extra 0s on the left. This just serves to make the two strings the same length. Our first task is to do the 1+0 for the column on the right. In binary, as in any number system, the answer is 1. We get the same result for the 0+1 of the second column. Next, we have 1+1. As you\u2019ll surely be aware, 1+1=2. In binary, the number 2 is written 10, and so requires two bits. This means that we need to carry the 1, just as we would for the number 10 in decimal. The next column now requires us to calculate 1+1+1. This means adding three numbers together, so things are getting complicated for our computer. But we can still compile it down to simpler operations, and do it in a way that only ever requires us to add two bits together. For this, we can start with just the first two 1s. Now we need to add this 10 to the final 1 , which can be done using our usual method of going through the columns. The final answer is 11 (also known as 3). Now we can get back to the rest of the problem. With the answer of 11, we have another carry bit. So now we have another 1+1+1 to do. But we already know how to do that, so it\u2019s not a big deal. In fact, everything left so far is something we already know how to do. This is because, if you break everything down into adding just two bits, there are only four possible things you\u2019ll ever need to calculate. Here are the four basic sums (we\u2019ll write all the answers with two bits to be consistent). This is called a half adder. If our computer can implement this, and if it can chain many of them together, it can add anything.   Let's make our own half adder using Qiskit. This will include a part of the circuit that encodes the input, a part that executes the algorithm, and a part that extracts the result. The first part will need to be changed whenever we want to use a new input, but the rest will always remain the same. The two bits we want to add are encoded in the qubits 0 and 1. The above example encodes a 1 in both these qubits, and so it seeks to find the solution of 1+1. The result will be a string of two bits, which we will read out from the qubits 2 and 3 and store in classical bits 0 and 1, respectively. The basic operations of computing are known as logic gates. We\u2019ve already used the NOT gate, but this is not enough to make our half adder. We could only use it to manually write out the answers. Since we want the computer to do the actual computing for us, we\u2019ll need some more powerful gates. To see what we need, let\u2019s take another look at what our half adder needs to do. The rightmost bit in all four of these answers is completely determined by whether the two bits we are adding are the same or different. So for 0+0 and 1+1, where the two bits are equal, the rightmost bit of the answer comes out 01. For 0+1 and 1+0, where we are adding different bit values, the rightmost bit is 10. To get this part of our solution correct, we need something that can figure out whether two bits are different or not. Traditionally, in the study of digital computation, this is called an XOR gate. In quantum computers, the job of the XOR gate is done by the controlled-NOT gate. Since that's quite a long name, we usually just call it the CNOT. In Qiskit its name is cx, which is even shorter. In circuit diagrams, it is drawn as in the image below. This is applied to a pair of qubits. One acts as the control qubit (this is the one with the little dot). The other acts as the target qubit (with the big circle that has a + inside it). There are multiple ways to explain the effect of the CNOT. One is to say that it looks at its two input bits to see whether they are the same or different. Next, it overwrites the target qubit with the answer. The target becomes 0 if they are the same, and 1 if they are different. Another way of explaining the CNOT is to say that it does a NOT on the target if the control is 1, and does nothing otherwise. This explanation is just as valid as the previous one (in fact, it\u2019s the one that gives the gate its name). Try the CNOT out for yourself by trying each of the possible inputs. For example, here's a circuit that tests the CNOT with the input 1. If you execute this circuit, you\u2019ll find that the output is 11. We can think of this happening because of either of the following reasons. The CNOT calculates whether the input values are different and finds that they are, which means that it wants to output 1. It does this by writing over the state of qubit 1 (which, remember, is on the left of the bit string), turning 1 into 11. The CNOT sees that qubit 0 is in state 1, and so applies a NOT to qubit 1. This flips the 0 of qubit 1 into a 1, and so turns 1 into 11. Here is a table showing all the possible inputs and corresponding outputs of the CNOT gate: For our half adder, we don\u2019t want to overwrite one of our inputs. Instead, we want to write the result on a different pair of qubits. For this, we can use two CNOTs. We are now halfway to a fully working half adder. We just have the other bit of the output left to do: the one that will live on qubit 3. If you look again at the four possible sums, you\u2019ll notice that there is only one case for which this is 1 instead of 0: 1+1=10. It happens only when both the bits we are adding are 1. To calculate this part of the output, we could just get our computer to look at whether both of the inputs are 1. If they are\u200a\u2014\u200aand only if they are\u200a\u2014\u200awe need to do a NOT gate on qubit 3. That will flip it to the required value of 1 for this case only, giving us the output we need. For this, we need a new gate: like a CNOT but controlled on two qubits instead of just one. This will perform a NOT on the target qubit only when both controls are in state 1. This new gate is called the Toffoli. For those of you who are familiar with Boolean logic gates, it is basically an AND gate. In Qiskit, the Toffoli is represented with the ccx command. In this example, we are calculating 1+1, because the two input bits are both 1. Let's see what we get. The result is 10, which is the binary representation of the number 2. We have built a computer that can solve the famous mathematical problem of 1+1! Now you can try it out with the other three possible inputs, and show that our algorithm gives the right results for those too. The half adder contains everything you need for addition. With the NOT, CNOT, and Toffoli gates, we can create programs that add any set of numbers of any size. These three gates are enough to do everything else in computing too. In fact, we can even do without the CNOT. Additionally, the NOT gate is only really needed to create bits with value 1. The Toffoli gate is essentially the atom of mathematics. It is the simplest element, from which every other problem-solving technique can be compiled. As we'll see, in quantum computing we split the atom. Programming a quantum computer is now something that anyone can do in the comfort of their own home. But what to create? What is a quantum program anyway? In fact, what is a quantum computer? These questions can be answered by making comparisons to standard digital computers. Unfortunately, most people don\u2019t actually understand how digital computers work either. In this article, we\u2019ll look at the basics principles behind these devices. To help us transition over to quantum computing later on, we\u2019ll do it using the same tools as we'll use for quantum. Below is some Python code we'll need to run if we want to use the code in this page:   The first thing we need to know about is the idea of bits. These are designed to be the world\u2019s simplest alphabet. With only two characters, 0 and 1, we can represent any piece of information. One example is numbers. You are probably used to representing a number through a string of the ten digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In this string of digits, each digit represents how many times the number contains a certain power of ten. For example, when we write 9213, we mean \\begin{align*} 9000 + 200 + 10 + 3 \\end{align*} or, expressed in a way that emphasizes the powers of ten \\begin{align*} (9\\times10^3) + (2\\times10^2) + (1\\times10^1) + (3\\times10^0) \\end{align*} Though we usually use this system based on the number 10, we can just as easily use one based on any other number. The binary number system, for example, is based on the number two. This means using the two characters 0 and 1 to express numbers as multiples of powers of two. For example, 9213 becomes 10001111111101, since \\begin{align*}\\begin{aligned} 9213 &= (1 \\times 2^{13}) + (0 \\times 2^{12}) + (0 \\times 2^{11}) + (0 \\times 2^{10}) \\\\ &+ (1 \\times 2^9) + (1 \\times 2^8) + (1 \\times 2^7) + (1 \\times 2^6) \\\\ &+ (1 \\times 2^5) + (1 \\times 2^4) + (1 \\times 2^3) + (1 \\times 2^2) \\\\ &+ (0 \\times 2^1) + (1 \\times 2^0) \\end{aligned}\\end{align*} In this we are expressing numbers as multiples of 2, 4, 8, 16, 32, etc. instead of 10, 100, 1000, etc. These strings of bits, known as binary strings, can be used to represent more than just numbers. For example, there is a way to represent any text using bits. For any letter, number, or punctuation mark you want to use, you can find a corresponding string of at most eight bits using this table. Though these are quite arbitrary, this is a widely agreed-upon standard. In fact, it's what was used to transmit this article to you through the internet. This is how all information is represented in computers. Whether numbers, letters, images, or sound, it all exists in the form of binary strings. Like our standard digital computers, quantum computers are based on this same basic idea. The main difference is that they use qubits, an extension of the bit to quantum mechanics. In the rest of this textbook, we will explore what qubits are, what they can do, and how they do it. In this section, however, we are not talking about quantum at all. So, we just use qubits as if they were bits. Complete these sentences:   Whether we are using qubits or bits, we need to manipulate them in order to turn the inputs we have into the outputs we need. For the simplest programs with very few bits, it is useful to represent this process in a diagram known as a circuit diagram. These have inputs on the left, outputs on the right, and operations represented by arcane symbols in between. These operations are called 'gates', mostly for historical reasons. Here's an example of what a circuit looks like for standard, bit-based computers. You aren't expected to understand what it does. It should simply give you an idea of what these circuits look like. For quantum computers, we use the same basic idea but have different conventions for how to represent inputs, outputs, and the symbols used for operations. Here is the quantum circuit that represents the same process as above. In the rest of this section, we will explain how to build circuits. At the end, you'll know how to create the circuit above, what it does, and why it is useful.   In a circuit, we typically need to do three jobs: First, encode the input, then do some actual computation, and finally extract an output. For your first quantum circuit, we'll focus on the last of these jobs. We start by creating a circuit with eight qubits and eight outputs. This circuit, which we have called qc_output, is created by Qiskit using QuantumCircuit. The QuantumCircuit takes the number of qubits in the quantum circuit as an argument.  The extraction of outputs in a quantum circuit is done using an operation called measure_all(). Each measurement tells a specific qubit to give an output to a specific output bit. The command qc_output.measure_all() adds a measurement to each qubit in the circuit qc_output, and also adds some classical bits to write the output to. Now that our circuit has something in it, let's take a look at it. Qubits are always initialized to give the output 0. Since we don't do anything to our qubits in the circuit above, this is exactly the result we'll get when we measure them. We can see this by running the circuit many times and plotting the results in a histogram. We will find that the result is always 0: a 0 from each qubit. The reason for running many times and showing the result as a histogram is because quantum computers may have some randomness in their results. In this case, since we aren\u2019t doing anything quantum, we get just the 0 result with certainty. Note that this result comes from a quantum simulator, which is a standard computer calculating what an ideal quantum computer would do. Simulations are only possible for small numbers of qubits (~30 qubits), but they are nevertheless a very useful tool when designing your first quantum circuits. To run on a real device you simply need to replace Aer.getbackend\u2019aersimulator\u2019 with the backend object of the device you want to use.        Now let's look at how to encode a different binary string as an input. For this, we need what is known as a NOT gate. This is the most basic operation that you can do in a computer. It simply flips the bit value: 0 becomes 1 and 1 becomes 0. For qubits, it is an operation called x that does the job of the NOT. Below we create a new circuit dedicated to the job of encoding and call it qc_encode. For now, we only specify the number of qubits. Extracting results can be done using the circuit we have from before: qc_output. Now we can run the combined circuit and look at the results. Now our computer outputs the string 10000000 instead. The bit we flipped, which comes from qubit 7, lives on the far left of the string. This is because Qiskit numbers the bits in a string from right to left. Some prefer to number their bits the other way around, but Qiskit's system certainly has its advantages when we are using the bits to represent numbers. Specifically, it means that qubit 7 is telling us about how many 2^7s we have in our number. So by flipping this bit, we\u2019ve now written the number 1282566432 in our simple 8-bit computer. Now try out writing another number for yourself. You could do your age, for example. Just use a search engine to find out what the number looks like in binary (if it includes a \u20180b\u2019, just ignore it), and then add some 0s to the left side if you are younger than 128. Now we know how to encode information in a computer. The next step is to process it: To take an input that we have encoded, and turn it into an output that we need.   To look at turning inputs into outputs, we need a problem to solve. Let\u2019s do some basic maths. In primary school, you will have learned how to take large mathematical problems and break them down into manageable pieces. For example, how would you go about solving the following? One way is to do it digit by digit, from right to left. So we start with 3+4 And then 1+5 Then we have 2+8=10. Since this is a two digit answer, we need to carry the one over to the next column. Finally we have 9+1+1=11, and get our answer This may just be simple addition, but it demonstrates the principles behind all algorithms. Whether the algorithm is designed to solve mathematical problems or process text or images, we always break big tasks down into small and simple steps. To run on a computer, algorithms need to be compiled down to the smallest and simplest steps possible. To see what these look like, let\u2019s do the above addition problem again but in binary. Note that the second number has a bunch of extra 0s on the left. This just serves to make the two strings the same length. Our first task is to do the 1+0 for the column on the right. In binary, as in any number system, the answer is 1. We get the same result for the 0+1 of the second column. Next, we have 1+1. As you\u2019ll surely be aware, 1+1=2. In binary, the number 2 is written 10, and so requires two bits. This means that we need to carry the 1, just as we would for the number 10 in decimal. The next column now requires us to calculate 1+1+1. This means adding three numbers together, so things are getting complicated for our computer. But we can still compile it down to simpler operations, and do it in a way that only ever requires us to add two bits together. For this, we can start with just the first two 1s. Now we need to add this 10 to the final 1 , which can be done using our usual method of going through the columns. The final answer is 11 (also known as 3). Now we can get back to the rest of the problem. With the answer of 11, we have another carry bit. So now we have another 1+1+1 to do. But we already know how to do that, so it\u2019s not a big deal. In fact, everything left so far is something we already know how to do. This is because, if you break everything down into adding just two bits, there are only four possible things you\u2019ll ever need to calculate. Here are the four basic sums (we\u2019ll write all the answers with two bits to be consistent). This is called a half adder. If our computer can implement this, and if it can chain many of them together, it can add anything.   Let's make our own half adder using Qiskit. This will include a part of the circuit that encodes the input, a part that executes the algorithm, and a part that extracts the result. The first part will need to be changed whenever we want to use a new input, but the rest will always remain the same. The two bits we want to add are encoded in the qubits 0 and 1. The above example encodes a 1 in both these qubits, and so it seeks to find the solution of 1+1. The result will be a string of two bits, which we will read out from the qubits 2 and 3 and store in classical bits 0 and 1, respectively. The basic operations of computing are known as logic gates. We\u2019ve already used the NOT gate, but this is not enough to make our half adder. We could only use it to manually write out the answers. Since we want the computer to do the actual computing for us, we\u2019ll need some more powerful gates. To see what we need, let\u2019s take another look at what our half adder needs to do. The rightmost bit in all four of these answers is completely determined by whether the two bits we are adding are the same or different. So for 0+0 and 1+1, where the two bits are equal, the rightmost bit of the answer comes out 01. For 0+1 and 1+0, where we are adding different bit values, the rightmost bit is 10. To get this part of our solution correct, we need something that can figure out whether two bits are different or not. Traditionally, in the study of digital computation, this is called an XOR gate. In quantum computers, the job of the XOR gate is done by the controlled-NOT gate. Since that's quite a long name, we usually just call it the CNOT. In Qiskit its name is cx, which is even shorter. In circuit diagrams, it is drawn as in the image below. This is applied to a pair of qubits. One acts as the control qubit (this is the one with the little dot). The other acts as the target qubit (with the big circle that has a + inside it). There are multiple ways to explain the effect of the CNOT. One is to say that it looks at its two input bits to see whether they are the same or different. Next, it overwrites the target qubit with the answer. The target becomes 0 if they are the same, and 1 if they are different. Another way of explaining the CNOT is to say that it does a NOT on the target if the control is 1, and does nothing otherwise. This explanation is just as valid as the previous one (in fact, it\u2019s the one that gives the gate its name). Try the CNOT out for yourself by trying each of the possible inputs. For example, here's a circuit that tests the CNOT with the input 1. If you execute this circuit, you\u2019ll find that the output is 11. We can think of this happening because of either of the following reasons. The CNOT calculates whether the input values are different and finds that they are, which means that it wants to output 1. It does this by writing over the state of qubit 1 (which, remember, is on the left of the bit string), turning 1 into 11. The CNOT sees that qubit 0 is in state 1, and so applies a NOT to qubit 1. This flips the 0 of qubit 1 into a 1, and so turns 1 into 11. Here is a table showing all the possible inputs and corresponding outputs of the CNOT gate: For our half adder, we don\u2019t want to overwrite one of our inputs. Instead, we want to write the result on a different pair of qubits. For this, we can use two CNOTs. We are now halfway to a fully working half adder. We just have the other bit of the output left to do: the one that will live on qubit 3. If you look again at the four possible sums, you\u2019ll notice that there is only one case for which this is 1 instead of 0: 1+1=10. It happens only when both the bits we are adding are 1. To calculate this part of the output, we could just get our computer to look at whether both of the inputs are 1. If they are\u200a\u2014\u200aand only if they are\u200a\u2014\u200awe need to do a NOT gate on qubit 3. That will flip it to the required value of 1 for this case only, giving us the output we need. For this, we need a new gate: like a CNOT but controlled on two qubits instead of just one. This will perform a NOT on the target qubit only when both controls are in state 1. This new gate is called the Toffoli. For those of you who are familiar with Boolean logic gates, it is basically an AND gate. In Qiskit, the Toffoli is represented with the ccx command. In this example, we are calculating 1+1, because the two input bits are both 1. Let's see what we get. The result is 10, which is the binary representation of the number 2. We have built a computer that can solve the famous mathematical problem of 1+1! Now you can try it out with the other three possible inputs, and show that our algorithm gives the right results for those too. The half adder contains everything you need for addition. With the NOT, CNOT, and Toffoli gates, we can create programs that add any set of numbers of any size. These three gates are enough to do everything else in computing too. In fact, we can even do without the CNOT. Additionally, the NOT gate is only really needed to create bits with value 1. The Toffoli gate is essentially the atom of mathematics. It is the simplest element, from which every other problem-solving technique can be compiled. As we'll see, in quantum computing we split the atom. You now know something about bits, and about how our familiar digital computers work. All the complex variables, objects and data structures used in modern software are basically all just big piles of bits. Those of us who work on quantum computing call these classical variables. The computers that use them, like the one you are using to read this article, we call classical computers. In quantum computers, our basic variable is the qubit: a quantum variant of the bit. These have exactly the same restrictions as normal bits do: they can store only a single binary piece of information, and can only ever give us an output of 0 or 1-1 or 101-1. However, they can also be manipulated in ways that can only be described by quantum mechanics. This gives us new gates to play with, allowing us to find new ways to design algorithms. To fully understand these new gates, we first need to understand how to write down qubit states. For this we will use the mathematics of vectors, matrices, and complex numbers. Though we will introduce these concepts as we go, it would be best if you are comfortable with them already. If you need a more in-depth explanation or a refresher, you can find the guide here.     In quantum physics we use statevectors to describe the state of our system. Say we wanted to describe the position of a car along a track, this is a classical system so we could use a number x: \\begin{align*} x=4 \\end{align*} Alternatively, we could instead use a collection of numbers in a vector called a statevector. Each element in the statevector contains the probability of finding the car in a certain place: \\begin{align*} \\cssId{x_ket}{|x\\rangle} = \\cssId{vector}{\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}} \\begin{matrix} \\\\ \\\\ \\\\ \\leftarrow \\\\ \\\\ \\\\ \\\\ \\end{matrix} \\begin{matrix} \\\\ \\\\ \\text{Probability of} \\\\ \\text{car being at} \\\\ \\text{position 4} \\\\ \\\\ \\\\ \\end{matrix} \\end{align*} This isn\u2019t limited to position, we could also keep a statevector of all the possible speeds the car could have, and all the possible colours the car could be. With classical systems (like the car example above), this is a silly thing to do as it requires keeping huge vectors when we only really need one number. But as we will see in this chapter, statevectors happen to be a very good way of keeping track of quantum systems, including quantum computers.   Classical bits are always either 0 or 1 at every point during a computation. There is no more detail we can add to the state of a bit than this. So to write down the state of a of classical bit (c), we can just use these two binary values. For example: \\begin{align*} c = 0 \\end{align*} This restriction is lifted for quantum bits. Whether we get a 0 or a 1 from a qubit only needs to be well-defined when a measurement is made to extract an output. At that point, it must commit to one of these two options. At all other times, its state will be something more complex than can be captured by a simple binary value. To see how to describe these, we can first focus on the two simplest cases. As we saw in the last section, it is possible to prepare a qubit in a state for which it definitely gives the outcome 0 when measured. We need a name for this state. Let's be unimaginative and call it 0 . Similarly, there exists a qubit state that is certain to output a 1. We'll call this 1. These two states are completely mutually exclusiveinclusive. Either the qubit definitely outputs a 0, or it definitely outputs a 1. There is no overlap. One way to represent this with mathematics is to use two orthogonal vectors. \\begin{align*}\\cssId{ket0}{|0\\rangle} = \\begin{bmatrix} \\cssId{p0}{1} \\\\ \\cssId{p1}{0} \\end{bmatrix}, \\quad \\cssId{ket1}{|1\\rangle} =\\begin{bmatrix} \\cssId{p0}{0} \\\\ \\cssId{p1}{1} \\end{bmatrix}\\end{align*} This is a lot of notation to take in all at once. First, let's unpack the weird | and \\rangle. Their job is essentially just to remind us that we are talking about the vectors that represent qubit states labelled 0 and 1. This helps us distinguish them from things like the bit values 0 and 1 or the numbers 0 and 1. It is part of the bra-ket notation, introduced by Dirac. If you are not familiar with vectors, you can essentially just think of them as lists of numbers which we manipulate using certain rules. If you are familiar with vectors from your high school physics classes, you'll know that these rules make vectors well-suited for describing quantities with a magnitude and a direction. For example, the velocity of an object is described perfectly with a vector. However, the way we use vectors for quantum states is slightly different to this, so don't hold on too hard to your previous intuition. It's time to do something new! With vectors we can describe more complex states than just |0\\rangle and |1\\rangle. For example, consider the vector \\begin{align*}\\cssId{q0}{|q_0\\rangle} = \\begin{bmatrix} \\cssId{p0}{\\tfrac{1}{\\sqrt{2}}} \\\\ \\cssId{p1}{\\tfrac{i}{\\sqrt{2}}} \\end{bmatrix}\\end{align*} To understand what this state means, we'll need to use the mathematical rules for manipulating vectors. Specifically, we'll need to understand how to add vectors together and how to multiply them by scalars.   \\begin{align*} |a\\rangle = \\begin{bmatrix}a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix}, \\quad |b\\rangle = \\begin{bmatrix}b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\end{align*} \\begin{align*} |a\\rangle + |b\\rangle = \\begin{bmatrix}a_0 + b_0 \\\\ a_1 + b_1 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix} \\end{align*} And to multiply a vector by a scalar, we multiply each element by the scalar: \\begin{align*} x|a\\rangle = \\begin{bmatrix}x \\times a_0 \\\\ x \\times a_1 \\\\ \\vdots \\\\ x \\times a_n \\end{bmatrix} \\end{align*} These two rules are used to rewrite the vector |q_0\\rangle (as shown above): \\begin{align*}\\begin{aligned} |q_0\\rangle &= \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{i}{\\sqrt{2}}|1\\rangle \\\\ &= \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + \\tfrac{i}{\\sqrt{2}}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\\\ &= \\begin{bmatrix}\\tfrac{1}{\\sqrt{2}} \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ \\tfrac{i}{\\sqrt{2}}\\end{bmatrix} \\\\ &= \\begin{bmatrix}\\tfrac{1}{\\sqrt{2}} \\\\ \\tfrac{i}{\\sqrt{2}} \\end{bmatrix} \\\\ \\end{aligned}\\end{align*}   And normalised means their magnitudes (length of the arrow) is equal to 1. The two vectors |0\\rangle and |1\\rangle are linearly independent, which means we cannot describe |0\\rangle in terms of |1\\rangle, and vice versa. However, using both the vectors |0\\rangle and |1\\rangle, and our rules of addition and multiplication by scalars, we can describe all possible vectors in 2D space: Because the vectors |0\\rangle and |1\\rangle are linearly independent, and can be used to describe any vector in 2D space using vector addition and scalar multiplication, we say the vectors |0\\rangle and |1\\rangle form a basis. In this case, since they are both orthogonal and normalised, we call it an orthonormal basis. Since the states |0\\rangle and |1\\rangle form an orthonormal basis, we can represent any 2D vector with a combination of these two states. This allows us to write the state of our qubit in the alternative form: \\begin{align*} \\cssId{q0}{|q_0\\rangle} = \\cssId{term1}{\\tfrac{1}{\\sqrt{2}}|0\\rangle} + \\cssId{term2}{\\tfrac{i}{\\sqrt{2}}|1\\rangle} \\end{align*} This vector, |q_0\\rangle is called the qubit's statevector, it tells us everything we could possibly know about this qubit. For now, we are only able to draw a few simple conclusions about this particular example of a statevector: it is not entirely |0\\rangle and not entirely |1\\rangle. Instead, it is described by a linear combination of the two. In quantum mechanics, we typically describe linear combinations such as this using the word 'superposition'. Though our example state |q_0\\rangle can be expressed as a superposition of |0\\rangle and |1\\rangle, it is no less a definite and well-defined qubit state than they are. To see this, we can begin to explore how a qubit can be manipulated.   First, we need to import all the tools we will need: In Qiskit, we use the QuantumCircuit object to store our circuits, this is essentially a list of the quantum operations on our circuit and the qubits they are applied to. In our quantum circuits, our qubits always start out in the state |0\\rangle. We can use the initialize() method to transform this into any state. We give initialize() the vector we want in the form of a list, and tell it which qubit(s) we want to initialize in this state: We can then use one of Qiskit\u2019s simulators to view the resulting state of our qubit. To get the results from our circuit, we use run to execute our circuit, giving the circuit and the backend as arguments. We then use .result() to get the result of this: from result, we can then get the final statevector using .get_statevector(): Note: Python uses j to represent i in complex numbers. We see a vector with two complex elements: 0.+0.j = 0, and 1.+0.j = 1. Let\u2019s now measure our qubit as we would in a real quantum computer and see the result: This time, instead of the statevector we will get the counts for the 0 and 1 results using .get_counts(): We can see that we (unsurprisingly) have a % chance of measuring |1\\rangle.  This time, let\u2019s instead put our qubit into a superposition and see what happens. We will use the state |q_0\\rangle from earlier in this section: \\begin{align*} |q_0\\rangle = \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{\\cssId{i}{i}}{\\sqrt{2}}|1\\rangle \\end{align*} We need to add these amplitudes to a python list. To add a complex amplitude, Python uses j for the imaginary unit (we normally call it \"i\" mathematically): And we then repeat the steps for initialising the qubit as before: We can see we have equallessgreater probability of measuring |0\\rangle compared to |1\\rangle. To explain this, we need to talk about measurement.       There is a simple rule for measurement. To find the probability of measuring a state |\\psi \\rangle in the state |x\\rangle we do: \\begin{align*}p(|x\\rangle) = | \\langle x| \\psi \\rangle|^2\\end{align*} The symbols \\langle and | tell us \\langle x | is a row vector. In quantum mechanics we call the column vectors kets and the row vectors bras. Together they make up bra-ket notation. Any ket |a\\rangle has a corresponding bra \\langle a|, and we convert between them using the conjugate transpose.   \\begin{align*} \\quad|a\\rangle = \\begin{bmatrix}a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\end{align*} To get the conjugate transpose, the matrix is transposed and the elements are complex conjugated (represented by the \"\u2217\" operation) where complex conjugate of a complex number is a number with an equal real part and an imaginary part equal in magnitude but opposite in sign. This gives the corresponding bra (row vector) as follows: \\begin{align*} \\langle a| = \\begin{bmatrix}a_0^*, & a_1^*, & \\dots & a_n^* \\end{bmatrix} \\end{align*}   \\begin{align*}\\langle a| = \\begin{bmatrix}a_0^*, & a_1^*, & \\dots & a_n^* \\end{bmatrix} \\end{align*} \\begin{align*}|b\\rangle = \\begin{bmatrix}b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\end{align*} \\begin{align*}\\langle a|b\\rangle = a_0^* b_0 + a_1^* b_1 \\dots a_n^* b_n \\end{align*} We can see that the inner product of two vectors always gives us a scalar. A useful thing to remember is that the inner product of two orthogonal vectors is 0, for example if we have the orthogonal vectors |0\\rangle and |1\\rangle: \\begin{align*}\\langle1|0\\rangle = \\begin{bmatrix} 0 , & 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = 0\\end{align*} Additionally, remember that the vectors |0\\rangle and |1\\rangle are also normalised (magnitudes are equal to 1): \\begin{align*}\\begin{aligned} \\langle0|0\\rangle &= \\begin{bmatrix} 1 , & 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = 1 \\\\ \\langle1|1\\rangle &= \\begin{bmatrix} 0 , & 1\\end{bmatrix}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = 1 \\end{aligned}\\end{align*} In the equation above, |x\\rangle can be any qubit state. To find the probability of measuring |x\\rangle, we take the inner product of |x\\rangle and the state we are measuring (in this case |\\psi\\rangle), then square the magnitude. This may seem a little convoluted, but it will soon become second nature. If we look at the state |q_0\\rangle from before, we can see the probability of measuring |0\\rangle is indeed 0.5: \\begin{align*}\\begin{aligned} |q_0\\rangle & = \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{i}{\\sqrt{2}}|1\\rangle \\\\ \\langle 0| q_0 \\rangle & = \\tfrac{1}{\\sqrt{2}}\\langle 0|0\\rangle + \\tfrac{i}{\\sqrt{2}}\\langle 0|1\\rangle \\\\ & = \\tfrac{1}{\\sqrt{2}}\\cdot 1 + \\tfrac{i}{\\sqrt{2}} \\cdot 0\\\\ & = \\tfrac{1}{\\sqrt{2}}\\\\ |\\langle 0| q_0 \\rangle|^2 & = \\tfrac{1}{2} \\end{aligned}\\end{align*} You should verify the probability of measuring |1\\rangle as an exercise. This rule governs how we get information out of quantum states. It is therefore very important for everything we do in quantum computation. It also immediately implies several important facts.       The rule shows us that amplitudes are related to probabilities. If we want the probabilities to add up to 1 (which they should!), we need to ensure that the statevector is properly normalized. Specifically, we need the magnitude of the state vector to be 1. \\begin{align*} \\langle\\psi|\\psi\\rangle = 1 \\end{align*} Thus if: \\begin{align*} |\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle \\end{align*} Then: \\begin{align*} |\\alpha|^2 + |\\beta|^2 = 1 \\end{align*} This explains the factors of \\sqrt{2} you have seen throughout this chapter. In fact, if we try to give initialize() a vector that isn\u2019t normalised, it will give us an error: You can check your answer in the widget below (accepts answers \u00b11% accuracy, you can use numpy terms such as 'pi' and 'sqrt()' in the vector):   The measurement rule gives us the probability p(|x\\rangle) that a state |\\psi\\rangle is measured as |x\\rangle. Nowhere does it tell us that |x\\rangle can only be either |0\\rangle or |1\\rangle. The measurements we have considered so far are in fact only one of an infinite number of possible ways to measure a qubit. For any orthogonal pair of states, we can define a measurement that would cause a qubit to choose between the two. This possibility will be explored more in the next section. For now, just bear in mind that |x\\rangle is not limited to being simply |0\\rangle or |1\\rangle.   We know that measuring the state |1\\rangle will give us the output 1 with certainty. But we are also able to write down states such as  \\begin{align*}\\begin{bmatrix}0 \\\\ i\\end{bmatrix} = i|1\\rangle.\\end{align*} To see how this behaves, we apply the measurement rule. \\begin{align*} |\\langle x| (i|1\\rangle) |^2 = | i \\langle x|1\\rangle|^2 = |\\langle x|1\\rangle|^2 \\end{align*} Here we find that the factor of i disappears once we take the magnitude of the complex number. This effect is completely independent of the measured state |x\\rangle. It does not matter what measurement we are considering, the probabilities for the state i|1\\rangle are identical to those for |1\\rangle. Since measurements are the only way we can extract any information from a qubit, this implies that these two states are equivalent in all ways that are physically relevant. More generally, we refer to any overall factor \\gamma on a state for which |\\gamma|=1 as a 'global phase'. States that differ only by a global phase are physically indistinguishable. \\begin{align*} |\\langle x| ( \\gamma |a\\rangle) |^2 = | \\gamma \\langle x|a\\rangle|^2 = |\\langle x|a\\rangle|^2 \\end{align*} Note that this is distinct from the phase difference between terms in a superposition, which is known as the 'relative phase'. This becomes relevant once we consider different types of measurement and multiple qubits.   We know that the amplitudes contain information about the probability of us finding the qubit in a specific state, but once we have measured the qubit, we know with certainty what the state of the qubit is. For example, if we measure a qubit in the state: \\begin{align*} |q\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\\end{align*} And find it in the state |0\\rangle, if we measure again, there is a 100% chance of finding the qubit in the state |0\\rangle. This means the act of measuring changes the state of our qubits. \\begin{align*} |q\\rangle = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} \\xrightarrow{\\text{Measure }|0\\rangle} |q\\rangle = |0\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\end{align*} We sometimes refer to this as collapsing the state of the qubit. It is a potent effect, and so one that must be used wisely. For example, were we to constantly measure each of our qubits to keep track of their value at each point in a computation, they would always simply be in a well-defined state of either |0\\rangle or |1\\rangle. As such, they would be no different from classical bits and our computation could be easily replaced by a classical computation. To achieve truly quantum computation we must allow the qubits to explore more complex states. Measurements are therefore only used when we need to extract an output. This means that we often place all the measurements at the end of our quantum circuit.  We can demonstrate this using Qiskit\u2019s statevector simulator. Let's initialize a qubit in superposition: This should initialize our qubit in the state: \\begin{align*} |q\\rangle = \\tfrac{i}{\\sqrt{2}}|0\\rangle + \\tfrac{1}{\\sqrt{2}}|1\\rangle \\end{align*} We can verify this using the simulator: We can see here the qubit is initialized in the state [0.+0.70710678j 0.70710678+0.j], which is the state we expected. Let\u2019s now create a circuit where we measure this qubit: When we simulate this entire circuit, we can see that one of the amplitudes is always 0: You can re-run this cell a few times to reinitialize the qubit and measure it again. You will notice that either outcome is equally probable, but that the state of the qubit is never a superposition of |0\\rangle and |1\\rangle. Somewhat interestingly, the global phase on the state |0\\rangle survives, but since this is global phase, we can never measure it on a real quantum computer.   We can see that writing down a qubit\u2019s state requires keeping track of two complex numbers, but when using a real quantum computer we will only ever receive a yes-or-no (0 or 1) answer for each qubit. The output of a 10-qubit quantum computer will look like this: 0110111110 Just 10 bits, no superposition or complex amplitudes. When using a real quantum computer, we cannot see the states of our qubits mid-computation, as this would destroy them! This behaviour is not ideal for learning, so Qiskit provides different quantum simulators: By default, the aer_simulator mimics the execution of a real quantum computer, but will also allow you to peek at quantum states before measurement if we include certain instructions in our circuit. For example, here we have included the instruction .save_statevector(), which means we can use .get_statevector() on the result of the simulation.        We saw earlier in this chapter that the general state of a qubit (|q\\rangle) is: \\begin{align*}|q\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\\end{align*} \\begin{align*}\\alpha, \\beta \\in \\mathbb{C}\\end{align*} (The second line tells us \\alpha and \\beta are complex numbers). The first two implications in section 2 tell us that we cannot differentiate between some of these states. This means we can be more specific in our description of the qubit.  Firstly, since we cannot measure global phase, we can only measure the difference in phase between the states |0\\rangle and |1\\rangle. Instead of having \\alpha and \\beta be complex, we can confine them to the real numbers and add a term to tell us the relative phase between them: \\begin{align*}|q\\rangle = \\alpha|0\\rangle + e^{i\\phi}\\beta|1\\rangle\\end{align*} \\begin{align*}\\alpha, \\beta, \\phi \\in \\mathbb{R}\\end{align*} Finally, since the qubit state must be normalised, i.e. \\begin{align*}\\sqrt{\\alpha^2 + \\beta^2} = 1\\end{align*} we can use the trigonometric identity: \\begin{align*}\\sqrt{\\sin^2{x} + \\cos^2{x}} = 1\\end{align*} to describe the real \\alpha and \\beta in terms of one variable, \\theta: \\begin{align*}\\alpha = \\cos{\\tfrac{\\theta}{2}}, \\quad \\beta=\\sin{\\tfrac{\\theta}{2}}\\end{align*} From this we can describe the state of any qubit using the two variables \\phi and \\theta: \\begin{align*}|q\\rangle = \\cos{\\tfrac{\\theta}{2}}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle\\end{align*} \\begin{align*}\\theta, \\phi \\in \\mathbb{R}\\end{align*}   We want to plot our general qubit state: \\begin{align*}|q\\rangle = \\cos{\\tfrac{\\theta}{2}}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle\\end{align*} If we interpret \\theta and \\phi as spherical co-ordinates (r = 1, since the magnitude of the qubit state is 1), we can plot any single qubit state on the surface of a sphere, known as the Bloch sphere. Below we have plotted a qubit in the state |{+}\\rangle. In this case, \\theta = \\pi/2 and \\phi = 0. (Qiskit has a function to plot a bloch sphere, plot_bloch_vector(), but at the time of writing it only takes cartesian coordinates. We have included a function that does the conversion automatically). You can also try this interactive Bloch sphere demo.     Use plot_bloch_vector() or plot_bloch_sphere_spherical() to plot a qubit in the states: We have also included below a widget that converts from spherical co-ordinates to cartesian, for use with plot_bloch_vector(): You now know something about bits, and about how our familiar digital computers work. All the complex variables, objects and data structures used in modern software are basically all just big piles of bits. Those of us who work on quantum computing call these classical variables. The computers that use them, like the one you are using to read this article, we call classical computers. In quantum computers, our basic variable is the qubit: a quantum variant of the bit. These have exactly the same restrictions as normal bits do: they can store only a single binary piece of information, and can only ever give us an output of 0 or 1-1 or 101-1. However, they can also be manipulated in ways that can only be described by quantum mechanics. This gives us new gates to play with, allowing us to find new ways to design algorithms. To fully understand these new gates, we first need to understand how to write down qubit states. For this we will use the mathematics of vectors, matrices, and complex numbers. Though we will introduce these concepts as we go, it would be best if you are comfortable with them already. If you need a more in-depth explanation or a refresher, you can find the guide here.     In quantum physics we use statevectors to describe the state of our system. Say we wanted to describe the position of a car along a track, this is a classical system so we could use a number x: \\begin{align*} x=4 \\end{align*} Alternatively, we could instead use a collection of numbers in a vector called a statevector. Each element in the statevector contains the probability of finding the car in a certain place: \\begin{align*} \\cssId{x_ket}{|x\\rangle} = \\cssId{vector}{\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}} \\begin{matrix} \\\\ \\\\ \\\\ \\leftarrow \\\\ \\\\ \\\\ \\\\ \\end{matrix} \\begin{matrix} \\\\ \\\\ \\text{Probability of} \\\\ \\text{car being at} \\\\ \\text{position 4} \\\\ \\\\ \\\\ \\end{matrix} \\end{align*} This isn\u2019t limited to position, we could also keep a statevector of all the possible speeds the car could have, and all the possible colours the car could be. With classical systems (like the car example above), this is a silly thing to do as it requires keeping huge vectors when we only really need one number. But as we will see in this chapter, statevectors happen to be a very good way of keeping track of quantum systems, including quantum computers.   Classical bits are always either 0 or 1 at every point during a computation. There is no more detail we can add to the state of a bit than this. So to write down the state of a of classical bit (c), we can just use these two binary values. For example: \\begin{align*} c = 0 \\end{align*} This restriction is lifted for quantum bits. Whether we get a 0 or a 1 from a qubit only needs to be well-defined when a measurement is made to extract an output. At that point, it must commit to one of these two options. At all other times, its state will be something more complex than can be captured by a simple binary value. To see how to describe these, we can first focus on the two simplest cases. As we saw in the last section, it is possible to prepare a qubit in a state for which it definitely gives the outcome 0 when measured. We need a name for this state. Let's be unimaginative and call it 0 . Similarly, there exists a qubit state that is certain to output a 1. We'll call this 1. These two states are completely mutually exclusiveinclusive. Either the qubit definitely outputs a 0, or it definitely outputs a 1. There is no overlap. One way to represent this with mathematics is to use two orthogonal vectors. \\begin{align*}\\cssId{ket0}{|0\\rangle} = \\begin{bmatrix} \\cssId{p0}{1} \\\\ \\cssId{p1}{0} \\end{bmatrix}, \\quad \\cssId{ket1}{|1\\rangle} =\\begin{bmatrix} \\cssId{p0}{0} \\\\ \\cssId{p1}{1} \\end{bmatrix}\\end{align*} This is a lot of notation to take in all at once. First, let's unpack the weird | and \\rangle. Their job is essentially just to remind us that we are talking about the vectors that represent qubit states labelled 0 and 1. This helps us distinguish them from things like the bit values 0 and 1 or the numbers 0 and 1. It is part of the bra-ket notation, introduced by Dirac. If you are not familiar with vectors, you can essentially just think of them as lists of numbers which we manipulate using certain rules. If you are familiar with vectors from your high school physics classes, you'll know that these rules make vectors well-suited for describing quantities with a magnitude and a direction. For example, the velocity of an object is described perfectly with a vector. However, the way we use vectors for quantum states is slightly different to this, so don't hold on too hard to your previous intuition. It's time to do something new! With vectors we can describe more complex states than just |0\\rangle and |1\\rangle. For example, consider the vector \\begin{align*}\\cssId{q0}{|q_0\\rangle} = \\begin{bmatrix} \\cssId{p0}{\\tfrac{1}{\\sqrt{2}}} \\\\ \\cssId{p1}{\\tfrac{i}{\\sqrt{2}}} \\end{bmatrix}\\end{align*} To understand what this state means, we'll need to use the mathematical rules for manipulating vectors. Specifically, we'll need to understand how to add vectors together and how to multiply them by scalars.   \\begin{align*} |a\\rangle = \\begin{bmatrix}a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix}, \\quad |b\\rangle = \\begin{bmatrix}b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\end{align*} \\begin{align*} |a\\rangle + |b\\rangle = \\begin{bmatrix}a_0 + b_0 \\\\ a_1 + b_1 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix} \\end{align*} And to multiply a vector by a scalar, we multiply each element by the scalar: \\begin{align*} x|a\\rangle = \\begin{bmatrix}x \\times a_0 \\\\ x \\times a_1 \\\\ \\vdots \\\\ x \\times a_n \\end{bmatrix} \\end{align*} These two rules are used to rewrite the vector |q_0\\rangle (as shown above): \\begin{align*}\\begin{aligned} |q_0\\rangle &= \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{i}{\\sqrt{2}}|1\\rangle \\\\ &= \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + \\tfrac{i}{\\sqrt{2}}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\\\ &= \\begin{bmatrix}\\tfrac{1}{\\sqrt{2}} \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ \\tfrac{i}{\\sqrt{2}}\\end{bmatrix} \\\\ &= \\begin{bmatrix}\\tfrac{1}{\\sqrt{2}} \\\\ \\tfrac{i}{\\sqrt{2}} \\end{bmatrix} \\\\ \\end{aligned}\\end{align*}   And normalised means their magnitudes (length of the arrow) is equal to 1. The two vectors |0\\rangle and |1\\rangle are linearly independent, which means we cannot describe |0\\rangle in terms of |1\\rangle, and vice versa. However, using both the vectors |0\\rangle and |1\\rangle, and our rules of addition and multiplication by scalars, we can describe all possible vectors in 2D space: Because the vectors |0\\rangle and |1\\rangle are linearly independent, and can be used to describe any vector in 2D space using vector addition and scalar multiplication, we say the vectors |0\\rangle and |1\\rangle form a basis. In this case, since they are both orthogonal and normalised, we call it an orthonormal basis. Since the states |0\\rangle and |1\\rangle form an orthonormal basis, we can represent any 2D vector with a combination of these two states. This allows us to write the state of our qubit in the alternative form: \\begin{align*} \\cssId{q0}{|q_0\\rangle} = \\cssId{term1}{\\tfrac{1}{\\sqrt{2}}|0\\rangle} + \\cssId{term2}{\\tfrac{i}{\\sqrt{2}}|1\\rangle} \\end{align*} This vector, |q_0\\rangle is called the qubit's statevector, it tells us everything we could possibly know about this qubit. For now, we are only able to draw a few simple conclusions about this particular example of a statevector: it is not entirely |0\\rangle and not entirely |1\\rangle. Instead, it is described by a linear combination of the two. In quantum mechanics, we typically describe linear combinations such as this using the word 'superposition'. Though our example state |q_0\\rangle can be expressed as a superposition of |0\\rangle and |1\\rangle, it is no less a definite and well-defined qubit state than they are. To see this, we can begin to explore how a qubit can be manipulated.   First, we need to import all the tools we will need: In Qiskit, we use the QuantumCircuit object to store our circuits, this is essentially a list of the quantum operations on our circuit and the qubits they are applied to. In our quantum circuits, our qubits always start out in the state |0\\rangle. We can use the initialize() method to transform this into any state. We give initialize() the vector we want in the form of a list, and tell it which qubit(s) we want to initialize in this state: We can then use one of Qiskit\u2019s simulators to view the resulting state of our qubit. To get the results from our circuit, we use run to execute our circuit, giving the circuit and the backend as arguments. We then use .result() to get the result of this: from result, we can then get the final statevector using .get_statevector(): Note: Python uses j to represent i in complex numbers. We see a vector with two complex elements: 0.+0.j = 0, and 1.+0.j = 1. Let\u2019s now measure our qubit as we would in a real quantum computer and see the result: This time, instead of the statevector we will get the counts for the 0 and 1 results using .get_counts(): We can see that we (unsurprisingly) have a % chance of measuring |1\\rangle.  This time, let\u2019s instead put our qubit into a superposition and see what happens. We will use the state |q_0\\rangle from earlier in this section: \\begin{align*} |q_0\\rangle = \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{\\cssId{i}{i}}{\\sqrt{2}}|1\\rangle \\end{align*} We need to add these amplitudes to a python list. To add a complex amplitude, Python uses j for the imaginary unit (we normally call it \"i\" mathematically): And we then repeat the steps for initialising the qubit as before: We can see we have equallessgreater probability of measuring |0\\rangle compared to |1\\rangle. To explain this, we need to talk about measurement.       There is a simple rule for measurement. To find the probability of measuring a state |\\psi \\rangle in the state |x\\rangle we do: \\begin{align*}p(|x\\rangle) = | \\langle x| \\psi \\rangle|^2\\end{align*} The symbols \\langle and | tell us \\langle x | is a row vector. In quantum mechanics we call the column vectors kets and the row vectors bras. Together they make up bra-ket notation. Any ket |a\\rangle has a corresponding bra \\langle a|, and we convert between them using the conjugate transpose.   \\begin{align*} \\quad|a\\rangle = \\begin{bmatrix}a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\end{align*} To get the conjugate transpose, the matrix is transposed and the elements are complex conjugated (represented by the \"\u2217\" operation) where complex conjugate of a complex number is a number with an equal real part and an imaginary part equal in magnitude but opposite in sign. This gives the corresponding bra (row vector) as follows: \\begin{align*} \\langle a| = \\begin{bmatrix}a_0^*, & a_1^*, & \\dots & a_n^* \\end{bmatrix} \\end{align*}   \\begin{align*}\\langle a| = \\begin{bmatrix}a_0^*, & a_1^*, & \\dots & a_n^* \\end{bmatrix} \\end{align*} \\begin{align*}|b\\rangle = \\begin{bmatrix}b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\end{align*} \\begin{align*}\\langle a|b\\rangle = a_0^* b_0 + a_1^* b_1 \\dots a_n^* b_n \\end{align*} We can see that the inner product of two vectors always gives us a scalar. A useful thing to remember is that the inner product of two orthogonal vectors is 0, for example if we have the orthogonal vectors |0\\rangle and |1\\rangle: \\begin{align*}\\langle1|0\\rangle = \\begin{bmatrix} 0 , & 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = 0\\end{align*} Additionally, remember that the vectors |0\\rangle and |1\\rangle are also normalised (magnitudes are equal to 1): \\begin{align*}\\begin{aligned} \\langle0|0\\rangle &= \\begin{bmatrix} 1 , & 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = 1 \\\\ \\langle1|1\\rangle &= \\begin{bmatrix} 0 , & 1\\end{bmatrix}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = 1 \\end{aligned}\\end{align*} In the equation above, |x\\rangle can be any qubit state. To find the probability of measuring |x\\rangle, we take the inner product of |x\\rangle and the state we are measuring (in this case |\\psi\\rangle), then square the magnitude. This may seem a little convoluted, but it will soon become second nature. If we look at the state |q_0\\rangle from before, we can see the probability of measuring |0\\rangle is indeed 0.5: \\begin{align*}\\begin{aligned} |q_0\\rangle & = \\tfrac{1}{\\sqrt{2}}|0\\rangle + \\tfrac{i}{\\sqrt{2}}|1\\rangle \\\\ \\langle 0| q_0 \\rangle & = \\tfrac{1}{\\sqrt{2}}\\langle 0|0\\rangle + \\tfrac{i}{\\sqrt{2}}\\langle 0|1\\rangle \\\\ & = \\tfrac{1}{\\sqrt{2}}\\cdot 1 + \\tfrac{i}{\\sqrt{2}} \\cdot 0\\\\ & = \\tfrac{1}{\\sqrt{2}}\\\\ |\\langle 0| q_0 \\rangle|^2 & = \\tfrac{1}{2} \\end{aligned}\\end{align*} You should verify the probability of measuring |1\\rangle as an exercise. This rule governs how we get information out of quantum states. It is therefore very important for everything we do in quantum computation. It also immediately implies several important facts.       The rule shows us that amplitudes are related to probabilities. If we want the probabilities to add up to 1 (which they should!), we need to ensure that the statevector is properly normalized. Specifically, we need the magnitude of the state vector to be 1. \\begin{align*} \\langle\\psi|\\psi\\rangle = 1 \\end{align*} Thus if: \\begin{align*} |\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle \\end{align*} Then: \\begin{align*} |\\alpha|^2 + |\\beta|^2 = 1 \\end{align*} This explains the factors of \\sqrt{2} you have seen throughout this chapter. In fact, if we try to give initialize() a vector that isn\u2019t normalised, it will give us an error: You can check your answer in the widget below (accepts answers \u00b11% accuracy, you can use numpy terms such as 'pi' and 'sqrt()' in the vector):   The measurement rule gives us the probability p(|x\\rangle) that a state |\\psi\\rangle is measured as |x\\rangle. Nowhere does it tell us that |x\\rangle can only be either |0\\rangle or |1\\rangle. The measurements we have considered so far are in fact only one of an infinite number of possible ways to measure a qubit. For any orthogonal pair of states, we can define a measurement that would cause a qubit to choose between the two. This possibility will be explored more in the next section. For now, just bear in mind that |x\\rangle is not limited to being simply |0\\rangle or |1\\rangle.   We know that measuring the state |1\\rangle will give us the output 1 with certainty. But we are also able to write down states such as  \\begin{align*}\\begin{bmatrix}0 \\\\ i\\end{bmatrix} = i|1\\rangle.\\end{align*} To see how this behaves, we apply the measurement rule. \\begin{align*} |\\langle x| (i|1\\rangle) |^2 = | i \\langle x|1\\rangle|^2 = |\\langle x|1\\rangle|^2 \\end{align*} Here we find that the factor of i disappears once we take the magnitude of the complex number. This effect is completely independent of the measured state |x\\rangle. It does not matter what measurement we are considering, the probabilities for the state i|1\\rangle are identical to those for |1\\rangle. Since measurements are the only way we can extract any information from a qubit, this implies that these two states are equivalent in all ways that are physically relevant. More generally, we refer to any overall factor \\gamma on a state for which |\\gamma|=1 as a 'global phase'. States that differ only by a global phase are physically indistinguishable. \\begin{align*} |\\langle x| ( \\gamma |a\\rangle) |^2 = | \\gamma \\langle x|a\\rangle|^2 = |\\langle x|a\\rangle|^2 \\end{align*} Note that this is distinct from the phase difference between terms in a superposition, which is known as the 'relative phase'. This becomes relevant once we consider different types of measurement and multiple qubits.   We know that the amplitudes contain information about the probability of us finding the qubit in a specific state, but once we have measured the qubit, we know with certainty what the state of the qubit is. For example, if we measure a qubit in the state: \\begin{align*} |q\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\\end{align*} And find it in the state |0\\rangle, if we measure again, there is a 100% chance of finding the qubit in the state |0\\rangle. This means the act of measuring changes the state of our qubits. \\begin{align*} |q\\rangle = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} \\xrightarrow{\\text{Measure }|0\\rangle} |q\\rangle = |0\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\end{align*} We sometimes refer to this as collapsing the state of the qubit. It is a potent effect, and so one that must be used wisely. For example, were we to constantly measure each of our qubits to keep track of their value at each point in a computation, they would always simply be in a well-defined state of either |0\\rangle or |1\\rangle. As such, they would be no different from classical bits and our computation could be easily replaced by a classical computation. To achieve truly quantum computation we must allow the qubits to explore more complex states. Measurements are therefore only used when we need to extract an output. This means that we often place all the measurements at the end of our quantum circuit.  We can demonstrate this using Qiskit\u2019s statevector simulator. Let's initialize a qubit in superposition: This should initialize our qubit in the state: \\begin{align*} |q\\rangle = \\tfrac{i}{\\sqrt{2}}|0\\rangle + \\tfrac{1}{\\sqrt{2}}|1\\rangle \\end{align*} We can verify this using the simulator: We can see here the qubit is initialized in the state [0.+0.70710678j 0.70710678+0.j], which is the state we expected. Let\u2019s now create a circuit where we measure this qubit: When we simulate this entire circuit, we can see that one of the amplitudes is always 0: You can re-run this cell a few times to reinitialize the qubit and measure it again. You will notice that either outcome is equally probable, but that the state of the qubit is never a superposition of |0\\rangle and |1\\rangle. Somewhat interestingly, the global phase on the state |0\\rangle survives, but since this is global phase, we can never measure it on a real quantum computer.   We can see that writing down a qubit\u2019s state requires keeping track of two complex numbers, but when using a real quantum computer we will only ever receive a yes-or-no (0 or 1) answer for each qubit. The output of a 10-qubit quantum computer will look like this: 0110111110 Just 10 bits, no superposition or complex amplitudes. When using a real quantum computer, we cannot see the states of our qubits mid-computation, as this would destroy them! This behaviour is not ideal for learning, so Qiskit provides different quantum simulators: By default, the aer_simulator mimics the execution of a real quantum computer, but will also allow you to peek at quantum states before measurement if we include certain instructions in our circuit. For example, here we have included the instruction .save_statevector(), which means we can use .get_statevector() on the result of the simulation.        We saw earlier in this chapter that the general state of a qubit (|q\\rangle) is: \\begin{align*}|q\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\\end{align*} \\begin{align*}\\alpha, \\beta \\in \\mathbb{C}\\end{align*} (The second line tells us \\alpha and \\beta are complex numbers). The first two implications in section 2 tell us that we cannot differentiate between some of these states. This means we can be more specific in our description of the qubit.  Firstly, since we cannot measure global phase, we can only measure the difference in phase between the states |0\\rangle and |1\\rangle. Instead of having \\alpha and \\beta be complex, we can confine them to the real numbers and add a term to tell us the relative phase between them: \\begin{align*}|q\\rangle = \\alpha|0\\rangle + e^{i\\phi}\\beta|1\\rangle\\end{align*} \\begin{align*}\\alpha, \\beta, \\phi \\in \\mathbb{R}\\end{align*} Finally, since the qubit state must be normalised, i.e. \\begin{align*}\\sqrt{\\alpha^2 + \\beta^2} = 1\\end{align*} we can use the trigonometric identity: \\begin{align*}\\sqrt{\\sin^2{x} + \\cos^2{x}} = 1\\end{align*} to describe the real \\alpha and \\beta in terms of one variable, \\theta: \\begin{align*}\\alpha = \\cos{\\tfrac{\\theta}{2}}, \\quad \\beta=\\sin{\\tfrac{\\theta}{2}}\\end{align*} From this we can describe the state of any qubit using the two variables \\phi and \\theta: \\begin{align*}|q\\rangle = \\cos{\\tfrac{\\theta}{2}}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle\\end{align*} \\begin{align*}\\theta, \\phi \\in \\mathbb{R}\\end{align*}   We want to plot our general qubit state: \\begin{align*}|q\\rangle = \\cos{\\tfrac{\\theta}{2}}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle\\end{align*} If we interpret \\theta and \\phi as spherical co-ordinates (r = 1, since the magnitude of the qubit state is 1), we can plot any single qubit state on the surface of a sphere, known as the Bloch sphere. Below we have plotted a qubit in the state |{+}\\rangle. In this case, \\theta = \\pi/2 and \\phi = 0. (Qiskit has a function to plot a bloch sphere, plot_bloch_vector(), but at the time of writing it only takes cartesian coordinates. We have included a function that does the conversion automatically). You can also try this interactive Bloch sphere demo.     Use plot_bloch_vector() or plot_bloch_sphere_spherical() to plot a qubit in the states: We have also included below a widget that converts from spherical co-ordinates to cartesian, for use with plot_bloch_vector(): In the previous section we looked at all the possible states a qubit could be in. We saw that qubits could be represented by 2D vectors, and that their states are limited to the form: \\begin{align*} |q\\rangle = \\cos{(\\tfrac{\\theta}{2})}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle \\end{align*} Where \\theta and \\phi are real numbers. In this section we will cover gates, the operations that change a qubit between these states. Due to the number of gates and the similarities between them, this chapter is at risk of becoming a list. To counter this, we have included a few digressions to introduce important ideas at appropriate places throughout the chapter.  In The Atoms of Computation we came across some gates and used them to perform a classical computation. An important feature of quantum circuits is that, between initializing the qubits and measuring them, the operations (gates) are always reversible! These reversible gates can be represented as matrices, and as rotations around the Bloch sphere.          \\begin{align*} X = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = |0\\rangle\\langle1| + |1\\rangle\\langle0| \\end{align*} To see the effect a gate has on a qubit, we simply multiply the qubit\u2019s statevector by the gate. We can see that the X-gate switches the amplitudes of the states |0\\rangle and |1\\rangle: \\begin{align*} X|0\\rangle = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = |1\\rangle\\end{align*}   \\begin{align*} M|v\\rangle = \\begin{bmatrix}a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix}v_0 \\\\ v_1 \\end{bmatrix} = \\begin{bmatrix}a\\cdot v_0 + b \\cdot v_1 \\\\ c \\cdot v_0 + d \\cdot v_1 \\end{bmatrix} \\end{align*} In quantum computing, we can write our matrices in terms of basis vectors: \\begin{align*}X = |0\\rangle\\langle1| + |1\\rangle\\langle0|\\end{align*} This can sometimes be clearer than using a box matrix as we can see what different multiplications will result in: \\begin{align*}\\begin{aligned} X|1\\rangle & = (|0\\rangle\\langle1| + |1\\rangle\\langle0|)|1\\rangle \\\\ & = |0\\rangle\\langle1|1\\rangle + |1\\rangle\\langle0|1\\rangle \\\\ & = |0\\rangle \\times 1 + |1\\rangle \\times 0 \\\\ & = |0\\rangle \\end{aligned}\\end{align*} In fact, when we see a ket and a bra multiplied like this: \\begin{align*} |a\\rangle\\langle b| \\end{align*} this is called the outer product, which follows the rule: \\begin{align*} |a\\rangle\\langle b| = \\begin{bmatrix} a_0 b_0 & a_0 b_1 & \\dots & a_0 b_n\\\\ a_1 b_0 & \\ddots & & \\vdots \\\\ \\vdots & & \\ddots & \\vdots \\\\ a_n b_0 & \\dots & \\dots & a_n b_n \\\\ \\end{bmatrix}\\end{align*} We can see this does indeed result in the X-matrix as seen above: \\begin{align*} |0\\rangle\\langle1| + |1\\rangle\\langle0| = \\begin{bmatrix}0 & 1 \\\\ 0 & 0 \\end{bmatrix} + \\begin{bmatrix}0 & 0 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix}0 & 1 \\\\ 1 & 0 \\end{bmatrix} = X\\end{align*} In Qiskit, we can create a short circuit to verify this: Let's see the result of the above circuit. Note: Here we use plot_bloch_multivector() which takes a qubit's statevector instead of the Bloch vector. We can indeed see the state of the qubit is |1\\rangle as expected. We can think of this as a rotation by \\pi radians around the x-axis of the Bloch sphere. The X-gate is also often called a NOT-gate, referring to its classical analogue.     \\begin{align*} Y = \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\quad\\quad\\quad\\quad Z = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\end{align*} \\begin{align*} Y = -i|0\\rangle\\langle1| + i|1\\rangle\\langle0| \\quad\\quad Z = |0\\rangle\\langle0| - |1\\rangle\\langle1| \\end{align*} And, unsurprisingly, they also respectively perform rotations by \\pi2\\pi\\frac{\\pi}{2} around the y and z-axis of the Bloch sphere. Below is a widget that displays a qubit\u2019s state on the Bloch sphere, pressing one of the buttons will perform the gate on the qubit: In Qiskit, we can apply the Y and Z-gates to our circuit using:     \\begin{align*} M|v\\rangle = |v'\\rangle \\leftarrow \\text{new vector} \\end{align*} If we chose the right vectors and matrices, we can find a case in which this matrix multiplication is the same as doing a multiplication by a scalar: \\begin{align*} M|v\\rangle = \\lambda|v\\rangle \\end{align*} (Above, M is a matrix, and \\lambda is a scalar). For a matrix M, any vector that has this property is called an eigenvector of M. For example, the eigenvectors of the Z-matrix are the states |0\\rangle and |1\\rangle: \\begin{align*} \\begin{aligned} Z|0\\rangle & = |0\\rangle \\\\ Z|1\\rangle & = -|1\\rangle \\end{aligned} \\end{align*} Since we use vectors to describe the state of our qubits, we often call these vectors eigenstates in this context. Eigenvectors are very important in quantum computing, and it is important you have a solid grasp of them. You may also notice that the Z-gate appears to have no effect on our qubit when it is in either of these two states. This is because the states |0\\rangle and |1\\rangle are the two eigenstates of the Z-gate. In fact, the computational basis (the basis formed by the states |0\\rangle and |1\\rangle) is often called the Z-basis. This is not the only basis we can use, a popular basis is the X-basis, formed by the eigenstates of the X-gate. We call these two vectors |+\\rangle and |-\\rangle: \\begin{align*} |+\\rangle = \\tfrac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\end{align*} \\begin{align*} |-\\rangle = \\tfrac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\end{align*} Another less commonly used basis is that formed by the eigenstates of the Y-gate. These are called: \\begin{align*} |\\circlearrowleft\\rangle, \\quad |\\circlearrowright\\rangle\\end{align*} We leave it as an exercise to calculate these. There are in fact an infinite number of bases; to form one, we simply need two orthogonal vectors. The eigenvectors of both Hermitian and unitary matrices form a basis for the vector space. Due to this property, we can be sure that the eigenstates of the X-gate and the Y-gate indeed form a basis for 1-qubit states (read more about this in the linear algebra page in the appendix)     Using only the Pauli-gates it is impossible to move our initialized qubit to any state other than |0\\rangle or |1\\rangle, i.e. we cannot achieve superposition. This means we can see no behaviour different to that of a classical bit. To create more interesting states we will need more gates!   The Hadamard gate (H-gate) is a fundamental quantum gate. It allows us to move away from the poles of the Bloch sphere and create a superposition of |0\\rangle and |1\\rangle. It has the matrix: \\begin{align*} H = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\end{align*} We can see that this performs the transformations below: \\begin{align*} H|0\\rangle = |+\\rangle \\end{align*} \\begin{align*} H|1\\rangle = |-\\rangle \\end{align*} This can be thought of as a rotation around the Bloch vector [1,0,1] (the line between the x & z-axis), or as transforming the state of the qubit between the X and Z bases. You can play around with these gates using the widget below:         As an example, let\u2019s try measuring in the X-basis. We can calculate the probability of measuring either |+\\rangle or |-\\rangle: \\begin{align*} p(|+\\rangle) = |\\langle+|q\\rangle|^2, \\quad p(|-\\rangle) = |\\langle-|q\\rangle|^2 \\end{align*} And after measurement, the superposition is destroyed. Since Qiskit only allows measuring in the Z-basis, we must create our own using Hadamard gates: In the quick exercises above, we saw you could create an X-gate by sandwiching our Z-gate between two H-gates: \\begin{align*} X = HZH \\end{align*} Starting in the Z-basis, the H-gate switches our qubit to the X-basis, the Z-gate performs a NOT in the X-basis, and the final H-gate returns our qubit to the Z-basis. We can verify this always behaves like an X-gate by multiplying the matrices: \\begin{align*}HZH = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} =X\\end{align*} Following the same logic, we have created an X-measurement by transforming from the X-basis to the Z-basis before our measurement. Since the process of measuring can have different effects depending on the system (e.g. some systems always return the qubit to |0\\rangle after measurement, whereas others may leave it as the measured state), the state of the qubit post-measurement is undefined and we must reset it if we want to use it again. There is another way to see why the Hadamard gate indeed takes us from the Z-basis to the X-basis. Suppose the qubit we want to measure in the X-basis is in the (normalized) state a |0\\rangle + b |1\\rangle. To measure it in X-basis, we first express the state as a linear combination of |+\\rangle and |-\\rangle. Using the relations |0\\rangle = \\frac{|+\\rangle + |-\\rangle}{\\sqrt{2}} and |1\\rangle = \\frac{|+\\rangle - |-\\rangle}{\\sqrt{2}}, the state becomes \\frac{a + b}{\\sqrt{2}}|+\\rangle + \\frac{a - b}{\\sqrt{2}}|-\\rangle. Observe that the probability amplitudes in X-basis can be obtained by applying a Hadamard matrix on the state vector expressed in Z-basis. Let\u2019s now see the results: We initialized our qubit in the state |-\\rangle, but we can see that, after the measurement, we have collapsed our qubit to the state |1\\rangle. If you run the cell again, you will see the same result, since along the X-basis, the state |-\\rangle is a basis state and measuring it along X will always yield the same result.     Measuring in different bases allows us to see Heisenberg\u2019s famous uncertainty principle in action. Having certainty of measuring a state in the Z-basis removes all certainty of measuring a specific state in the X-basis, and vice versa. A common misconception is that the uncertainty is due to the limits in our equipment, but here we can see the uncertainty is actually part of the nature of the qubit.  For example, if we put our qubit in the state |0\\rangle, our measurement in the Z-basis is certain to be |0\\rangle, but our measurement in the X-basis is completely random! Similarly, if we put our qubit in the state |-\\rangle, our measurement in the X-basis is certain to be |-\\rangle, but now any measurement in the Z-basis will be completely random. More generally: Whatever state our quantum system is in, there is always a measurement that has a deterministic outcome.  The introduction of the H-gate has allowed us to explore some interesting phenomena, but we are still very limited in our quantum operations. Let us now introduce a new type of gate:   The P-gate (phase gate) is parametrised, that is, it needs a number (\\phi) to tell it exactly what to do. The P-gate performs a rotation of \\phi around the Z-axis direction. It has the matrix form: \\begin{align*}P(\\phi) = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{i\\phi} \\end{bmatrix}\\end{align*} Where \\phi is a real number. You can use the widget below to play around with the P-gate, specify \\phi using the slider: In Qiskit, we specify a P-gate using p(phi, qubit): You may notice that the Z-gate is a special case of the P-gate, with \\phi = \\pi. In fact there are three more commonly referenced gates we will mention in this chapter, all of which are special cases of the P-gate:     First comes the I-gate (aka \u2018Id-gate\u2019 or \u2018Identity gate\u2019). This is simply a gate that does nothing. Its matrix is the identity matrix: \\begin{align*}I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix}\\end{align*} Applying the identity gate anywhere in your circuit should have no effect on the qubit state, so it\u2019s interesting this is even considered a gate. There are two main reasons behind this, one is that it is often used in calculations, for example: proving the X-gate is its own inverse: \\begin{align*} I = XX \\end{align*} The second, is that it is often useful when considering real hardware to specify a \u2018do-nothing\u2019 or \u2018none\u2019 operation.       The next gate to mention is the S-gate (sometimes known as the \\sqrt{Z}-gate), this is a P-gate with \\phi = \\pi/2. It does a quarter-turn around the Bloch sphere. It is important to note that unlike every gate introduced in this chapter so far, the S-gate is not its own inverse! As a result, you will often see the S\u2020-gate, (also \u201cS-dagger\u201d, \u201cSdg\u201d or \\sqrt{Z}^\\dagger-gate). The S\u2020-gate is clearly an P-gate with \\phi = -\\pi/2: \\begin{align*} S = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{\\frac{i\\pi}{2}} \\end{bmatrix}, \\quad S^\\dagger = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{-\\frac{i\\pi}{2}} \\end{bmatrix}\\end{align*} The name \"\\sqrt{Z}-gate\" is due to the fact that two successively applied S-gates has the same effect as one Z-gate: \\begin{align*} SS|q\\rangle = Z|q\\rangle \\end{align*} This notation is common throughout quantum computing. To add an S-gate in Qiskit:     \\begin{align*} T = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{\\frac{i\\pi}{4}} \\end{bmatrix}, \\quad T^\\dagger = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{-\\frac{i\\pi}{4}} \\end{bmatrix}\\end{align*} As with the S-gate, the T-gate is sometimes also known as the \\sqrt[4]{Z}-gate. In Qiskit: You can use the widget below to play around with all the gates introduced in this chapter so far:   As we saw earlier, the I, Z, S & T-gates were all special cases of the more general P-gate. In the same way, the U-gate is the most general of all single-qubit quantum gates. It is a parametrised gate of the form: \\begin{align*}U(\\theta, \\phi, \\lambda) = \\begin{bmatrix} \\cos(\\frac{\\theta}{2}) & -e^{i\\lambda}\\sin(\\frac{\\theta}{2}) \\\\ e^{i\\phi}\\sin(\\frac{\\theta}{2}) & e^{i(\\phi+\\lambda)}\\cos(\\frac{\\theta}{2}) \\end{bmatrix}\\end{align*} Every gate in this chapter could be specified as U(\\theta,\\phi,\\lambda), but it is unusual to see this in a circuit diagram, possibly due to the difficulty in reading this. As an example, we see some specific cases of the U-gate in which it is equivalent to the H-gate and P-gate respectively. \\begin{align*}\\begin{aligned} U(\\tfrac{\\pi}{2}, 0, \\pi) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} = H & \\quad & U(0, 0, \\lambda) = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{i\\lambda}\\\\ \\end{bmatrix} = P \\end{aligned}\\end{align*} It should be obvious from this that there are an infinite number of possible gates, and that this also includes Rx and Ry-gates, although they are not mentioned here. It must also be noted that there is nothing special about the Z-basis, except that it has been selected as the standard computational basis. Qiskit also provides the X equivalent of the S and Sdg-gate i.e. the SX-gate and SXdg-gate respectively. These gates do a quarter-turn with respect to the X-axis around the Block sphere and are a special case of the Rx-gate. Before running on real IBM quantum hardware, all single-qubit operations are compiled down to I , X, SX and R_{z} . For this reason they are sometimes called the physical gates.   You can find a community-created cheat-sheet with some of the common quantum gates, and their properties here. In the previous section we looked at all the possible states a qubit could be in. We saw that qubits could be represented by 2D vectors, and that their states are limited to the form: \\begin{align*} |q\\rangle = \\cos{(\\tfrac{\\theta}{2})}|0\\rangle + e^{i\\phi}\\sin{\\tfrac{\\theta}{2}}|1\\rangle \\end{align*} Where \\theta and \\phi are real numbers. In this section we will cover gates, the operations that change a qubit between these states. Due to the number of gates and the similarities between them, this chapter is at risk of becoming a list. To counter this, we have included a few digressions to introduce important ideas at appropriate places throughout the chapter.  In The Atoms of Computation we came across some gates and used them to perform a classical computation. An important feature of quantum circuits is that, between initializing the qubits and measuring them, the operations (gates) are always reversible! These reversible gates can be represented as matrices, and as rotations around the Bloch sphere.          \\begin{align*} X = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = |0\\rangle\\langle1| + |1\\rangle\\langle0| \\end{align*} To see the effect a gate has on a qubit, we simply multiply the qubit\u2019s statevector by the gate. We can see that the X-gate switches the amplitudes of the states |0\\rangle and |1\\rangle: \\begin{align*} X|0\\rangle = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = |1\\rangle\\end{align*}   \\begin{align*} M|v\\rangle = \\begin{bmatrix}a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix}v_0 \\\\ v_1 \\end{bmatrix} = \\begin{bmatrix}a\\cdot v_0 + b \\cdot v_1 \\\\ c \\cdot v_0 + d \\cdot v_1 \\end{bmatrix} \\end{align*} In quantum computing, we can write our matrices in terms of basis vectors: \\begin{align*}X = |0\\rangle\\langle1| + |1\\rangle\\langle0|\\end{align*} This can sometimes be clearer than using a box matrix as we can see what different multiplications will result in: \\begin{align*}\\begin{aligned} X|1\\rangle & = (|0\\rangle\\langle1| + |1\\rangle\\langle0|)|1\\rangle \\\\ & = |0\\rangle\\langle1|1\\rangle + |1\\rangle\\langle0|1\\rangle \\\\ & = |0\\rangle \\times 1 + |1\\rangle \\times 0 \\\\ & = |0\\rangle \\end{aligned}\\end{align*} In fact, when we see a ket and a bra multiplied like this: \\begin{align*} |a\\rangle\\langle b| \\end{align*} this is called the outer product, which follows the rule: \\begin{align*} |a\\rangle\\langle b| = \\begin{bmatrix} a_0 b_0 & a_0 b_1 & \\dots & a_0 b_n\\\\ a_1 b_0 & \\ddots & & \\vdots \\\\ \\vdots & & \\ddots & \\vdots \\\\ a_n b_0 & \\dots & \\dots & a_n b_n \\\\ \\end{bmatrix}\\end{align*} We can see this does indeed result in the X-matrix as seen above: \\begin{align*} |0\\rangle\\langle1| + |1\\rangle\\langle0| = \\begin{bmatrix}0 & 1 \\\\ 0 & 0 \\end{bmatrix} + \\begin{bmatrix}0 & 0 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix}0 & 1 \\\\ 1 & 0 \\end{bmatrix} = X\\end{align*} In Qiskit, we can create a short circuit to verify this: Let's see the result of the above circuit. Note: Here we use plot_bloch_multivector() which takes a qubit's statevector instead of the Bloch vector. We can indeed see the state of the qubit is |1\\rangle as expected. We can think of this as a rotation by \\pi radians around the x-axis of the Bloch sphere. The X-gate is also often called a NOT-gate, referring to its classical analogue.     \\begin{align*} Y = \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix} \\quad\\quad\\quad\\quad Z = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\end{align*} \\begin{align*} Y = -i|0\\rangle\\langle1| + i|1\\rangle\\langle0| \\quad\\quad Z = |0\\rangle\\langle0| - |1\\rangle\\langle1| \\end{align*} And, unsurprisingly, they also respectively perform rotations by \\pi2\\pi\\frac{\\pi}{2} around the y and z-axis of the Bloch sphere. Below is a widget that displays a qubit\u2019s state on the Bloch sphere, pressing one of the buttons will perform the gate on the qubit: In Qiskit, we can apply the Y and Z-gates to our circuit using:     \\begin{align*} M|v\\rangle = |v'\\rangle \\leftarrow \\text{new vector} \\end{align*} If we chose the right vectors and matrices, we can find a case in which this matrix multiplication is the same as doing a multiplication by a scalar: \\begin{align*} M|v\\rangle = \\lambda|v\\rangle \\end{align*} (Above, M is a matrix, and \\lambda is a scalar). For a matrix M, any vector that has this property is called an eigenvector of M. For example, the eigenvectors of the Z-matrix are the states |0\\rangle and |1\\rangle: \\begin{align*} \\begin{aligned} Z|0\\rangle & = |0\\rangle \\\\ Z|1\\rangle & = -|1\\rangle \\end{aligned} \\end{align*} Since we use vectors to describe the state of our qubits, we often call these vectors eigenstates in this context. Eigenvectors are very important in quantum computing, and it is important you have a solid grasp of them. You may also notice that the Z-gate appears to have no effect on our qubit when it is in either of these two states. This is because the states |0\\rangle and |1\\rangle are the two eigenstates of the Z-gate. In fact, the computational basis (the basis formed by the states |0\\rangle and |1\\rangle) is often called the Z-basis. This is not the only basis we can use, a popular basis is the X-basis, formed by the eigenstates of the X-gate. We call these two vectors |+\\rangle and |-\\rangle: \\begin{align*} |+\\rangle = \\tfrac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\end{align*} \\begin{align*} |-\\rangle = \\tfrac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\end{align*} Another less commonly used basis is that formed by the eigenstates of the Y-gate. These are called: \\begin{align*} |\\circlearrowleft\\rangle, \\quad |\\circlearrowright\\rangle\\end{align*} We leave it as an exercise to calculate these. There are in fact an infinite number of bases; to form one, we simply need two orthogonal vectors. The eigenvectors of both Hermitian and unitary matrices form a basis for the vector space. Due to this property, we can be sure that the eigenstates of the X-gate and the Y-gate indeed form a basis for 1-qubit states (read more about this in the linear algebra page in the appendix)     Using only the Pauli-gates it is impossible to move our initialized qubit to any state other than |0\\rangle or |1\\rangle, i.e. we cannot achieve superposition. This means we can see no behaviour different to that of a classical bit. To create more interesting states we will need more gates!   The Hadamard gate (H-gate) is a fundamental quantum gate. It allows us to move away from the poles of the Bloch sphere and create a superposition of |0\\rangle and |1\\rangle. It has the matrix: \\begin{align*} H = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\end{align*} We can see that this performs the transformations below: \\begin{align*} H|0\\rangle = |+\\rangle \\end{align*} \\begin{align*} H|1\\rangle = |-\\rangle \\end{align*} This can be thought of as a rotation around the Bloch vector [1,0,1] (the line between the x & z-axis), or as transforming the state of the qubit between the X and Z bases. You can play around with these gates using the widget below:         As an example, let\u2019s try measuring in the X-basis. We can calculate the probability of measuring either |+\\rangle or |-\\rangle: \\begin{align*} p(|+\\rangle) = |\\langle+|q\\rangle|^2, \\quad p(|-\\rangle) = |\\langle-|q\\rangle|^2 \\end{align*} And after measurement, the superposition is destroyed. Since Qiskit only allows measuring in the Z-basis, we must create our own using Hadamard gates: In the quick exercises above, we saw you could create an X-gate by sandwiching our Z-gate between two H-gates: \\begin{align*} X = HZH \\end{align*} Starting in the Z-basis, the H-gate switches our qubit to the X-basis, the Z-gate performs a NOT in the X-basis, and the final H-gate returns our qubit to the Z-basis. We can verify this always behaves like an X-gate by multiplying the matrices: \\begin{align*}HZH = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} =X\\end{align*} Following the same logic, we have created an X-measurement by transforming from the X-basis to the Z-basis before our measurement. Since the process of measuring can have different effects depending on the system (e.g. some systems always return the qubit to |0\\rangle after measurement, whereas others may leave it as the measured state), the state of the qubit post-measurement is undefined and we must reset it if we want to use it again. There is another way to see why the Hadamard gate indeed takes us from the Z-basis to the X-basis. Suppose the qubit we want to measure in the X-basis is in the (normalized) state a |0\\rangle + b |1\\rangle. To measure it in X-basis, we first express the state as a linear combination of |+\\rangle and |-\\rangle. Using the relations |0\\rangle = \\frac{|+\\rangle + |-\\rangle}{\\sqrt{2}} and |1\\rangle = \\frac{|+\\rangle - |-\\rangle}{\\sqrt{2}}, the state becomes \\frac{a + b}{\\sqrt{2}}|+\\rangle + \\frac{a - b}{\\sqrt{2}}|-\\rangle. Observe that the probability amplitudes in X-basis can be obtained by applying a Hadamard matrix on the state vector expressed in Z-basis. Let\u2019s now see the results: We initialized our qubit in the state |-\\rangle, but we can see that, after the measurement, we have collapsed our qubit to the state |1\\rangle. If you run the cell again, you will see the same result, since along the X-basis, the state |-\\rangle is a basis state and measuring it along X will always yield the same result.     Measuring in different bases allows us to see Heisenberg\u2019s famous uncertainty principle in action. Having certainty of measuring a state in the Z-basis removes all certainty of measuring a specific state in the X-basis, and vice versa. A common misconception is that the uncertainty is due to the limits in our equipment, but here we can see the uncertainty is actually part of the nature of the qubit.  For example, if we put our qubit in the state |0\\rangle, our measurement in the Z-basis is certain to be |0\\rangle, but our measurement in the X-basis is completely random! Similarly, if we put our qubit in the state |-\\rangle, our measurement in the X-basis is certain to be |-\\rangle, but now any measurement in the Z-basis will be completely random. More generally: Whatever state our quantum system is in, there is always a measurement that has a deterministic outcome.  The introduction of the H-gate has allowed us to explore some interesting phenomena, but we are still very limited in our quantum operations. Let us now introduce a new type of gate:   The P-gate (phase gate) is parametrised, that is, it needs a number (\\phi) to tell it exactly what to do. The P-gate performs a rotation of \\phi around the Z-axis direction. It has the matrix form: \\begin{align*}P(\\phi) = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{i\\phi} \\end{bmatrix}\\end{align*} Where \\phi is a real number. You can use the widget below to play around with the P-gate, specify \\phi using the slider: In Qiskit, we specify a P-gate using p(phi, qubit): You may notice that the Z-gate is a special case of the P-gate, with \\phi = \\pi. In fact there are three more commonly referenced gates we will mention in this chapter, all of which are special cases of the P-gate:     First comes the I-gate (aka \u2018Id-gate\u2019 or \u2018Identity gate\u2019). This is simply a gate that does nothing. Its matrix is the identity matrix: \\begin{align*}I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix}\\end{align*} Applying the identity gate anywhere in your circuit should have no effect on the qubit state, so it\u2019s interesting this is even considered a gate. There are two main reasons behind this, one is that it is often used in calculations, for example: proving the X-gate is its own inverse: \\begin{align*} I = XX \\end{align*} The second, is that it is often useful when considering real hardware to specify a \u2018do-nothing\u2019 or \u2018none\u2019 operation.       The next gate to mention is the S-gate (sometimes known as the \\sqrt{Z}-gate), this is a P-gate with \\phi = \\pi/2. It does a quarter-turn around the Bloch sphere. It is important to note that unlike every gate introduced in this chapter so far, the S-gate is not its own inverse! As a result, you will often see the S\u2020-gate, (also \u201cS-dagger\u201d, \u201cSdg\u201d or \\sqrt{Z}^\\dagger-gate). The S\u2020-gate is clearly an P-gate with \\phi = -\\pi/2: \\begin{align*} S = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{\\frac{i\\pi}{2}} \\end{bmatrix}, \\quad S^\\dagger = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{-\\frac{i\\pi}{2}} \\end{bmatrix}\\end{align*} The name \"\\sqrt{Z}-gate\" is due to the fact that two successively applied S-gates has the same effect as one Z-gate: \\begin{align*} SS|q\\rangle = Z|q\\rangle \\end{align*} This notation is common throughout quantum computing. To add an S-gate in Qiskit:     \\begin{align*} T = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{\\frac{i\\pi}{4}} \\end{bmatrix}, \\quad T^\\dagger = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{-\\frac{i\\pi}{4}} \\end{bmatrix}\\end{align*} As with the S-gate, the T-gate is sometimes also known as the \\sqrt[4]{Z}-gate. In Qiskit: You can use the widget below to play around with all the gates introduced in this chapter so far:   As we saw earlier, the I, Z, S & T-gates were all special cases of the more general P-gate. In the same way, the U-gate is the most general of all single-qubit quantum gates. It is a parametrised gate of the form: \\begin{align*}U(\\theta, \\phi, \\lambda) = \\begin{bmatrix} \\cos(\\frac{\\theta}{2}) & -e^{i\\lambda}\\sin(\\frac{\\theta}{2}) \\\\ e^{i\\phi}\\sin(\\frac{\\theta}{2}) & e^{i(\\phi+\\lambda)}\\cos(\\frac{\\theta}{2}) \\end{bmatrix}\\end{align*} Every gate in this chapter could be specified as U(\\theta,\\phi,\\lambda), but it is unusual to see this in a circuit diagram, possibly due to the difficulty in reading this. As an example, we see some specific cases of the U-gate in which it is equivalent to the H-gate and P-gate respectively. \\begin{align*}\\begin{aligned} U(\\tfrac{\\pi}{2}, 0, \\pi) = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} = H & \\quad & U(0, 0, \\lambda) = \\begin{bmatrix} 1 & 0 \\\\ 0 & e^{i\\lambda}\\\\ \\end{bmatrix} = P \\end{aligned}\\end{align*} It should be obvious from this that there are an infinite number of possible gates, and that this also includes Rx and Ry-gates, although they are not mentioned here. It must also be noted that there is nothing special about the Z-basis, except that it has been selected as the standard computational basis. Qiskit also provides the X equivalent of the S and Sdg-gate i.e. the SX-gate and SXdg-gate respectively. These gates do a quarter-turn with respect to the X-axis around the Block sphere and are a special case of the Rx-gate. Before running on real IBM quantum hardware, all single-qubit operations are compiled down to I , X, SX and R_{z} . For this reason they are sometimes called the physical gates.   You can find a community-created cheat-sheet with some of the common quantum gates, and their properties here.   The case for quantum computers, simply put, is that they can solve certain problems that no classical computer ever could. To understand why this is, we first need to consider how much computational effort is required to solve certain problems. To begin, we can revisit the algorithm considered in the first section: adding two numbers. Adding two n-digit numbers can be done with a set of simple operations, each of which consists of just adding two single-digit numbers. To analyze the complexity of the procedure, we can think about how many of these basic additions are required and how this number depends on n. We'll refer to this number as c(n). In the easiest case, where we don't need to carry a 1 at any point, only n basic additions are required. In the worst case, we will need to perform n carry operations, each of which will require an extra basic addition. From these considerations, we can conclude that n \\leq c(n) \\leq 2n.   We can summarize this result by saying that c(n) grows linearly with n. More generally, we can say that a linear function of n can be found which acts as an upper bound for c(n) when n is large. Since this is a long and wordy sentence, we won't actually want to say this very often. Instead, we can express it more compactly using 'big O notation'.   \\begin{align*}f(x) \\leq M g(x) \\forall x>x_0. \\end{align*} Big O notation is useful as it allows us to compare how the resources/runtime required by an algorithm scale with input size, independent of the specific platform and algorithm implementation under consideration. Below are examples of common scaling factors of a runtime N as a function of input size n; it is clear that for a sufficiently large problem size the runtime of a O(a^n) algorithm will exceed that of a O(n^b) algorithm, where a and b are constants.   With this notation, the property described above is expressed simply as c(n) = O(n). This captures the linear behavior without needing to dwell on the specifics. Therefore, independent of whether c(n) = n, c(n) = 2n, or something else, we can simply say that c(n) = O(n). There is a hidden assumption in what we have considered so far. By talking about the number of digits, we have assumed the use of a specific number system. However, the number of digits will depend on which number system we are using, be it decimal, binary, or something else. For example, the number of bits n_2 required to express a number is related to the number of decimal digits n_{10} required to express the same number by n_2 = \\left\\lceil \\frac{\\log 10}{ \\log 2} , n_{10} \\right\\rceil \\approx 3.3 , n_{10}. Since this too is a linear relationship, it does not change how we express the complexity using big O notation. We can equally say that c(n_2) = O(n_2), c(n_{10}) = O(n_{10}), or even c(n_{10}) = O(n_{2}). It is for this reason that we can often simply speak of the number of digits, n, without needing to specify what number system is used.   Complexity theory is the study of the computational effort required to run any algorithm. By considering the best possible algorithm to solve a given problem, we can also study the computational effort inherent in solving this problem. For addition we already know the optimal algorithm, and so know that it is a problem with O(n) complexity. Multiplication is not quite so simple. Algorithms you learned at school for multiplying two n-digit numbers will have required O(n^2) basic operations, such as single-digit additions and multiplications. Though algorithms with lower asymptotic complexity have been found, it is widely regarded as impossible to perform multiplication with O(n) complexity. Even so, multiplication is far from being the most complex problem. An example of a problem with far greater complexity is factorization: taking an n-digit number and finding its prime factors. The best known algorithm in this case has a complexity that is worse than O\\left(e^{n^{1/3}}\\right). The exponential here means that the complexity grows very quickly and makes factorization a very hard problem to solve. To demonstrate this point using actual computation time, we can take a recent example.^{1} Consider the following 829-digit number. If you try using your computer to add or multiply numbers of this size, you'll find that it can solve such problems very quickly. If you multiply the number of processors your computer has with the number of seconds it takes to get the number of core-seconds, you are sure to find that very much less than 1 core-second is required. However, performing factorization on this number requires a supercomputer and around 2700 core-years, which eventually yields the following two factors. For the factorization of larger numbers, we easily get to a point where a planet-sized supercomputer would need to run for the age of the universe. Clearly, any such problem is practically impossible. So far we have considered only mathematical operations on n-digit numbers, with the complexity expressed as the number of simple single-digit operations required. However, complexity theory can be used to analyze any computational method for any kind of problem, be it searching databases, rendering graphics, simulating dynamics, or traversing a dungeon in Legend of Zelda. In each case, we are able to find a parameter or set of parameters that serve as our input size and express the complexity in terms of this input size using big O notation. For searching a database of N entries, for example, the complexity is O(N). Formally, defining the complexity of an algorithm depends on the exact theoretical model for computation we are using. Each model has a set of basic operations, known as primitive operations, with which any algorithm can be expressed. For Boolean circuits, as we considered in the first section, the primitive operations are the logic gates. For Turing machines, a hypothetical form of computer proposed by Alan Turing, we imagine a device stepping through and manipulating information stored on a tape. The RAM model has a more complex set of primitive operations and acts as an idealized form of the computers we use every day. All these are models of digital computation, based on discretized manipulations of discrete values. Different as they may seem from each other, it turns out that it is very easy for each of them to simulate the others. This means that in most cases the computational complexity does not significantly depend on which of these models is used. Rather than stating complexity specifically for the RAM model or Turing machines, we can therefore simply speak of the complexity for digital computers.   Though digital computers are dominant now, they are not the only form of computation. Analog computers were also widely studied and used in the past. Unlike the discrete values of digital computers, these are based on precise manipulations of continuously varying parameters. It has sometimes been claimed that such devices could quickly solve problems that are intractable for digital computers. However, such claims have never been realized. A major stumbling block for analog computers is the inability to build devices with arbitrarily high precision. In digital computers, the discretization means that errors must be relatively large in order to be noticeable, and methods for detecting and correcting such errors can then be implemented. In analog computers, however, errors can be arbitrarily small and impossible to detect, but still their effects can build up to ruin a computation. If one were to propose an ideal model of computation, it might seek to combine the robustness of a digital computer with the subtle manipulations of an analog computer. To achieve this we can look to quantum mechanics. We have already seen that qubits are a system with discrete outputs 0 and 1, and yet can exist in states that can only be described by continuous parameters. This is a particular instance of the well-known notion of 'wave-particle' duality that is typical of quantum systems. They cannot be fully described as either discrete or continuous, but rather a combination of the two. As Einstein said,^{2}   A quantum computer, whose primitive operations are gates applied to qubits, is therefore neither analog nor digital, but something unique. In further chapters we will explore the consequences of this unique nature. We will see that quantum computers can solve problems with a radically different complexity to digital computers. In fact, quantum computing is the only known technology that can be exponentially faster than classical computers for certain tasks, potentially reducing calculation times from years to minutes. We will also explore how quantum error correction can remove the effects of any imperfections.   With qubits and quantum gates, we can design novel algorithms that are fundamentally different from digital and analog classical ones. In this way, we hope to find solutions to problems that are intractable for classical computers. One way in which this can be done is when we have some function for which we want to determine a global property. For example, if we want to find the value of some parameter x for which some function f(x) is a minimum, or the period of the function if f(x) is periodic. An algorithm on a digital computer might use a process in which f(x) is computed for a variety of different inputs in order to get sufficient information about the global property. With a quantum computer, however, the fact that we can create superposition states means that the function can be applied to many possible inputs simultaneously. This does not mean that we can access all possible outputs since measurement of such a state simply gives us a single result. However, we can instead seek to induce a quantum interference effect, which will reveal the global property we require. This general description illustrates the workings of many of the quantum algorithms that have already been discovered. One prominent example is Grover's algorithm, which reduces the complexity of searching through N items from O(N) to O(N^{1/2}). This quadratic speedup could be useful in many applications with tasks that can be expressed as an unstructured search, such as optimization problems and machine learning. An even more impressive speedup is obtained with Shor's algorithm, which analyses periodic functions at the heart of the factorization problem. This allows a quantum solution for factoring n-digit numbers with complexity O(n^3). This is a superpolynomial speedup compared with the complexity for digital computers, which is worse than O\\left(e^{n^{1/3}}\\right). Another approach towards quantum algorithms is to use quantum computers to solve quantum problems. As we will see in the next chapter, expressing a quantum state requires an amount of information that scales exponentially with the number of qubits. Just writing down the state of n qubits therefore becomes an intractable task for digital computers as n increases. However, for a quantum computer we just need n qubits to do the same job. This natural capability to express and manipulate quantum states allows us to study and better understand quantum systems of interest, such as molecules and fundamental particles. Applying and adapting quantum algorithms in different industries therefore has the promise of enabling disruptive use cases in business and science. These include breakthroughs in drug discovery, machine learning, materials discovery, option pricing, protein folding, and supply chain.^{3} Particularly promising are those problems for which classical algorithms face inherent scaling limits and which do not require a large classical dataset to be loaded. For quantum advantage, a given problem's answers need to strongly depend on exponentially many entangled degrees of freedom with structure such that quantum mechanics evolves to a solution without having to go through all paths. Note, however, that the precise relationship between problems that are 'easy' for quantum computers (solvable in polynomial time) and other complexity-theoretic classes is still an open question.^{4} This is just a taste of how quantum algorithms can perform computation in an unique way. More details on these approaches can be found in later chapters. But first we need to look beyond the single qubit and invest some time into understanding the full set of quantum gates that we will need. This is the focus of the next chapter.     The case for quantum computers, simply put, is that they can solve certain problems that no classical computer ever could. To understand why this is, we first need to consider how much computational effort is required to solve certain problems. To begin, we can revisit the algorithm considered in the first section: adding two numbers. Adding two n-digit numbers can be done with a set of simple operations, each of which consists of just adding two single-digit numbers. To analyze the complexity of the procedure, we can think about how many of these basic additions are required and how this number depends on n. We'll refer to this number as c(n). In the easiest case, where we don't need to carry a 1 at any point, only n basic additions are required. In the worst case, we will need to perform n carry operations, each of which will require an extra basic addition. From these considerations, we can conclude that n \\leq c(n) \\leq 2n.   We can summarize this result by saying that c(n) grows linearly with n. More generally, we can say that a linear function of n can be found which acts as an upper bound for c(n) when n is large. Since this is a long and wordy sentence, we won't actually want to say this very often. Instead, we can express it more compactly using 'big O notation'.   \\begin{align*}f(x) \\leq M g(x) \\forall x>x_0. \\end{align*} Big O notation is useful as it allows us to compare how the resources/runtime required by an algorithm scale with input size, independent of the specific platform and algorithm implementation under consideration. Below are examples of common scaling factors of a runtime N as a function of input size n; it is clear that for a sufficiently large problem size the runtime of a O(a^n) algorithm will exceed that of a O(n^b) algorithm, where a and b are constants.   With this notation, the property described above is expressed simply as c(n) = O(n). This captures the linear behavior without needing to dwell on the specifics. Therefore, independent of whether c(n) = n, c(n) = 2n, or something else, we can simply say that c(n) = O(n). There is a hidden assumption in what we have considered so far. By talking about the number of digits, we have assumed the use of a specific number system. However, the number of digits will depend on which number system we are using, be it decimal, binary, or something else. For example, the number of bits n_2 required to express a number is related to the number of decimal digits n_{10} required to express the same number by n_2 = \\left\\lceil \\frac{\\log 10}{ \\log 2} , n_{10} \\right\\rceil \\approx 3.3 , n_{10}. Since this too is a linear relationship, it does not change how we express the complexity using big O notation. We can equally say that c(n_2) = O(n_2), c(n_{10}) = O(n_{10}), or even c(n_{10}) = O(n_{2}). It is for this reason that we can often simply speak of the number of digits, n, without needing to specify what number system is used.   Complexity theory is the study of the computational effort required to run any algorithm. By considering the best possible algorithm to solve a given problem, we can also study the computational effort inherent in solving this problem. For addition we already know the optimal algorithm, and so know that it is a problem with O(n) complexity. Multiplication is not quite so simple. Algorithms you learned at school for multiplying two n-digit numbers will have required O(n^2) basic operations, such as single-digit additions and multiplications. Though algorithms with lower asymptotic complexity have been found, it is widely regarded as impossible to perform multiplication with O(n) complexity. Even so, multiplication is far from being the most complex problem. An example of a problem with far greater complexity is factorization: taking an n-digit number and finding its prime factors. The best known algorithm in this case has a complexity that is worse than O\\left(e^{n^{1/3}}\\right). The exponential here means that the complexity grows very quickly and makes factorization a very hard problem to solve. To demonstrate this point using actual computation time, we can take a recent example.^{1} Consider the following 829-digit number. If you try using your computer to add or multiply numbers of this size, you'll find that it can solve such problems very quickly. If you multiply the number of processors your computer has with the number of seconds it takes to get the number of core-seconds, you are sure to find that very much less than 1 core-second is required. However, performing factorization on this number requires a supercomputer and around 2700 core-years, which eventually yields the following two factors. For the factorization of larger numbers, we easily get to a point where a planet-sized supercomputer would need to run for the age of the universe. Clearly, any such problem is practically impossible. So far we have considered only mathematical operations on n-digit numbers, with the complexity expressed as the number of simple single-digit operations required. However, complexity theory can be used to analyze any computational method for any kind of problem, be it searching databases, rendering graphics, simulating dynamics, or traversing a dungeon in Legend of Zelda. In each case, we are able to find a parameter or set of parameters that serve as our input size and express the complexity in terms of this input size using big O notation. For searching a database of N entries, for example, the complexity is O(N). Formally, defining the complexity of an algorithm depends on the exact theoretical model for computation we are using. Each model has a set of basic operations, known as primitive operations, with which any algorithm can be expressed. For Boolean circuits, as we considered in the first section, the primitive operations are the logic gates. For Turing machines, a hypothetical form of computer proposed by Alan Turing, we imagine a device stepping through and manipulating information stored on a tape. The RAM model has a more complex set of primitive operations and acts as an idealized form of the computers we use every day. All these are models of digital computation, based on discretized manipulations of discrete values. Different as they may seem from each other, it turns out that it is very easy for each of them to simulate the others. This means that in most cases the computational complexity does not significantly depend on which of these models is used. Rather than stating complexity specifically for the RAM model or Turing machines, we can therefore simply speak of the complexity for digital computers.   Though digital computers are dominant now, they are not the only form of computation. Analog computers were also widely studied and used in the past. Unlike the discrete values of digital computers, these are based on precise manipulations of continuously varying parameters. It has sometimes been claimed that such devices could quickly solve problems that are intractable for digital computers. However, such claims have never been realized. A major stumbling block for analog computers is the inability to build devices with arbitrarily high precision. In digital computers, the discretization means that errors must be relatively large in order to be noticeable, and methods for detecting and correcting such errors can then be implemented. In analog computers, however, errors can be arbitrarily small and impossible to detect, but still their effects can build up to ruin a computation. If one were to propose an ideal model of computation, it might seek to combine the robustness of a digital computer with the subtle manipulations of an analog computer. To achieve this we can look to quantum mechanics. We have already seen that qubits are a system with discrete outputs 0 and 1, and yet can exist in states that can only be described by continuous parameters. This is a particular instance of the well-known notion of 'wave-particle' duality that is typical of quantum systems. They cannot be fully described as either discrete or continuous, but rather a combination of the two. As Einstein said,^{2}   A quantum computer, whose primitive operations are gates applied to qubits, is therefore neither analog nor digital, but something unique. In further chapters we will explore the consequences of this unique nature. We will see that quantum computers can solve problems with a radically different complexity to digital computers. In fact, quantum computing is the only known technology that can be exponentially faster than classical computers for certain tasks, potentially reducing calculation times from years to minutes. We will also explore how quantum error correction can remove the effects of any imperfections.   With qubits and quantum gates, we can design novel algorithms that are fundamentally different from digital and analog classical ones. In this way, we hope to find solutions to problems that are intractable for classical computers. One way in which this can be done is when we have some function for which we want to determine a global property. For example, if we want to find the value of some parameter x for which some function f(x) is a minimum, or the period of the function if f(x) is periodic. An algorithm on a digital computer might use a process in which f(x) is computed for a variety of different inputs in order to get sufficient information about the global property. With a quantum computer, however, the fact that we can create superposition states means that the function can be applied to many possible inputs simultaneously. This does not mean that we can access all possible outputs since measurement of such a state simply gives us a single result. However, we can instead seek to induce a quantum interference effect, which will reveal the global property we require. This general description illustrates the workings of many of the quantum algorithms that have already been discovered. One prominent example is Grover's algorithm, which reduces the complexity of searching through N items from O(N) to O(N^{1/2}). This quadratic speedup could be useful in many applications with tasks that can be expressed as an unstructured search, such as optimization problems and machine learning. An even more impressive speedup is obtained with Shor's algorithm, which analyses periodic functions at the heart of the factorization problem. This allows a quantum solution for factoring n-digit numbers with complexity O(n^3). This is a superpolynomial speedup compared with the complexity for digital computers, which is worse than O\\left(e^{n^{1/3}}\\right). Another approach towards quantum algorithms is to use quantum computers to solve quantum problems. As we will see in the next chapter, expressing a quantum state requires an amount of information that scales exponentially with the number of qubits. Just writing down the state of n qubits therefore becomes an intractable task for digital computers as n increases. However, for a quantum computer we just need n qubits to do the same job. This natural capability to express and manipulate quantum states allows us to study and better understand quantum systems of interest, such as molecules and fundamental particles. Applying and adapting quantum algorithms in different industries therefore has the promise of enabling disruptive use cases in business and science. These include breakthroughs in drug discovery, machine learning, materials discovery, option pricing, protein folding, and supply chain.^{3} Particularly promising are those problems for which classical algorithms face inherent scaling limits and which do not require a large classical dataset to be loaded. For quantum advantage, a given problem's answers need to strongly depend on exponentially many entangled degrees of freedom with structure such that quantum mechanics evolves to a solution without having to go through all paths. Note, however, that the precise relationship between problems that are 'easy' for quantum computers (solvable in polynomial time) and other complexity-theoretic classes is still an open question.^{4} This is just a taste of how quantum algorithms can perform computation in an unique way. More details on these approaches can be found in later chapters. But first we need to look beyond the single qubit and invest some time into understanding the full set of quantum gates that we will need. This is the focus of the next chapter.     The case for quantum computers, simply put, is that they can solve certain problems that no classical computer ever could. To understand why this is, we first need to consider how much computational effort is required to solve certain problems. To begin, we can revisit the algorithm considered in the first section: adding two numbers. Adding two n-digit numbers can be done with a set of simple operations, each of which consists of just adding two single-digit numbers. To analyze the complexity of the procedure, we can think about how many of these basic additions are required and how this number depends on n. We'll refer to this number as c(n). In the easiest case, where we don't need to carry a 1 at any point, only n basic additions are required. In the worst case, we will need to perform n carry operations, each of which will require an extra basic addition. From these considerations, we can conclude that n \\leq c(n) \\leq 2n.   We can summarize this result by saying that c(n) grows linearly with n. More generally, we can say that a linear function of n can be found which acts as an upper bound for c(n) when n is large. Since this is a long and wordy sentence, we won't actually want to say this very often. Instead, we can express it more compactly using 'big O notation'.   \\begin{align*}f(x) \\leq M g(x) \\forall x>x_0. \\end{align*} Big O notation is useful as it allows us to compare how the resources/runtime required by an algorithm scale with input size, independent of the specific platform and algorithm implementation under consideration. Below are examples of common scaling factors of a runtime N as a function of input size n; it is clear that for a sufficiently large problem size the runtime of a O(a^n) algorithm will exceed that of a O(n^b) algorithm, where a and b are constants.   With this notation, the property described above is expressed simply as c(n) = O(n). This captures the linear behavior without needing to dwell on the specifics. Therefore, independent of whether c(n) = n, c(n) = 2n, or something else, we can simply say that c(n) = O(n). There is a hidden assumption in what we have considered so far. By talking about the number of digits, we have assumed the use of a specific number system. However, the number of digits will depend on which number system we are using, be it decimal, binary, or something else. For example, the number of bits n_2 required to express a number is related to the number of decimal digits n_{10} required to express the same number by n_2 = \\left\\lceil \\frac{\\log 10}{ \\log 2} , n_{10} \\right\\rceil \\approx 3.3 , n_{10}. Since this too is a linear relationship, it does not change how we express the complexity using big O notation. We can equally say that c(n_2) = O(n_2), c(n_{10}) = O(n_{10}), or even c(n_{10}) = O(n_{2}). It is for this reason that we can often simply speak of the number of digits, n, without needing to specify what number system is used.   Complexity theory is the study of the computational effort required to run any algorithm. By considering the best possible algorithm to solve a given problem, we can also study the computational effort inherent in solving this problem. For addition we already know the optimal algorithm, and so know that it is a problem with O(n) complexity. Multiplication is not quite so simple. Algorithms you learned at school for multiplying two n-digit numbers will have required O(n^2) basic operations, such as single-digit additions and multiplications. Though algorithms with lower asymptotic complexity have been found, it is widely regarded as impossible to perform multiplication with O(n) complexity. Even so, multiplication is far from being the most complex problem. An example of a problem with far greater complexity is factorization: taking an n-digit number and finding its prime factors. The best known algorithm in this case has a complexity that is worse than O\\left(e^{n^{1/3}}\\right). The exponential here means that the complexity grows very quickly and makes factorization a very hard problem to solve. To demonstrate this point using actual computation time, we can take a recent example.^{1} Consider the following 829-digit number. If you try using your computer to add or multiply numbers of this size, you'll find that it can solve such problems very quickly. If you multiply the number of processors your computer has with the number of seconds it takes to get the number of core-seconds, you are sure to find that very much less than 1 core-second is required. However, performing factorization on this number requires a supercomputer and around 2700 core-years, which eventually yields the following two factors. For the factorization of larger numbers, we easily get to a point where a planet-sized supercomputer would need to run for the age of the universe. Clearly, any such problem is practically impossible. So far we have considered only mathematical operations on n-digit numbers, with the complexity expressed as the number of simple single-digit operations required. However, complexity theory can be used to analyze any computational method for any kind of problem, be it searching databases, rendering graphics, simulating dynamics, or traversing a dungeon in Legend of Zelda. In each case, we are able to find a parameter or set of parameters that serve as our input size and express the complexity in terms of this input size using big O notation. For searching a database of N entries, for example, the complexity is O(N). Formally, defining the complexity of an algorithm depends on the exact theoretical model for computation we are using. Each model has a set of basic operations, known as primitive operations, with which any algorithm can be expressed. For Boolean circuits, as we considered in the first section, the primitive operations are the logic gates. For Turing machines, a hypothetical form of computer proposed by Alan Turing, we imagine a device stepping through and manipulating information stored on a tape. The RAM model has a more complex set of primitive operations and acts as an idealized form of the computers we use every day. All these are models of digital computation, based on discretized manipulations of discrete values. Different as they may seem from each other, it turns out that it is very easy for each of them to simulate the others. This means that in most cases the computational complexity does not significantly depend on which of these models is used. Rather than stating complexity specifically for the RAM model or Turing machines, we can therefore simply speak of the complexity for digital computers.   Though digital computers are dominant now, they are not the only form of computation. Analog computers were also widely studied and used in the past. Unlike the discrete values of digital computers, these are based on precise manipulations of continuously varying parameters. It has sometimes been claimed that such devices could quickly solve problems that are intractable for digital computers. However, such claims have never been realized. A major stumbling block for analog computers is the inability to build devices with arbitrarily high precision. In digital computers, the discretization means that errors must be relatively large in order to be noticeable, and methods for detecting and correcting such errors can then be implemented. In analog computers, however, errors can be arbitrarily small and impossible to detect, but still their effects can build up to ruin a computation. If one were to propose an ideal model of computation, it might seek to combine the robustness of a digital computer with the subtle manipulations of an analog computer. To achieve this we can look to quantum mechanics. We have already seen that qubits are a system with discrete outputs 0 and 1, and yet can exist in states that can only be described by continuous parameters. This is a particular instance of the well-known notion of 'wave-particle' duality that is typical of quantum systems. They cannot be fully described as either discrete or continuous, but rather a combination of the two. As Einstein said,^{2}   A quantum computer, whose primitive operations are gates applied to qubits, is therefore neither analog nor digital, but something unique. In further chapters we will explore the consequences of this unique nature. We will see that quantum computers can solve problems with a radically different complexity to digital computers. In fact, quantum computing is the only known technology that can be exponentially faster than classical computers for certain tasks, potentially reducing calculation times from years to minutes. We will also explore how quantum error correction can remove the effects of any imperfections.   With qubits and quantum gates, we can design novel algorithms that are fundamentally different from digital and analog classical ones. In this way, we hope to find solutions to problems that are intractable for classical computers. One way in which this can be done is when we have some function for which we want to determine a global property. For example, if we want to find the value of some parameter x for which some function f(x) is a minimum, or the period of the function if f(x) is periodic. An algorithm on a digital computer might use a process in which f(x) is computed for a variety of different inputs in order to get sufficient information about the global property. With a quantum computer, however, the fact that we can create superposition states means that the function can be applied to many possible inputs simultaneously. This does not mean that we can access all possible outputs since measurement of such a state simply gives us a single result. However, we can instead seek to induce a quantum interference effect, which will reveal the global property we require. This general description illustrates the workings of many of the quantum algorithms that have already been discovered. One prominent example is Grover's algorithm, which reduces the complexity of searching through N items from O(N) to O(N^{1/2}). This quadratic speedup could be useful in many applications with tasks that can be expressed as an unstructured search, such as optimization problems and machine learning. An even more impressive speedup is obtained with Shor's algorithm, which analyses periodic functions at the heart of the factorization problem. This allows a quantum solution for factoring n-digit numbers with complexity O(n^3). This is a superpolynomial speedup compared with the complexity for digital computers, which is worse than O\\left(e^{n^{1/3}}\\right). Another approach towards quantum algorithms is to use quantum computers to solve quantum problems. As we will see in the next chapter, expressing a quantum state requires an amount of information that scales exponentially with the number of qubits. Just writing down the state of n qubits therefore becomes an intractable task for digital computers as n increases. However, for a quantum computer we just need n qubits to do the same job. This natural capability to express and manipulate quantum states allows us to study and better understand quantum systems of interest, such as molecules and fundamental particles. Applying and adapting quantum algorithms in different industries therefore has the promise of enabling disruptive use cases in business and science. These include breakthroughs in drug discovery, machine learning, materials discovery, option pricing, protein folding, and supply chain.^{3} Particularly promising are those problems for which classical algorithms face inherent scaling limits and which do not require a large classical dataset to be loaded. For quantum advantage, a given problem's answers need to strongly depend on exponentially many entangled degrees of freedom with structure such that quantum mechanics evolves to a solution without having to go through all paths. Note, however, that the precise relationship between problems that are 'easy' for quantum computers (solvable in polynomial time) and other complexity-theoretic classes is still an open question.^{4} This is just a taste of how quantum algorithms can perform computation in an unique way. More details on these approaches can be found in later chapters. But first we need to look beyond the single qubit and invest some time into understanding the full set of quantum gates that we will need. This is the focus of the next chapter.   If you think quantum mechanics sounds challenging, you are not alone. All of our intuitions are based on day-to-day experiences, and so are better at understanding the behavior of balls and bananas than atoms or electrons. Though quantum objects can seem random and chaotic at first, they just follow a different set of rules. Once we know what those rules are, we can use them to create new and powerful technology. Quantum computing will be the most revolutionary example of this. To get you started on your journey towards quantum computing, let's test what you already know. Which of the following is the correct description of a bit? Actually, they are all correct: it's a very multi-purpose word! But if you chose the second one, it shows that you are already thinking along the right lines. The idea that information can be stored and processed as a series of 0s and 1s is quite a big conceptual hurdle, but it's something most people today know without even thinking about it. Taking this as a starting point, we can start to imagine bits that obey the rules of quantum mechanics. These quantum bits, or qubits, will then allow us to process information in new and different ways. We'll start diving deeper into the world of qubits. For this, we'll need some way of keeping track of what they are doing when we apply gates. The most powerful way to do this is to use the mathematical language of vectors and matrices. This chapter will be most effective for readers who are already familiar with vectors and matrices. Those who aren't familiar will likely be fine too, though it might be useful to consult our Introduction to Linear Algebra for Quantum Computing from time to time. Since we will be using Qiskit, our Python-based framework for quantum computing, it would also be useful to know the basics of Python. Those who need a primer can consult the Introduction to Python and Jupyter notebooks. If you think quantum mechanics sounds challenging, you are not alone. All of our intuitions are based on day-to-day experiences, and so are better at understanding the behavior of balls and bananas than atoms or electrons. Though quantum objects can seem random and chaotic at first, they just follow a different set of rules. Once we know what those rules are, we can use them to create new and powerful technology. Quantum computing will be the most revolutionary example of this. To get you started on your journey towards quantum computing, let's test what you already know.   Which of the following is the correct description of a bit? Actually, they are all correct: it's a very multi-purpose word! But if you chose \"the smallest unit of information\", it shows that you are already thinking along the right lines. The idea that information can be stored and processed as a series of 0s and 1s is quite a big conceptual hurdle, but it's something most people today know without even thinking about it. Taking this as a starting point, we can start to imagine bits that obey the rules of quantum mechanics. These quantum bits, or qubits, will then allow us to process information in new and different ways. We'll start diving deeper into the world of qubits. For this, we'll need some way of keeping track of what they are doing when we apply gates. The most powerful way to do this is to use the mathematical language of vectors and matrices. This chapter will be most effective for readers who are already familiar with vectors and matrices. Those who aren't familiar will likely be fine too, though it might be useful to consult our Introduction to Linear Algebra for Quantum Computing from time to time. Since we will be using Qiskit, our Python-based framework for quantum computing, it would also be useful to know the basics of Python. Those who need a primer can consult the Introduction to Python and Jupyter notebooks.   Linear algebra is the language of quantum computing. It is therefore crucial to develop a good understanding of the basic mathematical concepts that linear algebra is built upon, in order to arrive at many of the amazing and interesting constructions seen in quantum computation. The goal of this section is to create a foundation of introductory linear algebra knowledge, upon which the reader can build during their study of quantum computing.   We will start our investigation into introductory linear algebra by first discussing one of the most important mathematical quantities in quantum computation: the vector. Formally, a vector |v\\rangle is defined as elements of a set known as a vector space. A more intuitive and geometric definition is that a vector \"is a mathematical quantity with both direction and magnitude\". For instance, consider a vector with x and y components of the form \\begin{pmatrix} 3 \u00a05 \\end{pmatrix}. This vector can be visualized as an arrow pointing in the direction of 3 units along the x axis and 5 units along the y axis: Note that \"tail\" of the vector doesn't have to be positioned at the origin; it only needs to point in the correct direction.  In quantum computing, we often deal with state vectors, which are simply vectors that point to a specific point in space that corresponds to a particular quantum state. This can be visualized using a Bloch sphere. For instance, a vector representing the state of a quantum system could look something like this arrow, enclosed inside the Bloch sphere, which is the so-called \"state space\" of all possible points to which our state vectors can \"point\": This particular state corresponds to an even superposition between |0\\rangle and |1\\rangle (the arrow is halfway between |0\\rangle at the top and |1\\rangle at the bottom of the sphere). Our vectors are allowed to rotate anywhere on the surface of the sphere, and each of these points represents a different quantum state. Let's revisit our more formal definition of a vector, which is that a vector is an element of a vector space. We must now define a vector space. A vector space V over a field F is a set of objects (vectors), where two conditions hold. Firstly, vector addition of two vectors |a\\rangle, \u00a0|b\\rangle \u00a0\\in \u00a0V will yield a third vector |a\\rangle \u00a0+ \u00a0|b\\rangle \u00a0= \u00a0|c\\rangle, also contained in V. The second condition is that scalar multiplication between some |a\\rangle \u00a0\\in \u00a0V and some n \u00a0\\in \u00a0F, denoted by n|a\\rangle, is also contained within V. We will now clarify this previous definition by working through a basic example. Let us demonstrate that the set \\mathbb{R}^2 over the field \\mathbb{R} is a vector space. We assert that \\begin{align*}\\begin{pmatrix} x_1 \\\\ y_1 \\end{pmatrix} \\ + \\ \\begin{pmatrix} x_2 \\\\ y_2 \\end{pmatrix} \\ = \\ \\begin{pmatrix} x_1 \\ + \\ x_2 \\\\ y_1 \\ + \\ y_2 \\end{pmatrix}\\end{align*} is contained within \\mathbb{R}^2. This is evidently the case, as the sum of two real numbers is a real number, making both components of the newly-formed vector real numbers; thus, the vector is contained in \\mathbb{R}^2 by definition. We also assert that: \\begin{align*}n |v\\rangle \\ = \\ \\begin{pmatrix} nx \\\\ ny \\end{pmatrix} \\ \\in \\ V \\ \\ \\ \\ \\forall n \\ \\in \\ \\mathbb{R}\\end{align*} This is true as well, as the product of a real number and a real number is a real number, making the entire new vector real, and thus proving this statement.   Let's turn our attention to another fundamental concept: a matrix. Matrices are mathematical objects that transform vectors into other vectors: \\begin{align*}|v\\rangle \\ \\rightarrow \\ |v'\\rangle \\ = \\ M |v\\rangle\\end{align*} Generally, matrices are written as \"arrays\" of numbers, looking something like this: \\begin{align*}M \\ = \\ \\begin{pmatrix} 1 & -2 & 3 \\\\ 1 & 5i & 0 \\\\ 1 \\ + \\ i & 7 & -4 \\end{pmatrix}\\end{align*} We can \"apply\" a matrix to a vector by performing matrix multiplication. In general, matrix multiplication between two matrices involves taking the first row of the first matrix, and multiplying each element by its \"partner\" in the first column of the second matrix (the first number of the row is multiplied by the first number of the column, second number of the row and second number of column, etc.). The sum of these new numbers becomes the first element of the first row of the new matrix. To fill in the rest of the first row, we repeat this process for the second, third, etc. columns of the second matrix. Then we take the second row of the first matrix, and repeat the process for each column of the second matrix, to produce the second row. We perform this process until we have used all rows of the first matrix. The resulting matrix is our new matrix. Here is an example: \\begin{align*}\\begin{pmatrix} 2 & 0 \\\\ 5 & -1 \\end{pmatrix} \\begin{pmatrix} -3 & 1 \\\\ 2 & 1 \\end{pmatrix} \\ = \\ \\begin{pmatrix} (2)(-3) + (0)(2) & (2)(1) \\ + \\ (0)(1) \\\\ (5)(-3) + (-1)(2) & (5)(1) \\ + \\ (-1)(1) \\end{pmatrix} \\ = \\ \\begin{pmatrix} -6 & 2 \\\\ -17 & 4 \\end{pmatrix}\\end{align*} To perform a quantum computation, we have some quantum state vector we manipulate by applying a matrix to that vector. A vector is simply a matrix with one column. To apply a matrix to a vector, therefore, we follow the same matrix multiplication procedure described above. We manipulate qubits in our quantum computer by applying sequences of quantum gates. Each quantum gate can be expressed as a matrix that can be applied to state vectors, thus changing the state. For instance, a commonly seen quantum gate is the Pauli-X gate, which is represented by the following matrix: \\begin{align*}\\sigma_x \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\end{align*} This gate acts similarly to the classical NOT logic gate. It maps the computational basis state |0\\rangle to |1\\rangle and |1\\rangle to |0\\rangle (it \"flips\" the state). We write the two basis states as column vectors: \\begin{align*}|0\\rangle \\ = \\ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\ \\ \\ \\ \\ \\ \\ |1\\rangle \\ = \\ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\end{align*} When we apply this matrix to each of the vectors: \\begin{align*}\\sigma_x |0\\rangle \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\ = \\ \\begin{pmatrix} (0)(1) \\ + \\ (1)(0) \\\\ (1)(1) \\ + \\ (0)(0) \\end{pmatrix} \\ = \\ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\ = \\ |1\\rangle\\end{align*} \\begin{align*}\\sigma_x |1\\rangle \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\ = \\ \\begin{pmatrix} (0)(0) \\ + \\ (1)(1) \\\\ (1)(0) \\ + \\ (0)(1) \\end{pmatrix} \\ = \\ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\ = \\ |0\\rangle\\end{align*} The matrix acts on the state vectors as expected. Within quantum computation, we often encounter two important types of matrices: Hermitian and Unitary matrices. The former is more important in the study of quantum mechanics, but is still necessary to discuss in a study of quantum computation. The latter is of unparalleled importance in both quantum mechanics and quantum computation. If you take away only one concept from this section on linear algebra, it should be the concept of a unitary matrix. A Hermitian matrix is simply a matrix that is equal to its conjugate transpose (denoted with a \\dagger symbol). This means that flipping the sign of a Hermitian matrix's imaginary components, then reflecting its entries along its main diagonal (from the top left to bottom right corners), produces an equal matrix. For instance, the Pauli-Y matrix, commonly used in quantum computation, is Hermitian: \\begin{align*}\\sigma_y \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\ \\Rightarrow \\ \\sigma_y^{\\dagger} \\ = \\ \\begin{pmatrix} 0 & -(i) \\\\ -(-i) & 0 \\end{pmatrix} \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\ = \\ \\sigma_y\\end{align*} Notice how we switched the places of the i and the -i (as we reflect across the main diagonal, the zeroes remain unchanged), and then flipped the sign.  A unitary matrix is very similar. Specifically, it is a matrix such that the inverse matrix is equal to the conjugate transpose of the original matrix. The inverse of some matrix A, denoted as A^{-1}, is a matrix such that: \\begin{align*}A^{-1} A \\ = \\ A A^{-1} \\ = \\ \\mathbb{I}\\end{align*} where \\mathbb{I} is the identity matrix. The identity matrix has $1$s along the main diagonal (top left to bottom right), and $0$s in all other places. It is called the identity matrix because it acts trivially on any other matrix (it has no effect). You can prove this on your own by multiplying an identity matrix by any other matrix.  When matrices get larger than 2 \u00a0\\times \u00a02, calculating the inverse becomes sufficiently complicated that it is usually left to computers to calculate. For a 2 \u00a0\\times \u00a02 matrix, the inverse is defined as:  \\begin{align*}A \\ = \\ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\ \\Rightarrow \\ A^{-1} \\ = \\ \\frac{1}{\\text{det} \\ A} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix},\\end{align*} where \\text{det} \u00a0A is the determinant of the matrix. In the 2 \u00a0\\times \u00a02 case, \\text{det} \u00a0A \u00a0= \u00a0ad \u00a0- \u00a0bc. Calculating inverse matrices is rarely important in quantum computing. Since most of the matrices we encounter are unitary, we can assume that the inverse is simply given by taking the conjugate transpose. Let's look at a basic example. The Pauli-Y matrix, in addition to being Hermitian, is also unitary (it is equal to its conjugate transpose, which is also equal to its inverse; therefore, the Pauli-Y matrix is its own inverse!). We can verify that this matrix is in fact unitary: \\begin{align*}\\sigma_y \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\ \\ \\ \\ \\ \\sigma_y^{\\dagger} \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\ \\Rightarrow \\ \\sigma_y^{\\dagger} \\sigma_y \\ = \\ \\begin{pmatrix} (0)(0) + (-i)(i) & (0)(-i) \\ + \\ (-i)(0) \\\\ (i)(0) \\ + \\ (0)(i) & (i)(-i) \\ + \\ (0)(0) \\end{pmatrix} \\ = \\ \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\ = \\ \\mathbb{I}\\end{align*} The reason unitary matrices are important will become more apparent in the section on Hilbert spaces, and more so in the quantum mechanics subtopic of this textbook. The basic idea is that evolution of a quantum state by application of a unitary matrix \"preserves\" the norm (magnitude) of the quantum state.   We are now in a position to discuss the construction of vector spaces. Consider some vector space V. We say that some set of vectors S spans a subspace V_S \u00a0\\subset \u00a0V (subset closed under vector space operations) of the vector space, if we can write any vector in the subspace as a linear combination of vectors contained within the spanning set.  A linear combination of some collection vectors |v_1\\rangle, \u00a0..., \u00a0|v_n\\rangle in some vector space over a field F is defined as an arbitrary sum of these vectors (which of course will be another vector that we will call |v\\rangle): \\begin{align*}|v\\rangle \\ = \\ f_1 |v_1\\rangle \\ + \\ f_2 |v_2\\rangle \\ + \\ ... \\ + \\ f_n |v_n\\rangle \\ = \\ \\displaystyle\\sum_{i} \\ f_i |v_i\\rangle\\end{align*} where each f_i is some element of F. If we have a set of vectors that spans a space, we are saying that any other vector in the vector space can be written as a linear combination of these vectors. A set of vectors |v_1\\rangle, \u00a0..., \u00a0|v_n\\rangle is said to be linearly dependent if there exist corresponding coefficients for each vector, b_i \u00a0\\in \u00a0F, such that: \\begin{align*}b_1 |v_1\\rangle \\ + \\ b_2 |v_2\\rangle \\ + \\ ... \\ + \\ b_n |v_n\\rangle \\ = \\ \\displaystyle\\sum_{i} \\ b_i |v_i\\rangle \\ = \\ 0,\\end{align*} where at least one of the b_i coefficients is non-zero. This is equivalent to the more intuitive statement that \"the set of vectors can be expressed as linear combinations of each other\". For example, let us have the set  \\{|v_1\\rangle, \\ ..., \\ |v_n\\rangle\\}  along with the corresponding coefficients  \\{ b_1, \\ ..., \\ b_n \\} , such that the linear combination is equal to 0. Since there is at least one vector with a non-zero coefficient, we choose a term in the linear combination b_a |v_a\\rangle: \\begin{align*}\\displaystyle\\sum_{i} \\ b_i |v_i\\rangle \\ = \\ b_a |v_a\\rangle \\ + \\ \\displaystyle\\sum_{i, \\ i \\ \\neq \\ a} \\ b_i |v_i\\rangle \\ = \\ 0 \\ \\Rightarrow \\ |v_a\\rangle \\ = \\ - \\displaystyle\\sum_{i, \\ i \\ \\neq \\ a} \\ \\frac{b_i}{b_a} |v_i\\rangle \\ = \\ \\displaystyle\\sum_{i, \\ i \\ \\neq \\ a} \\ c_i |v_i\\rangle\\end{align*} In the case that b_a is the only non-zero coefficient, it is necessarily true that |v_a\\rangle is the null vector, automatically making the set linearly dependent. If this is not the case, |v_a\\rangle has been written as a linear combination of non-zero vectors, as was shown above. To prove the converse, we assume that there exists some vector |v_a\\rangle in the subspace |v_1\\rangle, ..., \u00a0|v_n\\rangle that can be written as a linear combination of other vectors in the subspace. This means that: \\begin{align*}|v_a\\rangle \\ = \\ \\displaystyle\\sum_{s} b_s |v_s\\rangle\\end{align*} where s is an index that runs over a subset of the subspace. It follows that: \\begin{align*}|v_a\\rangle \\ - \\ \\displaystyle\\sum_{s} b_s |v_s\\rangle \\ = \\ |v_a\\rangle \\ - \\ (b_1|v_{s_1}\\rangle \\ + \\ ... \\ + \\ b_r|v_{s_r}\\rangle) \\ = \\ 0\\end{align*} For all vectors in the subspace that are not included in the subset indexed by s, we set their coefficients, indexed by q, equal to 0. Thus, \\begin{align*}|v_a\\rangle \\ - \\ (b_1|v_{s_1}\\rangle \\ + \\ ... \\ + \\ b_r|v_{s_r}\\rangle) \\ + \\ (0)(|v_{q_1}\\rangle \\ + \\ ... \\ + \\ |v_{q_t}\\rangle) \\ = \\ 0\\end{align*} which is a linear combination of all elements in the subspace |v_1\\rangle, \u00a0..., \u00a0|v_n\\rangle. This is equal to 0, thus completing the proof that the two definitions of linear dependence imply each other. Let's now consider a basic example. Consider the set of two vectors in \\mathbb{R}^2, consisting of |a\\rangle \u00a0= \u00a0\\begin{pmatrix} 1 \u00a00 \\end{pmatrix} and |b\\rangle \u00a0= \u00a0\\begin{pmatrix} 2 \u00a00 \\end{pmatrix}. If we choose the field over our vector space to be \\mathbb{R}, then we can create a linear combination of these vectors that equates to 0. For example: \\begin{align*}2|a\\rangle \\ - \\ |b\\rangle \\ = \\ 0\\end{align*} A set of vectors is said to be linearly independent if there is no vector in the set that can be expressed as a linear combination of all the others. The notion of a basis is simply a linearly independent spanning set. In this sense, the basis of a vector space is the minimal possible set that spans the entire space. We call the size of the basis set the dimension of the vector space. Bases and spanning sets are important because they allow us to \"shrink down\" vector spaces and express them in terms of only a few vectors. We can come to certain conclusions about our basis set that we can generalize to the entire vector space, simply because we know every vector in the space is just a linear combination of the basis vectors.  In quantum computation, one of the bases that we often encounter is |0\\rangle, \u00a0|1\\rangle. We can write any other qubit state as a linear combination of these basis vectors. For instance, the linear combination \\begin{align*}\\frac{|0\\rangle \\ + \\ |1\\rangle}{\\sqrt{2}}\\end{align*} represents a superposition between the |0\\rangle and |1\\rangle basis state, with equal probability of measuring the state to be in either one of the basis vector states (this is intuitive, as the \"weight\" or the \"amount of each basis vector\" in the linear combination is equal, both being scaled by 1/\\sqrt{2}).   Hilbert Spaces are one of the most important mathematical constructs in quantum mechanics and quantum computation. A Hilbert space can be thought of as the state space in which all quantum state vectors \"live\". The main difference between a Hilbert space and any random vector space is that a Hilbert space is equipped with an inner product, which is an operation that can be performed between two vectors, returning a scalar.  In the context of quantum mechanics and quantum computation, the inner product between two state vectors returns a scalar quantity representing the amount to which the first vector lies along the second vector. From this, the probabilities of measurement in different quantum states (among other things) can be calculated (this will be discussed more in the quantum mechanics subtopic).  For two vectors |a\\rangle and |b\\rangle in a Hilbert space, we denote the inner product as \\langle a | b \\rangle, where \\langle a | is equal to the conjugate transpose of |a\\rangle, denoted |a\\rangle^{\\dagger}. Thus, the inner product between two vectors of the Hilbert space looks something like: \\begin{align*}\\langle a | b \\rangle \\ = \\ \\begin{pmatrix} a_1^{*} & a_2^{*} & ... & a_n^{*} \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ b_2 \\\\ . \\\\ . \\\\ . \\\\ b_n \\end{pmatrix} \\ = \\ a_1^{*} b_1 \\ + \\ a_2^{*} b_2 \\ + \\ ... \\ + \\ a_n^{*} b_n\\end{align*} where * denotes the complex conjugate of the vector. One of the most important conditions for a Hilbert space representing a quantum system is that the inner product of a vector with itself is equal to one: \\langle \\psi | \\psi \\rangle \u00a0= \u00a01. This is the so-called normalization condition, which states that the length of the vector squared (each component of the vector is squared and summed together, by definition of the inner product) must be equal to one. The physical significance of this is that the length of a vector in a particular direction is representative of the \"probability amplitude\" of the quantum system with regards to measurement in that particular state. Obviously, the probability of the quantum system being measured in the state that it is in must be 1 (after all, the sum of the probabilities of finding the quantum system in any particular state must equal 1). Let's consider the Bloch sphere: The surface of this sphere, along with the inner product between qubit state vectors, is a valid Hilbert space. In addition, the normalization condition holds true, as the radius of the Bloch sphere is 1, and thus the length squared of each vector must also equal 1. A final note regarding Hilbert spaces and the inner product is their relationship to unitary matrices. Unitary matrices are important in quantum computation because they preserve the inner product, meaning that no matter how you transform a vector under a sequence of unitary matrices, the normalization condition still holds true. This can be demonstrated in the following short proof: \\begin{align*}\\langle \\psi | \\psi \\rangle \\ = \\ 1 \\ \\Rightarrow \\ |\\psi\\rangle \\ \\rightarrow \\ U |\\psi\\rangle \\ = \\ |\\psi'\\rangle \\ \\Rightarrow \\ \\langle \\psi' | \\psi' \\rangle \\ = \\ (U |\\psi\\rangle)^{\\dagger} U|\\psi\\rangle \\ = \\ \\langle \\psi | U^{\\dagger} U |\\psi\\rangle \\ = \\ \\langle \\psi | \\psi \\rangle \\ = \\ 1\\end{align*} This means that unitary evolution sends quantum states to other valid quantum states. For a single-qubit Hilbert space, represented by the Bloch sphere, unitary transformations correspond to rotations of state vectors to different points on the sphere, not changing the length of the state vector in any way.   Inner products aren't the only way to multiply vectors. Occasionally, we'll switch the order of the bra and ket in order to take the outer product, whose outcome is a matrix, rather than a single number. For two vectors |a\\rangle and |b\\rangle in a Hilbert space, we denote the outer product as |a\\rangle \\langle b|, where \\langle b | is equal to the conjugate transpose of |b\\rangle, as before. This gets us: \\begin{align*}| a \\rangle \\langle b | \\ = \\ \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix} \\begin{pmatrix} b_1^{*} & b_2^{*} & \\cdots & b_n^{*} \\end{pmatrix} \\ = \\begin{pmatrix} a_1 b_1^{*} & a_1 b_2^{*} & \\cdots & a_1 b_n^{*} \\\\ a_2 b_1^{*} & a_2 b_2^{*} & & \\vdots \\\\ \\vdots & & \\ddots & \\vdots \\\\ a_n b_1^{*} & \\cdots & \\cdots & a_n b_n^{*} \\end{pmatrix}\\end{align*} Outer products give us a way to represent quantum gates with bras and kets, rather than matrices. For example, take the Pauli-X gate: \\begin{align*}\\sigma_x \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\end{align*} We can represent this as the sum |0\\rangle \\langle 1| + |1\\rangle \\langle 0|, since: \\begin{align*}|0\\rangle \\langle 1| \\ + \\ |1\\rangle \\langle 0| \\ = \\ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\ = \\ \\sigma_x \\end{align*} The outer product is, in fact, a specific example of the more general tensor product used to multiply vector spaces together.  Most often, you'll see the tensor product used to describe the shared state of two or more qubits. Notice here that the tensor product doesn't require taking one of the vector's conjugate transposes like the outer product does\u2014we're multiplying two kets together instead of a ket and a bra. The tensor product of vectors |a \\rangle and |b\\rangle, written |a\\rangle \\otimes |b\\rangle or |ab\\rangle, equals:  \\begin{align*} |a\\rangle \\otimes |b\\rangle \\ = \\ \\begin{pmatrix} a_{1} \\begin{pmatrix} b_{1} \\\\ b_{2} \\end{pmatrix} \\\\ a_{2} \\begin{pmatrix} b_{1} \\\\ b_{2} \\end{pmatrix} \\end{pmatrix} \\ = \\begin{pmatrix} a_{1} b_{1} \\\\ a_{1} b_{2} \\\\ a_{2} b_{1} \\\\ a_{2} b_{2} \\end{pmatrix} \\end{align*} If we want to act on the new vector produced by the tensor product of |a \\rangle and |b\\rangle, we'll have to take the tensor product of the operators we hope to act on them with as well. The tensor product of matrices A and B equals: \\begin{align*} A \\otimes B \\ = \\ \\begin{pmatrix} a_{11} B & \\cdots & a_{1n} B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} B & \\cdots & a_{mn} B \\end{pmatrix} \\end{align*} You can find examples of the tensor product in action here.   Consider the relationship of the form: \\begin{align*}A |v\\rangle \\ = \\ \\lambda |v\\rangle,\\end{align*} where A is a matrix, and \\lambda is some number. If we are given some matrix A, and need to find the vectors |v\\rangle and numbers \\lambda that satisfy this relationship, we call these vectors eigenvectors, and their corresponding number multipliers eigenvalues. Eigenvectors and eigenvalues have very important physical significance in the context of quantum mechanics, and therefore quantum computation. Given some A, we exploit an interesting trick in order to find the set of eigenvectors and corresponding eigenvalues. Let us rearrange our equation as: \\begin{align*}A |v\\rangle \\ - \\ \\lambda |v\\rangle \\ = 0 \\ \\Rightarrow \\ (A \\ - \\ \\lambda \\mathbb{I}) |v\\rangle \\ = \\ 0\\end{align*} If we multiply both sides of this equation by the inverse matrix (A \u00a0- \u00a0\\lambda \\mathbb{I})^{-1}, we get |v\\rangle \u00a0= \u00a00. This is an extraneous solution (we don't allow eigenvectors to be the null vector, or else any eigenvalue/matrix combination would satisfy the eigenvector-eigenvalue relationship). Thus, in order to find the allowed eigenvectors and eigenvalues, we have to assume that the matrix (A \u00a0- \u00a0\\lambda \\mathbb{I}) is non-invertible. Recall from earlier that the inverse of a matrix is of the form:  \\begin{align*}M^{-1} \\ = \\ \\frac{1}{\\text{det} (M)} \\ F(M),\\end{align*} where F(M) is some new matrix (the particulars of which do not matter in this context) that depends on M. The part of this equation in which we are interested is the inverse of the determinant. If the determinant of the matrix M is 0, it follows that the inverse is undefined, and thus so is the inverse, making the matrix M non-invertible. We therefore require that: \\begin{align*}\\text{det} (A \\ - \\ \\lambda \\mathbb{I}) \\ = \\ 0\\end{align*} From this, we can determine \\lambda, then we plug each value of \\lambda back into the original equation to get the eigenvectors. Let's do an example, and find the eigenvectors/eigenvalues of the Pauli-Z matrix, \\sigma_z. We start with: \\begin{align*}\\text{det} (\\sigma_z \\ - \\ \\lambda \\mathbb{I}) \\ = \\ \\text{det} \\begin{pmatrix} 1 \\ - \\ \\lambda & 0 \\\\ 0 & -1 \\ - \\ \\lambda \\end{pmatrix} \\ = \\ (-1 \\ - \\ \\lambda)(1 \\ - \\ \\lambda) \\ = \\lambda^2\\ - \\ \\ 1 \\ = \\ 0 \\ \\Rightarrow \\ \\lambda \\ = \\ \\pm 1\\end{align*} The equation, in terms of \\lambda, that results when solving the determinant is called the characteristic polynomial. We can then plug each of these values back into the original equation. We'll start with \\lambda \u00a0= \u00a01: \\begin{align*}\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} |v\\rangle \\ = \\ |v\\rangle \\ \\Rightarrow \\ \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\ = \\ \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\ \\Rightarrow \\begin{pmatrix} a \\\\ -b \\end{pmatrix} \\ = \\ \\begin{pmatrix} a \\\\ b \\end{pmatrix}\\end{align*} a can be any number, and b is 0; thus, the vector \\begin{pmatrix} 1 \u00a00 \\end{pmatrix} forms a basis for all vectors that satisfy our relationship, and is therefore the eigenvector that corresponds to the eigenvalue of 1. We do the same thing for \\lambda \u00a0= \u00a0-1: \\begin{align*}\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} |v\\rangle \\ = \\ -|v\\rangle \\ \\Rightarrow \\ \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\ = \\ \\begin{pmatrix} -a \\\\ -b \\end{pmatrix} \\ \\Rightarrow \\begin{pmatrix} a \\\\ -b \\end{pmatrix} \\ = \\ \\begin{pmatrix} -a \\\\ -b \\end{pmatrix}\\end{align*} This time, b can be any number, and a is 0; thus, our basis vector (and our eigenvector corresponding to -1) is \\begin{pmatrix} 0 \u00a01 \\end{pmatrix}. Notice how the eigenvectors of the Pauli-Z matrix are the quantum computational basis states |0\\rangle and |1\\rangle. This is no coincidence. For instance, when we measure a qubit in the Z-basis, we are referring to a measurement that collapses the qubit's state into one of the eigenvectors of the Z matrix, either |0\\rangle or |1\\rangle.  In fact, the following properties are very important in gate model of quantum computation, where we deal with finite dimensional vector spaces: A Hermitian matrix has linearly independent eigenvectors. The number of these eigenvectors is equal to the dimension of the vector space. In addition, when the corresponding eigenvalues are distinct, the eigenvectors are orthogonal. When the eigenvalues are the same, the eigenvectors are not orthogonal, but they are still linearly independent and can be orthogonalized. Therefore, eigenvectors of a Hermitian matrix form a basis for the vector space. Since a unitary matrix is a normal matrix, the eigenvectors of a unitary matrix form an orthonormal basis for the vector space. As an important special case, these can be verified for each of the Pauli matrices.   The notion of a matrix exponential is a very specific yet extremely important concept. We often see unitary transformations in the form:  \\begin{align*}U \\ = \\ e^{i\\gamma H},\\end{align*} where H is some Hermitian matrix and \\gamma is some real number. It is fairly simple to prove that all matrices of this form are unitary. Taking the conjugate transpose of U, we get: \\begin{align*}U^{\\dagger} \\ = \\ \\Big( e^{i\\gamma H} \\Big)^{\\dagger} \\ = \\ e^{-i \\gamma H^{\\dagger}}\\end{align*} But since H is Hermitian, we know that H^{\\dagger} \u00a0= \u00a0H, thus: \\begin{align*}e^{-i \\gamma H^{\\dagger}} \\ = \\ e^{-i \\gamma H} \\ \\Rightarrow \\ U^{\\dagger} U \\ = \\ e^{-i \\gamma H} e^{i\\gamma H} \\ = \\ \\mathbb{I}\\end{align*} You may wonder why a matrix inside of an exponential can still be considered a matrix. The answer becomes clearer when we expand our exponential function as a Taylor series. Recall from calculus that a Taylor series is essentially a way to write any function as an infinite-degree polynomial, and the main idea is to choose the terms of the polynomial and center it at some point x_0 lying on the function we are trying to transform into the polynomial, such that the zeroth, first, second, third, etc. derivative is the same for both the original function and the polynomial. Thus, we write our Taylor series in the form:  \\begin{align*}g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{(n)}(x_0) \\ \\frac{(x \\ - \\ x_0)^n}{n!},\\end{align*} where g(x) is the polynomial, f(x) is the original function, f^{(n)} is the n-th derivative of f, and x_0 is the point at which we center the function. Since we are not approximating, x_0 doesn't matter, so for simplicity, we choose x_0 \u00a0= \u00a00, and the Taylor series becomes a Maclaurin series: \\begin{align*}g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{(n)}(0) \\ \\frac{x^n}{n!}\\end{align*} If we choose f(x) \u00a0= \u00a0e^x, we can create an equivalent polynomial using the Maclaurin series. Since the derivative of e^x is simply e^x, and evidently, e^0 \u00a0= \u00a01, we get: \\begin{align*}g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{x^n}{n!} \\ = \\ e^x\\end{align*} Thus, for some matrix, i \\gamma H, we get: \\begin{align*}e^{i \\gamma H} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma H)^n}{n!}\\end{align*} Therefore, the exponential of a matrix is a matrix. It is an infinite sum of powers of matrices, which admittedly looks overly complex...but the point here is that the matrix exponential is indeed a matrix.  We are now in a position to demonstrate a very important fact: if we have some matrix B such that B^2 \u00a0= \u00a0\\mathbb{I} (this is called an involutory matrix), then: \\begin{align*}e^{i \\gamma B} \\ = \\ \\cos(\\gamma) \\mathbb{I} \\ + \\ i \\sin(\\gamma) B\\end{align*} We start with the Maclaurin series: \\begin{align*}e^{i \\gamma B} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma B)^n}{n!}\\end{align*} Notice that we can split the summation into an imaginary part and a real part, based on whether n is even or odd in each term of the sum: \\begin{align*}\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma B)^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n} B^{2n}}{(2n)!} \\ + \\ i \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1} B^{2n + 1}}{(2n + 1)!}\\end{align*} Now, let us find the Maclaurin series for both \\sin x and \\cos x. We'll start with f(x) \u00a0= \u00a0\\sin x: \\begin{align*}\\sin x \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!}\\end{align*} The derivative of \\sin x is cyclical in a sense (each arrow represents taking the derivative of the previous function): \\begin{align*}\\sin x \\ \\rightarrow \\ \\cos x \\ \\rightarrow \\ -\\sin x \\ \\rightarrow \\ -\\cos x \\ \\rightarrow \\ \\sin x\\end{align*} Since \\sin (0) \u00a0= \u00a00 and \\cos (0) \u00a0= \u00a01, all terms with even n become 0, and we get: \\begin{align*}\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n x^{2n \\ + \\ 1}}{(2n \\ + \\ 1)!}\\end{align*} This looks similar to the odd term of our original equation. In fact, if we let x \u00a0= \u00a0\\gamma B, they are exactly the same. We follow a process that is almost identical to show that the even terms are the same as the Maclaurin series for f(x) \u00a0= \u00a0\\cos x: \\begin{align*}\\cos x \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!}\\end{align*} \\begin{align*}\\Rightarrow \\ \\cos x \\ \\rightarrow \\ -\\sin x \\ \\rightarrow \\ -\\cos x \\ \\rightarrow \\ \\sin x \\ \\rightarrow \\ \\cos x\\end{align*} \\begin{align*}\\Rightarrow \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n x^{2n}}{(2n)!}\\end{align*} Let us go back to the original equation. Recall that B^2 \u00a0= \u00a0\\mathbb{I}. For any n, we have: \\begin{align*}B^{2n} \\ = \\ \\big( B^2 \\Big)^n \\ = \\ \\mathbb{I}^n \\ = \\ \\mathbb{I}\\end{align*} \\begin{align*}B^{2n \\ + \\ 1} \\ = \\ B \\ \\big( B^2 \\Big)^n \\ = \\ B \\ \\mathbb{I}^n \\ = \\ B \\ \\mathbb{I} \\ = \\ B\\end{align*} Substituting in this new information, we get: \\begin{align*}\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n} B^{2n}}{(2n)!} \\ + \\ i \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1} B^{2n + 1}}{(2n + 1)!} \\ = \\ \\mathbb{I} \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n}}{(2n)!} \\ + \\ i B \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1}}{(2n + 1)!} \\ = \\ \\cos (\\gamma) \\mathbb{I} \\ + \\ i \\sin (\\gamma) B\\end{align*} This fact is extremely useful in quantum computation. Consider the Pauli matrices: \\begin{align*}\\sigma_x \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\end{align*} \\begin{align*}\\sigma_y \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}\\end{align*} \\begin{align*}\\sigma_z \\ = \\ \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\\end{align*} These matrices are among the fundamental \"quantum gates\" used to manipulate qubits. These operations are not only unitary, they are also Hermitian and Involutory. This means that a matrix of the form e^{i \\gamma \\sigma_k} \u00a0k \u00a0\\in \u00a0\\{x, \u00a0y, \u00a0z\\} is not only a valid unitary matrix that can act upon a quantum state vector (a qubit), but it can be expressed using the sine-cosine relationship that we just proved. This is very powerful, and is seen throughout quantum computational theory, as gates of this type are used all the time. One last important fact about matrix exponentials: if we have some matrix M, with eigenvectors |v\\rangle and corresponding eigenvalues \\lambda, then: \\begin{align*}e^{M} |v\\rangle \\ = \\ e^\\lambda |v\\rangle\\end{align*} This one is much more straightforward to prove: \\begin{align*}e^M |v\\rangle \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{M^n |v\\rangle}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{\\lambda^n |v\\rangle}{n!} \\ = \\ e^\\lambda |v\\rangle\\end{align*} This fact is also very useful. When creating quantum circuits that simulate a certain Hamiltonian (especially for variational circuits), we frequently use gates of the form e^{i \\gamma \\sigma_z}. Since |0\\rangle and |1\\rangle are eigenvectors of \\sigma_z, we can easily determine mathematically that e^{i \\gamma \\sigma_z} will add a phase of e^{i \\gamma} to |0\\rangle, and will add a phase of e^{-i\\gamma} to |1\\rangle. We can then construct this gate in terms of CNOT and phase/rotation gates fairly easily, as we know the mathematical outcome of the gate on each of the computational basis states. This fact doesn't only apply to exponentials of the \\sigma_z gate. For example, we can determine the outcome of a gate of the form e^{i \\gamma \\sigma_x} on the eigenvectors of \\sigma_x, (|0\\rangle \u00a0+ \u00a0|1\\rangle)/\\sqrt{2} and (|0\\rangle \u00a0- \u00a0|1\\rangle)/\\sqrt{2}. The same applies to exponentials of the \\sigma_y matrix.     [2] A New Branch of Mathematics: The Ausdehnungslehre of 1844 and Other Works: Hermann Grassmann, Lloyd C. Kannenberg: 9780812692761 Python is a programming language where you don't need to compile. You can just run it line by line (which is how we can use it in a notebook). So if you are quite new to programming, Python is a great place to start. The current version is Python 3, which is what we'll be using here. One way to code in Python is to use a Jupyter notebook. This is probably the best way to combine programming, text and images. In a notebook, everything is laid out in cells. Text cells and code cells are the most common. If you are viewing this section as a Jupyter notebook, the text you are now reading is in a text cell. A code cell can be found just below. To run the contents of a code cell, you can click on it and press Shift + Enter. Or if there is a little arrow thing on the left, you can click on that. If you are viewing this section as a Jupyter notebook, execute each of the code cells as you read through. Above we created two variables, which we called a and b, and gave them values. Then we added them. Simple arithmetic like this is pretty straightforward in Python. Variables in Python come in many forms. Below are some examples. As well as numbers, another data structure we can use is the list. Lists in Python can contain any mixture of variable types. Lists are indexed from 0 in Python (unlike languages such as Fortran). So here's how you access the 42 at the beginning of the above list. A similar data structure is the tuple. A major difference between the list and the tuple is that list elements can be changed whereas tuple elements cannot Also, we can add an element to the end of a list, which we cannot do with tuples. Another useful data structure is the dictionary. This stores a set of values, each labeled by a unique key. Values can be any data type. Keys can be anything sufficiently simple (integer, float, Boolean, string). It cannot be a list, but it can be a tuple. The values are accessed using the keys New key/value pairs can be added by just supplying the new value for the new key To loop over a range of numbers, the syntax is Note that it starts at 0 (by default), and ends at n-1 for range(n). You can also loop over any 'iterable' object, such as lists or dictionaries Conditional statements are done with if, elif and else with the following syntax. Importing packages is done with a line such as The numpy package is important for doing maths We have to write numpy. in front of every numpy command so that it knows how to find that command defined in numpy. To save writing, it is common to use Then you only need the shortened name. Most people use np, but you can choose what you like. You can also pull everything straight out of numpy with Then you can use the commands directly. But this can cause packages to mess with each other, so use with caution. If you want to do trigonometry, linear algebra, etc, you can use numpy. For plotting, use matplotlib. For graph theory, use networkx. For quantum computing, use qiskit. For whatever you want, there will probably be a package to help you do it. A good thing to know about in any language is how to make a function. Here's a function, whose name was chosen to be do_some_maths, whose inputs are named Input1 and Input2 and whose output is named the_answer. It's used as follows If you give a function an object, and the function calls a method of that object to alter its state, the effect will persist. So if that's all you want to do, you don't need to return anything. For example, let's do it with the append method of a list. Randomness can be generated using the random package. These are the basics. Now all you need is a search engine, and the intuition to know who is worth listening to on Stack Exchange. Then you can do anything with Python. Your code might not be the most 'Pythonic', but only Pythonistas really care about that.", "url": "https://learn.qiskit.org/course/ch-prerequisites/introduction-to-python-and-jupyter-notebooks"}