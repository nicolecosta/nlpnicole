{"title": "What is Unsupervised Learning?  | IBM", "content": "Unsupervised learning, also known as\u00a0unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition. Train, validate, tune and deploy foundation and machine learning models, with ease Watson Studio IBM Cloud Pak for Data Unsupervised learning models are utilized for three main tasks\u2014clustering, association, and dimensionality reduction. Below we\u2019ll define each learning method and highlight common algorithms and approaches to conduct them effectively. Clustering is a data mining technique which groups unlabeled data based on their similarities or differences. Clustering algorithms are used to process raw, unclassified data objects into groups represented by structures or patterns in the information. Clustering algorithms can be categorized into a few types, specifically exclusive, overlapping, hierarchical, and probabilistic. Exclusive clustering is a form of grouping that stipulates a data point can exist only in one cluster. This can also be referred to as \u201chard\u201d clustering. The K-means clustering algorithm is an example of exclusive clustering. Overlapping clusters differs from exclusive clustering in that it allows data points to belong to multiple clusters with separate degrees of membership. \u201cSoft\u201d or fuzzy k-means clustering is an example of overlapping clustering. Hierarchical clustering, also known as hierarchical cluster analysis (HCA), is an unsupervised clustering algorithm that can be categorized in two ways; they can be agglomerative or divisive. Agglomerative clustering is considered a \u201cbottoms-up approach.\u201d Its data points are isolated as separate groupings initially, and then they are merged together iteratively on the basis of similarity until one cluster has been achieved. Four different methods are commonly used to measure similarity: Euclidean distance is the most common metric used to calculate these distances; however, other metrics, such as Manhattan distance, are also cited in clustering literature. Divisive clustering can be defined as the opposite of agglomerative clustering; instead it takes a \u201ctop-down\u201d approach. In this case, a single data cluster is divided based on the differences between data points. Divisive clustering is not commonly used, but it is still worth noting in the context of hierarchical clustering. These clustering processes are usually visualized using a dendrogram, a tree-like diagram that documents the merging or splitting of data points at each iteration. A probabilistic model is an unsupervised technique that helps us solve density estimation or \u201csoft\u201d clustering problems. In probabilistic clustering, data points are clustered based on the likelihood that they belong to a particular distribution. The Gaussian Mixture Model (GMM) is the one of the most commonly used probabilistic clustering methods. An association rule is a rule-based method for finding relationships between variables in a given dataset. These methods are frequently used for market basket analysis, allowing companies to better understand relationships between different products. Understanding consumption habits of customers enables businesses to develop better cross-selling strategies and recommendation engines. Examples of this can be seen in Amazon\u2019s \u201cCustomers Who Bought This Item Also Bought\u201d or Spotify\u2019s \"Discover Weekly\" playlist. While there are a few different algorithms used to generate association rules, such as Apriori, Eclat, and FP-Growth, the Apriori algorithm is most widely used. Apriori algorithms have been popularized through market basket analyses, leading to different recommendation engines for music platforms and online retailers. They are used within transactional datasets to identify frequent itemsets, or collections of items, to identify the likelihood of consuming a product given the consumption of another product. For example, if I play Black Sabbath\u2019s radio on Spotify, starting with their song \u201cOrchid\u201d, one of the other songs on this channel will likely be a Led Zeppelin song, such as \u201cOver the Hills and Far Away.\u201d This is based on my prior listening habits as well as the ones of others. Apriori algorithms use a hash tree\u00a0to count itemsets, navigating through the dataset in a breadth-first manner. While more data generally yields more accurate results, it can also impact the performance of machine learning algorithms (e.g. overfitting) and it can also make it difficult to visualize datasets. Dimensionality reduction is a technique used when the number of features, or dimensions, in a given dataset is too high. It reduces the number of data inputs to a manageable size while also preserving the integrity of the dataset as much as possible. It is commonly used in the preprocessing data stage, and there are a few different dimensionality reduction methods that can be used, such as: Principal component analysis (PCA) is a type of dimensionality reduction algorithm which is used to reduce redundancies and to compress datasets through feature extraction. This method uses a linear transformation to create a new data representation, yielding a set of \"principal components.\" The first principal component is the direction which maximizes the variance of the dataset. While the second principal component also finds the maximum variance in the data, it is completely uncorrelated to the first principal component, yielding a direction that is perpendicular, or orthogonal, to the first component. This process repeats based on the number of dimensions, where a next principal component is the direction orthogonal to the prior components with the most variance. Singular value decomposition (SVD) is another dimensionality reduction approach which factorizes a matrix, A, into three, low-rank matrices. SVD is denoted by the formula, A = USVT, where U and V are orthogonal matrices. S is a diagonal matrix, and S values are considered singular values of matrix A. Similar to PCA, it is commonly used to reduce noise and compress data, such as image files. Autoencoders leverage neural networks to compress data and then recreate a new representation of the original data\u2019s input. Looking at the image below, you can see that the hidden layer specifically acts as a bottleneck to compress the input layer prior to reconstructing within the output layer. The stage from the input layer to the hidden layer is referred to as \u201cencoding\u201d while the stage from the hidden layer to the output layer is known as \u201cdecoding.\u201d Machine learning techniques have become a common method to improve a product user experience and to test systems for quality assurance. Unsupervised learning provides an exploratory path to view data, allowing businesses to identify patterns in large volumes of data more quickly when compared to manual observation. Some of the most common real-world applications of unsupervised learning are: Unsupervised learning and supervised learning are frequently discussed together. Unlike unsupervised learning algorithms, supervised learning algorithms use labeled data. From that data, it either predicts future outcomes or assigns data to specific categories based on the regression or classification problem that it is trying to solve. While supervised learning algorithms tend to be more accurate than unsupervised learning models, they require upfront human intervention to label the data appropriately. However, these labelled datasets allow supervised learning algorithms to avoid computational complexity as they don\u2019t need a large training set to produce intended outcomes. Common regression and classification techniques are linear and logistic regression, na\u00efve bayes, KNN algorithm, and random forest. Semi-supervised learning occurs when only part of the given input data has been labelled. Unsupervised and semi-supervised learning can be more appealing alternatives as it can be time-consuming and costly to rely on domain expertise to label data appropriately for supervised learning. For a deep dive into the differences between these approaches, check out \"Supervised vs. Unsupervised Learning: What's the Difference?\" While unsupervised learning has many benefits, some challenges can occur when it allows machine learning models to execute without any human intervention. Some of these challenges can include: Build and scale trusted AI on any cloud. Automate the AI lifecycle for ModelOps.  Connect the right data, at the right time, to the right people anywhere.  Hybrid. Open. Resilient. Your platform and partner for digital transformation.  Explore the basics of two data science approaches: supervised and unsupervised. Find out which approach is right for your situation. Learn about the three categories of algorithms: supervised, unsupervised, and reinforcement learning. See the ideas behind them and some key algorithms used for each. Unsupervised machine learning models are powerful tools when you are working with large amounts of data.\u00a0IBM Watson Studio\u00a0on\u00a0IBM Cloud Pak for Data\u00a0offers an open source solution for data scientists and developers looking to accelerate their unsupervised machine learning deployments. Scale your learning models across any cloud environment and benefit from IBM resources and expertise to get the most out of your unsupervised machine learning models. ", "url": "https://www.ibm.com/topics/unsupervised-learning", "threshold": -0.9891699475454194}