{"title": "What is Artificial Intelligence (AI) ? | IBM", "content": "While a number of definitions of artificial intelligence (AI) have surfaced over the last few decades, John McCarthy offers the following definition in this 2004\u00a0paper\u00a0(PDF, 127 KB) (link resides outside IBM), \" It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.\" However, decades before this definition, the birth of the artificial intelligence conversation was denoted by Alan Turing's seminal work, \"Computing Machinery and Intelligence\" (PDF, 92 KB) (link resides outside of IBM), which was published in 1950. In this paper, Turing, often referred to as the \"father of computer science\", asks the following question, \"Can machines think?\"\u00a0 From there, he offers a test, now famously known as the \"Turing Test\", where a human interrogator would try to distinguish between a computer and human text response. While this test has undergone much scrutiny since its publish, it remains an important part of the history of AI as well as an ongoing concept within philosophy as it utilizes ideas around linguistics. Stuart Russell and Peter Norvig then proceeded to publish,\u00a0Artificial Intelligence: A Modern Approach\u00a0(link resides outside IBM), becoming one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting: Human approach: Ideal approach: Alan Turing\u2019s definition would have fallen under the category of \u201csystems that act like humans.\u201d At its simplest form, artificial intelligence is a field, which combines computer science and robust datasets, to enable problem-solving. It also encompasses sub-fields of machine learning and deep learning, which are frequently mentioned in conjunction with artificial intelligence. These disciplines are comprised of AI algorithms which seek to create expert systems which make predictions or classifications based on input data. Over the years, artificial intelligence has gone through many cycles of hype, but even toskeptics, the release of OpenAI\u2019s ChatGPT seems to mark a turning point.\u00a0The last time generative AI loomed this large, the breakthroughs were in computer vision, but now the leap forward is in natural language\u00a0processing. And it\u2019s not just language: Generative models can also learn the grammar of software code, molecules, natural images, and a variety of other data types. The applications for this technology are growing every day, and we\u2019re just starting toexplore the possibilities. But as the hype around the use of AI in business takes off,conversations around ethics become critically important.\u00a0To read more on where IBM stands within the conversation around\u00a0AI ethics, read more\u00a0here. Multiply the power of AI for your enterprise with IBM\u2019s next-generation AI and data platform Magic Quadrant for Enterprise Conversational AI Platforms, 2023 IBM Watson Orchestrate IBM Watson Assistant Weak AI\u2014also called Narrow AI or Artificial Narrow Intelligence (ANI)\u2014is AI trained and focused to perform specific tasks. Weak AI drives most of the AI that surrounds us today. \u2018Narrow\u2019 might be a more accurate descriptor for this type of AI as it is anything but weak; it enables some very robust applications, such as Apple's Siri, Amazon's Alexa, IBM Watson, and autonomous vehicles. Strong AI is made up of Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI). Artificial general intelligence (AGI), or general AI, is a theoretical form of AI where a machine would have an intelligence equaled to humans; it would have a self-aware consciousness that has the ability to solve problems, learn, and plan for the future. Artificial Super Intelligence (ASI)\u2014also known as superintelligence\u2014would surpass the intelligence and ability of the human brain. While strong AI is still entirely theoretical with no practical examples in use today, that doesn't mean AI researchers aren't also exploring its development. In the meantime, the best examples of ASI might be from science fiction, such as HAL, the superhuman, rogue computer assistant in\u00a02001: A Space Odyssey. Since deep learning and machine learning tend to be used interchangeably, it\u2019s worth noting the nuances between the two. As mentioned above, both deep learning and machine learning are sub-fields of artificial intelligence, and deep learning is actually a sub-field of machine learning. Deep learning is actually comprised of neural networks. \u201cDeep\u201d in deep learning refers to a neural network comprised of more than three layers\u2014which would be inclusive of the inputs and the output\u2014can be considered a deep learning algorithm. This is generally represented using the diagram below. The way in which deep learning and machine learning differ is in how each algorithm learns. Deep learning automates much of the feature extraction piece of the process, eliminating some of the manual human intervention required and enabling the use of larger data sets. You can think of deep learning as \"scalable machine learning\" as Lex Fridman noted in same MIT lecture from above. Classical, or \"non-deep\", machine learning is more dependent on human intervention to learn. Human experts determine the hierarchy of features to understand the differences between data inputs, usually requiring more structured data to learn. \"Deep\" machine learning can leverage labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn\u2019t necessarily require a labeled dataset. It can ingest unstructured data in its raw form (e.g. text, images), and it can automatically determine the hierarchy of features which distinguish different categories of data from one another. Unlike machine learning, it doesn't require human intervention to process data, allowing us to scale machine learning in more interesting ways. Generative AI refers to deep-learning models that can take raw data \u2014 say, all of Wikipedia or the collected works of Rembrandt \u2014 and \u201clearn\u201d to generate statistically probable outputs when prompted. At a high level, generative models encode a simplifiedrepresentation of their training data and draw from it to create a new work that\u2019s similar,but not identical, to the original data. Generative models have been used for years in statistics to analyze numerical data. The rise of deep learning, however, made it possible to extend them to images, speech, and other complex data types. Among the first class of models to achieve this cross-over feat were variational autoencoders, or VAEs, introduced in 2013. VAEs were the first deep-learning models to be widely used for generating realistic images and speech. \u201cVAEs opened the floodgates to deep generative modeling by making models easier toscale,\u201d said Akash Srivastava, an expert on generative AI at the MIT-IBM Watson AI Lab.\u201cMuch of what we think of today as generative AI started here.\u201d Early examples of models, like GPT-3, BERT, or DALL-E 2, have shown what\u2019s possible. The future is models that are trained on a broad set of unlabeled data that can be used for different tasks, with minimal fine-tuning. Systems that execute specific tasks in a single\u00a0domain are giving way to broad AI that learns more generally and works across domains and problems. Foundation models, trained on large, unlabeled datasets and fine-tuned for an array of applications, are driving this shift. When it comes to generative AI, it is predicted that foundation models will dramaticallyaccelerate AI adoption in enterprise. Reducing labeling requirements will make it mucheasier for businesses to dive in, and the highly accurate, efficient AI-driven automation they enable will mean that far more companies will be able to deploy AI in a wider range of mission-critical situations. For IBM, the hope is that the power of foundation models can eventually be brought to every enterprise in a frictionless hybrid-cloud environment. There are numerous, real-world applications of AI systems today. Below are some of the most common use cases: The idea of 'a machine that thinks' dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of artificial intelligence include the following: \u00a0 Put AI to work in your business with IBM\u2019s industry-leading AI expertise and portfolio of solutions at your side. Reinvent critical workflows and operations by adding AI to maximize experiences, decision-making and business value. AI is changing the game for cybersecurity, analyzing massive quantities of risk data to speed response times and augment under-resourced security operations.  Discover fresh insights into the opportunities, challenges and lessons learned from infusing AI into businesses. Access our full catalog of over 100 online courses by purchasing an individual or multi-user digital learning subscription today allowing you to expand your skills across a range of our products at one low price. IBM again recognized as a Leader in the 2023 Gartner\u00ae Magic Quadrant\u2122 for Enterprise Conversational AI. IBM has been a leader in\u00a0advancing AI-driven technologies\u00a0for enterprises and has pioneered\u00a0the future of machine learning systems\u00a0for multiple industries.\u00a0Learn how\u00a0IBM Watson\u00a0gives enterprises the AI tools they need to transform their business systems and workflows, while significantly improving automation and efficiency.\u00a0 ", "url": "https://www.ibm.com/topics/artificial-intelligence", "threshold": -0.5706726761882419}