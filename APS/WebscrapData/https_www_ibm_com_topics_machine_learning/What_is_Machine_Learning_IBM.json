{"title": "What is Machine Learning? | IBM", "content": "Machine learning is a branch of\u00a0artificial intelligence (AI)\u00a0and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. IBM has a rich\u00a0history\u00a0with machine learning. One of its own, Arthur Samuel, is credited for coining the term, \u201cmachine learning\u201d with his\u00a0research\u00a0(PDF, 481 KB) (link resides outside IBM) around the game of checkers. Robert Nealey, the self-proclaimed checkers master, played the game on an IBM 7094 computer in 1962, and he lost to the computer. Compared to what can be done today, this feat seems trivial, but it\u2019s considered a major milestone in the field of artificial intelligence. Over the last couple of decades, the technological advances in storage and processing power have enabled some innovative products based on machine learning, such as Netflix\u2019s recommendation engine and self-driving cars. Machine learning is an important component of the growing field of data science. Through the use of statistical methods, algorithms are trained to make classifications or predictions, and to uncover key insights in data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics. As big data continues to expand and grow, the market demand for data scientists will increase. They will be required to help identify the most relevant business questions and the data to answer them. Machine learning algorithms are typically created using frameworks that accelerate solution development, such as TensorFlow and PyTorch. Train, validate, tune and deploy foundation and machine learning models, with ease Since deep learning and machine learning tend to be used interchangeably, it\u2019s worth noting the nuances between the two. Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence. However, neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks. The way in which deep learning and machine learning differ is in how each algorithm learns. \"Deep\" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn\u2019t necessarily require a labeled dataset. Deep learning can ingest unstructured data in its raw form (e.g., text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of larger data sets. You can think of deep learning as \"scalable machine learning\" as Lex Fridman notes in\u00a0this MIT lecture (01:08:05)\u00a0(link resides outside IBM). Classical, or \"non-deep\", machine learning is more dependent on human intervention to learn. Human experts determine the set of features to understand the differences between data inputs, usually requiring more structured data to learn. Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. The \u201cdeep\u201d in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers\u2014which would be inclusive of the input and the output\u2014can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network. Deep learning and neural networks are credited with accelerating progress in areas such as computer vision, natural language processing, and speech recognition. See the blog post \u201cAI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What\u2019s the Difference?\u201d for a closer look at how the different concepts relate. UC\u00a0Berkeley\u00a0(link resides outside IBM) breaks out the learning system of a machine learning algorithm into three main parts. Machine learning models fall into three primary categories. Supervised learning, also known as supervised machine learning, is defined by its use of labeled datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids\u00a0overfitting\u00a0or\u00a0underfitting. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, na\u00efve bayes, linear regression, logistic\u00a0regression, random forest, and support vector machine (SVM). Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. This method\u2019s ability to discover similarities and differences in information make it ideal for exploratory data analysis, cross-selling strategies, customer segmentation, and image and pattern recognition. It\u2019s also used to reduce the number of features in a model through the process of dimensionality reduction. Principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering, and probabilistic clustering methods. Semi-supervised learning offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of not having enough labeled data for a supervised learning algorithm.\u00a0It also helps if it\u2019s too costly to label enough data.\u00a0 For a deep dive into the differences between these approaches, check out \"Supervised vs. Unsupervised Learning: What's the Difference?\" Reinforcement machine learning is a machine\u00a0learning model that is similar to supervised learning, but the algorithm isn\u2019t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem. The IBM Watson\u00ae system that won the\u00a0Jeopardy!\u00a0challenge in 2011 is a good example. The system\u00a0used reinforcement learning\u00a0to learn when to attempt an answer (or question, as it were), which square to select on the board, and how much to wager\u2014especially on daily doubles. Learn more about reinforcement learning.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 A number of machine learning algorithms are commonly used. These include: Here are just a few examples of machine learning you might encounter every day: Speech recognition:\u00a0It is also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, and it is a capability which uses natural language processing (NLP) to translate human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search\u2014e.g. Siri\u2014or improve accessibility for texting. Customer service:\u00a0\u00a0Customer service:\u00a0 Online chatbots are replacing human agents along the customer journey, changing the way we think about customer engagement across websites and social media platforms. Chatbots answer frequently asked questions (FAQs) about topics such as shipping, or provide personalized advice, cross-selling products or suggesting sizes for users. Examples include\u00a0virtual agents\u00a0on e-commerce sites; messaging bots, using Slack and Facebook Messenger; and tasks usually done by virtual assistants and voice assistants. Computer vision:\u00a0This AI technology enables computers to derive meaningful information from digital images, videos, and other visual inputs, and then take the appropriate action. Powered by convolutional neural networks, computer vision has applications in photo tagging on social media, radiology imaging in healthcare, and self-driving cars in the automotive industry.\u00a0 Recommendation engines:\u00a0Using past consumption behavior data, AI algorithms can help to discover data trends that can be used to develop more effective cross-selling strategies. This approach is used by online retailers to make relevant product recommendations to customers during the checkout process. Automated stock trading:\u00a0Designed to optimize stock portfolios, AI-driven high-frequency trading platforms make thousands or even millions of trades per day without human intervention. Fraud detection:\u00a0Banks and other financial institutions can use machine learning to spot suspicious transactions. Supervised learning can train a model using information about known fraudulent transactions. Anomaly detection can identify transactions that look atypical and deserve further investigation. As machine learning technology has developed, it has certainly made our lives easier. However, implementing machine learning in businesses has also raised a number of ethical concerns about AI technologies. Some of these include: While this topic garners a lot of public attention, many researchers are not concerned with the idea of AI surpassing human intelligence in the near future. Technological singularity is also referred to as strong AI or superintelligence. Philosopher Nick Bostrum defines superintelligence as \u201cany intellect that vastly outperforms the best human brains in practically every field, including scientific creativity, general wisdom, and social skills.\u201d Despite the fact that superintelligence is not imminent in society, the idea of it raises some interesting questions as we consider the use of autonomous systems, like self-driving cars. It\u2019s unrealistic to think that a driverless car would never have an accident, but who is responsible and liable under those circumstances? Should we still develop autonomous vehicles, or do we limit this technology to semi-autonomous vehicles which help people drive safely? The jury is still out on this, but these are the types of ethical debates that are occurring as new, innovative AI technology develops. While a lot of public perception of artificial intelligence centers around job losses, this concern should probably be reframed. With every disruptive, new technology, we see that the market demand for specific job roles shifts. For example, when we look at the automotive industry, many manufacturers, like GM, are shifting to focus on electric vehicle production to align with green initiatives. The energy industry isn\u2019t going away, but the source of energy is shifting from a fuel economy to an electric one. In a similar way, artificial intelligence will shift the demand for jobs to other areas. There will need to be individuals to help manage AI systems. There will still need to be people to address more complex problems within the industries that are most likely to be affected by job demand shifts, such as customer service. The biggest challenge with artificial intelligence and its effect on the job market will be helping people to transition to new roles that are in demand. Privacy tends to be discussed in the context of data privacy, data protection, and data security. These concerns have allowed policymakers to make more strides in recent years. For example, in 2016, GDPR legislation was created to protect the personal data of people in the European Union and European Economic Area, giving individuals more control of their data. In the United States, individual states are developing policies, such as the California Consumer Privacy Act (CCPA), which was introduced in 2018 and requires businesses to inform consumers about the collection of their data. Legislation such as this has forced companies to rethink how they store and use personally identifiable information (PII). As a result, investments in security have become an increasing priority for businesses as they seek to eliminate any vulnerabilities and opportunities for surveillance, hacking, and cyberattacks. Instances of bias and discrimination across a number of machine learning systems have raised many ethical questions regarding the use of artificial intelligence. How can we safeguard against bias and discrimination when the training data itself may be generated by biased human processes? While companies typically have good intentions for their automation efforts,\u00a0Reuters\u00a0(link resides outside IBM) ) highlights some of the unforeseen consequences of incorporating AI into hiring practices. In their effort to automate and simplify a process, Amazon unintentionally discriminated against job candidates by gender for technical roles, and the company ultimately had to scrap the project.\u00a0Harvard Business Review\u00a0(link resides outside IBM) has raised other pointed questions about the use of AI in hiring practices, such as what data you should be able to use when evaluating a candidate for a role. Bias and discrimination aren\u2019t limited to the human resources function either; they can be found in a number of applications from facial recognition software to social media algorithms. As businesses become more aware of the risks with AI, they\u2019ve also become more active in this discussion around AI ethics and values. For example, IBM has sunset its general purpose facial recognition and analysis products. IBM CEO Arvind Krishna wrote: \u201cIBM firmly opposes and will not condone uses of any technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and\u00a0Principles of Trust and Transparency.\u201d Since there isn\u2019t significant legislation to regulate AI practices, there is no real enforcement mechanism to ensure that ethical AI is practiced. The current incentives for companies to be ethical are the negative repercussions of an unethical AI system on the bottom line. To fill the gap, ethical frameworks have emerged as part of a collaboration between ethicists and researchers to govern the construction and distribution of AI models within society. However, at the moment, these only serve to guide. Some\u00a0research\u00a0(link resides outside IBM) (PDF, 1 MB) shows that the combination of distributed responsibility and a lack of foresight into potential consequences aren\u2019t conducive to preventing harm to society. Read more about IBM's position on AI Ethics. Design complex neural networks. Experiment at scale to deploy optimized learning models within IBM Watson Studio.  Analyze data and build analytics models to predict future outcomes. Uncover risks and opportunities for your business.  NLP is AI that speaks the language of your business. Build solutions that drive 383% ROI over three years with IBM Watson Discovery.  AI technology has been rapidly evolving over the last couple of decades. Learn how businesses are implementing AI today. Learn tools businesses use to efficiently run and manage AI models and empower their data scientist with technology that can help optimize their data-driven decision making.  Explore how machine learning lets you continually learn from data and predict the future. IBM again recognized as a Leader in the 2023 Gartner\u00ae Magic Quadrant\u2122 for Enterprise Conversational AI. IBM Watson Studio\u00a0on\u00a0IBM Cloud Pak for Data\u00a0supports the end-to-end machine learning lifecycle on a data and AI platform. You can build, train and manage machine learning models wherever your data lives and deploy them anywhere in your hybrid multi-cloud environment.\u00a0Explore how to\u00a0build, train and manage machine learning models wherever your data lives and deploy them anywhere in your hybrid multi-cloud environment. ", "url": "https://www.ibm.com/topics/machine-learning", "threshold": -0.9904761732407653}