{"title": "AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What\u2019s the Difference? | IBM", "content": " 27 May  2020 6 min read By:  Eda Kavlakoglu, Program Manager Share this page on TwitterShare this page on FacebookShare this page on LinkedInE-mail this page Technology is becoming more embedded in our daily lives by the minute, and in order to keep up with the pace of consumer expectations, companies are more heavily relying on learning algorithms to make things easier. You can see its application in social media (through object recognition in photos) or in talking directly to\u00a0devices (like Alexa or Siri). These technologies are commonly associated with artificial intelligence, machine learning, deep learning, and neural networks, and while they do all play a role, these terms tend to be used interchangeably in conversation, leading to some confusion around the nuances between them. Hopefully, we can use this blog post to clarify some of the ambiguity here. Perhaps the easiest way to think about artificial intelligence, machine learning, neural networks, and deep learning is to think of them like Russian nesting dolls. Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three. Neural networks\u2014and more specifically, artificial neural networks (ANNs)\u2014mimic the human brain through a set of algorithms. At a basic level, a neural network is comprised of four main components: inputs, weights, a bias or threshold, and an output. Similar to linear regression, the algebraic formula would look something like this: From there, let\u2019s apply it to a more tangible example, like whether or not you should order a pizza for dinner. This will be our predicted outcome, or y-hat. Let\u2019s assume that there are three main factors that will influence your decision: Then, let\u2019s assume the following, giving us the following inputs: For simplicity purposes, our inputs will have a binary value of 0 or 1. This technically defines it as a perceptron as neural networks primarily leverage sigmoid neurons, which represent values from negative infinity to positive infinity. This distinction is important since most real-world problems are nonlinear, so we need values which reduce how much influence any single input can have on the outcome. However, summarizing in this way will help you understand the underlying math at play here.\u00a0\u00a0\u00a0 Moving on, we now need to assign some weights to determine importance. Larger weights make a single input\u2019s contribution to the output more significant compared to other inputs. Finally, we\u2019ll also assume a threshold value of 5, which would translate to a bias value of \u20135. Since we established all the relevant values for our summation, we can now plug them into this formula. Using the following activation function, we can now calculate the output (i.e., our decision to order pizza): In summary: \u00a0\u00a0Y-hat (our predicted outcome) = Decide to order pizza or not \u00a0 Y-hat = (1*5) + (0*3) + (1*2) - 5 \u00a0 Y-hat = 5 + 0 + 2 \u2013 5 \u00a0 Y-hat = 2, which is greater than zero. Since Y-hat is 2, the output from the activation function will be 1, meaning that we will order pizza (I mean, who doesn't love pizza).\u00a0\u00a0 If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. Now, imagine the above process being repeated multiple times for a single decision as neural networks tend to have multiple \u201chidden\u201d layers as part of deep learning algorithms. Each hidden layer has its own activation function, potentially passing information from the previous layer into the next one. Once all the outputs from the hidden layers are generated, then they are used as inputs to calculate the final output of the neural network. Again, the above example is just the most basic example of a neural network; most real-world examples are nonlinear and far more complex. The main difference between regression and a neural network is the impact of change on a single weight. In regression, you can change a weight without affecting the other inputs in a function. However, this isn\u2019t the case with neural networks. Since the output of one layer is passed into the next layer of the network, a single change can have a cascading effect on the other neurons in the network. See this IBM Developer article for a deeper explanation of the quantitative concepts involved in neural networks. While it was implied within the explanation of neural networks, it\u2019s worth noting more explicitly. The \u201cdeep\u201d in deep learning is referring to the depth of layers in a neural network. A neural network that consists of more than three layers\u2014which would be inclusive of the inputs and the output\u2014can be considered a deep learning algorithm. This is generally represented using the following diagram: Most deep neural networks are feed-forward, meaning they flow in one direction only from input to output. However, you can also train your model through backpropagation; that is, move in opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the algorithm appropriately. As we explain in our Learn Hub article on Deep Learning, deep learning is merely a subset of machine learning. The primary ways in which they differ is in how each algorithm learns and how much data each type of algorithm uses. Deep learning automates much of the feature extraction piece of the process, eliminating some of the manual human intervention required. It also enables the use of large data sets, earning itself the title of \"scalable machine learning\" in this MIT lecture. This capability will be particularly interesting as we begin to explore the use of unstructured data more, particularly since 80-90% of an organization\u2019s data is estimated to be unstructured.\u00a0 Classical, or \"non-deep\", machine learning is more dependent on human intervention to learn. Human experts determine the hierarchy of features to understand the differences between data inputs, usually requiring more structured data to learn. For example, let's say that I were to show you a series of images of different types of fast food, \u201cpizza,\u201d \u201cburger,\u201d or \u201ctaco.\u201d The human expert on these images would determine the characteristics which distinguish each picture as the specific fast food type. For example, the bread of each food type might be a distinguishing feature across each picture. Alternatively, you might just use labels, such as \u201cpizza,\u201d \u201cburger,\u201d or \u201ctaco\u201d, to streamline the learning process through supervised learning. \"Deep\" machine learning can leverage labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn\u2019t necessarily require a labeled dataset. It can ingest unstructured data in its raw form (e.g. text, images), and it can automatically determine the set of features which distinguish \"pizza\", \"burger\", and \"taco\" from one another. For a deep dive into the differences between these approaches, check out \"Supervised vs. Unsupervised Learning: What's the Difference?\" By observing patterns in the data, a deep learning model can cluster inputs appropriately. Taking the same example from earlier, we could group pictures of pizzas, burgers, and tacos into their respective categories based on the similarities or differences identified in the images. With that said, a deep learning model would require more data points to improve its accuracy, whereas a machine learning model relies on less data given the underlying data structure. Deep learning is primarily leveraged for more complex use cases, like virtual assistants or fraud detection. For further info on machine learning, check out the following video: Finally, artificial intelligence (AI) is the broadest term used to classify machines that mimic human intelligence. It is used to predict, automate, and optimize tasks that humans have historically done, such as speech and facial recognition, decision making, and translation. There are three main categories of AI: ANI is considered \u201cweak\u201d AI, whereas the other two types are classified as \u201cstrong\u201d AI. Weak AI is defined by its ability to complete a very specific task, like winning a chess game or identifying a specific individual in a series of photos. As we move into stronger forms of AI, like AGI and ASI, the incorporation of more human behaviors becomes more prominent, such as the ability to interpret tone and emotion. Chatbots and virtual assistants, like Siri, are scratching the surface of this, but they are still examples of ANI. Strong AI is defined by its ability compared to humans. Artificial General Intelligence (AGI) would perform on par with another human while Artificial Super Intelligence (ASI)\u2014also known as superintelligence\u2014would surpass a human\u2019s intelligence and ability. Neither forms of Strong AI exist yet, but ongoing research in this field continues. Since this area of AI is still rapidly evolving, the best example that I can offer on what this might look like is the character Dolores on the HBO show Westworld. While all these areas of AI can help streamline areas of your business and improve your customer experience, achieving AI goals can be challenging because you\u2019ll first need to ensure that you have the right systems in place to manage your data for the construction of learning algorithms. Data management is arguably harder than building the actual models that you\u2019ll use for your business. You\u2019ll need a place to store your data and mechanisms for cleaning it and controlling for bias before you can start building anything. Take a look at some of IBM\u2019s product offerings to help you and your business get on the right track to prepare and manage your data at scale. Program Manager Be the first to hear about news, product updates, and innovation from IBM Cloud.             Artificial intelligence           By:Shannon Cardwell         10 May  2023       icons             Artificial intelligence           By:Udi Barzelay, Tal Drory, Andrew Cabral, and Calin Furau         21 April  2023       icons             Artificial intelligence           By:IBM Cloud Team         20 April  2023       icons             Be the first to hear about news, product updates, and innovation from IBM Cloud           Get updates to your inbox.", "url": "https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks", "threshold": -0.8537207218794053}