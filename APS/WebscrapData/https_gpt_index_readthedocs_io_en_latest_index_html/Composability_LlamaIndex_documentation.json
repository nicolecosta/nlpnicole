{"title": "Composability \u2014 LlamaIndex  documentation", "content": "Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM\u2019s with external data. Github: https://github.com/jerryjliu/llama_index LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM\u2019s reasoning capabilities to generate a response. To perform LLM\u2019s data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing That\u2019s where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API\u2019s, PDF\u2019s, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM\u2019s. These indices help to abstract away common boilerplate and pain points for in-context learning: Storing context in an easy-to-access format for prompt insertion. Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big. Dealing with text splitting. Provides users an interface to query the index (feed in an input prompt) and obtain a knowledge-augmented output. Offers you a comprehensive toolset trading off cost and performance. Getting Started Guides Use Cases Key Components Reference Gallery \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery You can simply do: Git clone this repository: git clone git@github.com:jerryjliu/gpt_index.git. Then do: pip install -e . if you want to do an editable install (you can modify source files) of just the package itself. pip install -r requirements.txt if you want to install optional dependencies + dependencies used for development (e.g. unit testing). By default, we use the OpenAI GPT-3 text-davinci-003 model. In order to use this, you must have an OPENAI_API_KEY setup.You can register an API key by logging into OpenAI\u2019s page and creating a new API token. You can customize the underlying LLM in the Custom LLMs How-To (courtesy of Langchain). You mayneed additional environment keys + tokens setup depending on the LLM provider. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Here is a starter example for using LlamaIndex. Make sure you\u2019ve followed the installation steps first. LlamaIndex examples can be found in the examples folder of the LlamaIndex repository.We first want to download this examples folder. An easy way to do this is to just clone the repo: Next, navigate to your newly-cloned repository, and verify the contents: We now want to navigate to the following folder: This contains LlamaIndex examples around Paul Graham\u2019s essay, \u201cWhat I Worked On\u201d. A comprehensive set of examples are already provided in TestEssay.ipynb. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running. Create a new .py file with the following: This builds an index over the documents in the data folder (which in this case just consists of the essay text). We then run the following You should get back a response similar to the following: The author wrote short stories and tried to program on an IBM 1401. In a Jupyter notebook, you can view info and/or debugging logging using the following snippet: You can set the level to DEBUG for verbose output, or use level=logging.INFO for less. To save to disk and load from disk, do That\u2019s it! For more information on LlamaIndex features, please check out the numerous \u201cGuides\u201d to the left.If you are interested in further exploring how LlamaIndex works, check out our Primer Guide. Additionally, if you would like to play around with Example Notebooks, check out this link. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At its core, LlamaIndex contains a toolkit designed to easily connect LLM\u2019s with your external data.LlamaIndex helps to provide the following: A set of data structures that allow you to index your data for various LLM tasks, and remove concerns over prompt size limitations. Data connectors to your common data sources (Google Docs, Slack, etc.). Cost transparency + tools that reduce cost while increasing performance. Each data structure offers distinct use cases and a variety of customizable parameters. These indices can then bequeried in a general purpose manner, in order to achieve any task that you would typically achieve with an LLM: Question-Answering Summarization Text Generation (Stories, TODO\u2019s, emails, etc.) and more! The guides below are intended to help you get the most out of LlamaIndex. It gives a high-level overview of the following: The general usage pattern of LlamaIndex. Mapping Use Cases to LlamaIndex data Structures How Each Index Works General Guides \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This section contains a list of in-depth tutorials on how to best utilize different capabilitiesof LlamaIndex within your end-user application. They include a broad range of LlamaIndex concepts: Semantic search Structured data support Composability/Query Transformation They also showcase a variety of application settings that LlamaIndex can be used, from a simpleJupyter notebook to a chatbot to a full-stack web application. Tutorials \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery We offer a wide variety of example notebooks. They are referenced throughout the documentation. Example notebooks are found here. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,whether it\u2019s question-answering, summarization, or a component in a chatbot. This section describes the different ways you can query your data with LlamaIndex, roughly in orderof simplest (top-k semantic search), to more advanced capabilities. The most basic example usage of LlamaIndex is through semantic search. We providea simple in-memory vector store for you to get started, but you can also chooseto use any one of our vector store integrations: Relevant Resources: Quickstart Example notebook A summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.For instance, a summarization query could look like one of the following: \u201cWhat is a summary of this collection of text?\u201d \u201cGive me a summary of person X\u2019s experience with the company.\u201d In general, a list index would be suited for this use case. A list index by default goes through all the data. Empirically, setting response_mode=\"tree_summarize\" also leads to better summarization results. LlamaIndex supports queries over structured data, whether that\u2019s a Pandas DataFrame or a SQL Database. Here are some relevant resources: Guide on Text-to-SQL SQL Demo Notebook 1 SQL Demo Notebook 2 (Context) SQL Demo Notebook 3 (Big tables) Pandas Demo Notebook. LlamaIndex supports synthesizing across heterogenous data sources. This can be done by composing a graph over your existing data.Specifically, compose a list index over your subindices. A list index inherently combines information for each node; thereforeit can synthesize information across your heteregenous data sources. Here are some relevant resources: Composability City Analysis Demo. LlamaIndex also supports routing over heteregenous data sources - for instance, if you want to \u201croute\u201d a query to anunderlying Document or a subindex.Here you have three options: GPTTreeIndex, GPTKeywordTableIndex, or aVector Store Index. A GPTTreeIndex uses the LLM to select the child node(s) to send the query down to.A GPTKeywordTableIndex uses keyword matching, and a GPTVectorStoreIndex usesembedding cosine similarity. Here are some relevant resources: Composability Composable Keyword Table Graph. LlamaIndex can support compare/contrast queries as well. It can do this in the following fashion: Composing a graph over your data Adding in query transformations. You can perform compare/contrast queries by just composing a graph over your data. Here are some relevant resources: Composability SEC 10-k Analysis Example notebook. You can also perform compare/contrast queries with a query transformation module. This module will help break down a complex query into a simpler one over your existing index structure. Here are some relevant resources: Query Transformations City Analysis Example Notebook LlamaIndex can also support multi-step queries. Given a complex query, break it down into subquestions. For instance, given a question \u201cWho was in the first batch of the accelerator program the author started?\u201d,the module will first decompose the query into a simpler initial question \u201cWhat was the accelerator program the author started?\u201d,query the index, and then ask followup questions. Here are some relevant resources: Query Transformations Multi-Step Query Decomposition Notebook \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex modules provide plug and play data loaders, data structures, and query interfaces. They can be used in your downstream LLM Application. Some of these applications are described below. Chatbots are an incredibly popular use case for LLM\u2019s. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents. Relevant Resources: Building a Chatbot Using with a LangChain Agent LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit. We provide tutorials and resources to help you get started in this area. Relevant Resources: Fullstack Application Guide LlamaIndex Starter Pack \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Our data connectors are offered through LlamaHub \ud83e\udd99.LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  Some sample data connectors: local file directory (SimpleDirectoryReader). Can support parsing a wide range of file types: .pdf, .jpg, .png, .docx, etc. Notion (NotionPageReader) Google Docs (GoogleDocsReader) Slack (SlackReader) Discord (DiscordReader) Each data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a download_loader function, whichdownloads the loader file into a module that you can use within your application. Example usage: \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At the core of LlamaIndex is a set of index data structures. You can choose to use them on their own,or you can choose to compose a graph over these data structures. In the following sections, we detail how each index structure works, as well as some of the keycapabilities our indices/graphs provide. Index Structures \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a query interface over your index or graph structure. This query interfaceallows you to both retrieve the set of relevant documents, as well as synthesize a response. The basic query interface is found in our usage pattern guide. The guidedetails how to specify parameters for a basic query over a single index structure. A more advanced query interface is found in our composability guide. The guidedescribes how to specify a graph over multiple index structures. Finally, we provide a guide to our Query Transformations module. Query Interface \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides the ability to customize the following components: LLM Prompts Embedding model These are described in their respective guides below. Customizable Modules \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a variety of tools for analysis and optimizationof your indices and queries. Some of our tools involve the analysis/optimization of token usage and cost. We also offer a Playground module, giving you a visual means of analyzingthe token usage of various index structures + performance. Analysis and Optimization \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LLM output/validation capabilities are crucial to LlamaIndex in the following areas: Document retrieval: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \u201cANSWER: (number)\u201d. Response synthesis: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.) LlamaIndex supports integrations with output parsing modules offeredby other frameworks. These output parsing modules can be used in the following ways: To provide formatting instructions for any prompt / query (through output_parser.format) To provide \u201cparsing\u201d for LLM outputs (through output_parser.parse) Guardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example. Output: Langchain also offers output parsing modules that you can use within LlamaIndex. Output: \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a diverse range of integrations with other toolsets and storage providers. Some of these integrations are provided in more detailed guides below. Integrations \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This doc shows both the overarching class used to represent an index. Theseclasses allow for index creation, insertion, and also querying.We first show the different index subclasses.We then show the base class that all indices inherit from, which containsparameters and methods common to all indices. Index Data Structures Base index classes. Base LlamaIndex. nodes (List[Node]) \u2013 List of nodes to index service_context (ServiceContext) \u2013 Service context container (containscomponents like LLMPredictor, PromptHelper, etc.). Asynchronously answer a query. When query is called, we query the index with the given mode andquery_kwargs. The mode determines the type of query to run, andquery_kwargs are parameters that are specific to the query type. For a comprehensive documentation of available mode and query_kwargs toquery a given index, please visit Querying an Index. Delete a document from the index. All nodes in the index related to the index will be deleted. doc_id (str) \u2013 document id Get the docstore corresponding to the index. Create index from documents. documents (Optional[Sequence[BaseDocument]]) \u2013 List of documents tobuild the index from. Get query map. Get the index struct. Load index from dict. Load index from disk. This method loads the index from a JSON file stored on disk. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). NOTE: load_from_disk should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_disk and load_from_disk on that instead. save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Load index from string (in JSON-format). This method loads the index from a JSON string. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). NOTE: load_from_string should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_string and load_from_string on that instead. index_string (str) \u2013 The index string (in JSON-format). The loaded index. BaseGPTIndex Answer a query. When query is called, we query the index with the given mode andquery_kwargs. The mode determines the type of query to run, andquery_kwargs are parameters that are specific to the query type. For a comprehensive documentation of available mode and query_kwargs toquery a given index, please visit Querying an Index. Refresh an index with documents that have changed. This allows users to save LLM and Embedding model calls, while onlyupdating documents that have any changes in text or extra_info. Itwill also insert any documents that previously were not stored. Save to dict. Save to file. This method stores the index into a JSON file stored on disk. NOTE: save_to_disk should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_disk and load_from_disk on that instead. save_path (str) \u2013 The save_path of the file. encoding (str) \u2013 The encoding of the file. Save to string. This method stores the index into a JSON string. NOTE: save_to_string should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_string and load_from_string on that instead. The JSON string of the index. str Update a document. This is equivalent to deleting the document and then inserting it again. document (Union[BaseDocument, BaseGPTIndex]) \u2013 document to update insert_kwargs (Dict) \u2013 kwargs to pass to insert delete_kwargs (Dict) \u2013 kwargs to pass to delete \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This doc shows the classes that are used to query indices.We first show index-specific query subclasses.We then show how to define a query config in order to recursively querymultiple indices that are composed together.We then show the base query class, which contains parameters that are sharedamong all queries.Lastly, we show how to customize the string(s) used for an embedding-based query. Index-specific Query Subclasses This section shows how to define a query config in order to recursively querymultiple indices that are composed together. Query Configs for Composed Indices Base query classes. Base LlamaIndex Query. Helper class that is used to query an index. Can be called within querymethod of a BaseGPTIndex object, or instantiated independently. service_context (ServiceContext) \u2013 service context container (contains componentslike LLMPredictor, PromptHelper). required_keywords (List[str]) \u2013 Optional list of keywords that must be presentin nodes. Can be used to query most indices (tree index is an exception). exclude_keywords (List[str]) \u2013 Optional list of keywords that must not bepresent in nodes. Can be used to query most indices (tree index is anexception). response_mode (ResponseMode) \u2013 Optional ResponseMode. If not provided, willuse the default ResponseMode. text_qa_template (QuestionAnswerPrompt) \u2013 Optional QuestionAnswerPrompt object.If not provided, will use the default QuestionAnswerPrompt. refine_template (RefinePrompt) \u2013 Optional RefinePrompt object. If not provided,will use the default RefinePrompt. include_summary (bool) \u2013 Optional bool. If True, will also use the summarytext of the index when generating a response (the summary text can be setthrough index.set_text(\u201c<text>\u201d)). similarity_cutoff (float) \u2013 Optional float. If set, will filter out nodes withsimilarity below this cutoff threshold when computing the response streaming (bool) \u2013 Optional bool. If True, will return a StreamingResponseobject. If False, will return a Response object. Get the index struct. Get list of tuples of node and similarity for response. First part of the tuple is the node.Second part of tuple is the distance from query to the node.If not applicable, it\u2019s None. Query bundle enables user to customize the string(s) used for embedding-based query. Query Configuration Schema. This schema is used under the hood for all queries, but is primarilyexposed for recursive queries over composable indices. Query bundle. This dataclass contains the original query string and associated transformations. query_str (str) \u2013 the original user-specified query string.This is currently used by all non embedding-based queries. embedding_strs (list[str]) \u2013 list of strings used for embedding the query.This is currently used by all embedding-based queries. embedding (list[float]) \u2013 the stored embedding for the query. Use custom embedding strs if specified, otherwise use query str. Query config. Used under the hood for all queries.The user must explicitly specify a list of query config objects is passed duringa query call to define configurations for each individual subindex within anoverall composed index. The user may choose to specify either the query config objects directly,or as a list of JSON dictionaries. For instance, the following are equivalent: index_struct_id (Optional[str]) \u2013 The index struct id. This can be obtainedby calling\u201cget_doc_id\u201d on the original index class. This can be set by calling\u201cset_doc_id\u201d on the original index class. index_struct_type (IndexStructType) \u2013 The type of index struct. query_mode (QueryMode) \u2013 The query mode. query_kwargs (Dict[str, Any], optional) \u2013 The query kwargs. Defaults to {}. Query mode enum. Can be passed as the enum struct, or as the underlying string. Default query mode. \u201cdefault\u201d Retrieve mode. \u201cretrieve\u201d Embedding mode. \u201cembedding\u201d Summarize mode. Used for hierarchicalsummarization in the tree index. \u201csummarize\u201d Simple mode. Used for keyword extraction. \u201csimple\u201d RAKE mode. Used for keyword extraction. \u201crake\u201d Recursive mode. Used to recursively queryover composed indices. \u201crecursive\u201d Query Transform Query transform augments a raw query string with associated transformationsto improve index querying. Query Transforms. Decompose query transform. Decomposes query into a subquery given the current index struct.Performs a single step transformation. llm_predictor (Optional[LLMPredictor]) \u2013 LLM for generatinghypothetical documents Run query transform. Hypothetical Document Embeddings (HyDE) query transform. It uses an LLM to generate hypothetical answer(s) to a given query,and use the resulting documents as embedding strings. As described in [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496) Run query transform. Step decompose query transform. Decomposes query into a subquery given the current index structand previous reasoning. NOTE: doesn\u2019t work yet. llm_predictor (Optional[LLMPredictor]) \u2013 LLM for generatinghypothetical documents Run query transform. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Below we show the API reference for composable data structures. Init composability. Composable graph. Query the index. Create composable graph using this index class as the root. NOTE: this is mostly syntactic sugar,roughly equivalent to directly calling ComposableGraph.from_indices. Get index from index struct id. Load index from disk. This method loads the index from a JSON file stored on disk. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Load index from string (in JSON-format). This method loads the index from a JSON string. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Query the index. Save to file. This method stores the index into a JSON file stored on disk. save_path (str) \u2013 The save_path of the file. Save to string. This method stores the index into a JSON file stored on disk. save_path (str) \u2013 The save_path of the file. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.      ", "url": "https://gpt-index.readthedocs.io/en/latest/reference/composability.html"}