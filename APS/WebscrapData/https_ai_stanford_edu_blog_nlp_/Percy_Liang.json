{"title": "Percy Liang", "content": "If you are a Stanford undergraduate or masters student interested in joining    my research group, please apply here.   I publish mostly in machine learning (ICML, NeurIPS) and natural language processing (ACL, NAACL, EMNLP) conferences.  I am drawn to simple things, want to understand things deeply, and like to build useful systems.     These days, I am fascinated by foundation models (e.g., GPT-3, PaLM, DALL-E),  models trained on broad data using self-supervision at immense scale, which can then be adapted to myriad downstream tasks.  Foundation models represent a paradigm shift in how AI systems are built and how people might interact with AI.  How can we better understand how they work and make them more efficient, modular, and robust?     I am also a strong proponent of efficient and reproducible research.  We have been developing CodaLab Worksheets,  a platform that allows researchers to run and manage their experiments by maintaining the full provenance  of an experiment from raw data to final results.  Most of our recent papers have been published on CodaLab as executable  papers.  We are actively looking for contributors,  so please contact me if you're interested!   Here is some code for older projects.", "url": "https://cs.stanford.edu/~pliang/", "threshold": -0.016845068856175094}