{"title": "Intel (Intel)", "content": "Build, train and deploy state of the art models powered by the reference open source in machine learning.\t\t\t Hub Create, discover and collaborate on ML better.Join the community to start your ML journey.\t\t\t\t\t\t Tasks Thousands of creators work as a community to solve Audio, Vision, and Language with AI.\t\t\t\t\t\t 347 models 2,697 models 326 models 4,091 models 923 models 18,799 models 1,962 models Open Source Transformers is our natural language processing library and our hub is now open to all ML models, with\t\t\t\t\t\t\tsupport from libraries like\t\t\t\t\t\t\tFlair,\t\t\t\t\t\t\tAsteroid,\t\t\t\t\t\t\tESPnet,\t\t\t\t\t\t\tPyannote, and\t\t\t\t\t\t\tmore to come.\t\t\t\t\t\t On demand Serve your models directly from Hugging Face infrastructure and run large scale NLP models in\t\t\t\t\t\t\t\tmilliseconds with just a few lines of code.\t\t\t\t\t\t\t We\u2019re on a journey to advance and democratize NLP for everyone. Along the way, we contribute to the\t\t\t\t\t\tdevelopment of technology for the better.\t\t\t\t\t \ud83c\udf38 T0 Open source state-of-the-art zero-shot language model out of\t\t\t\t\t\t\tBigScience.\t\t\t\t\t\t \ud83d\udc0e DistilBERT A smaller, faster, lighter, cheaper version of BERT obtained via model distillation.\t\t\t\t\t\t \ud83d\udcda HMTL Learning embeddings from semantic tasks for multi-task learning. We have open-sourced code and a demo.\t\t\t\t\t\t \ud83d\udc38 Dynamical Language Models A meta learner is trained via gradient descent to continuously and dynamically update language model\t\t\t\t\t\t\tweights.\t\t\t\t\t\t \ud83e\udd16 State of the art Our open source coreference resolution library for coreference. You can train it on your own dataset and\t\t\t\t\t\t\tlanguage.\t\t\t\t\t\t \ud83e\udd84 Auto-complete your thoughts This web app is the official demo of the Transformers repository's text generation capabilities.\t\t\t\t\t\t Discover amazing ML apps made by the community! Users and organizations already use the Hub as a collaboration platform,\t\t\t\t\twe\u2019re making it easy to seamlessly and scalably launch ML compute directly from the Hub.\t\t\t\t Collaborate on Machine Learning Forever Upgrade your Space compute Starting at Deploy models on fully managed infrastructure Starting at Show your support for the best ML community Subscribe for Create powerful AI models without code Starting at ML Solutions from research to production Contact us The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine\t\t\t\t\tLearning. Join the open source Machine Learning movement!\t\t\t\t Packed with ML features, like model eval, dataset preview and much more. Git based and designed for collaboration at its core. Learn by experimenting and sharing with our awesome community. Share your work with the world and build your own ML profile. Spaces are one of the most popular ways to share ML applications and demos with the world.\t\t\t\t\tUpgrade your Spaces with our selection of custom on-demand hardware:\t\t\t\t Building something cool as a side project? We also offer community GPU grants.\t\t\t Inference Endpoints offers a secure production solution to easily deploy any ML model on dedicated and\t\t\t\t\tautoscaling infrastructure, right from the HF Hub.\t\t\t\t Create powerful AI models without code. AutoTrain is a new way to automatically train, evaluate and deploy\t\t\t\t\tstate-of-the-art Machine Learning models by simply uploading data.\t\t\t\t A monthly subscription to access exclusive features. Don't have an account?\t\t\t\t\tSign Up  Forgot your password?  Join the community of machine learners! Already have an account?\t\t\t\t\tLog in  \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.               Use Git or checkout with SVN using the web URL.           Work fast with our official CLI.      Learn more.                     Please                sign in                to use Codespaces.                   If nothing happens, download GitHub Desktop and try again.       If nothing happens, download GitHub Desktop and try again.       If nothing happens, download Xcode and try again.   Your codespace will open once ready. There was a problem preparing your codespace, please try again.   English |        \u7b80\u4f53\u4e2d\u6587 |        \u7e41\u9ad4\u4e2d\u6587 |        \ud55c\uad6d\uc5b4 |        Espa\u00f1ol |        \u65e5\u672c\u8a9e |        \u0939\u093f\u0928\u094d\u0926\u0940  State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. These models can be applied on: Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. You can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models. Here are a few examples: In Natural Language Processing: In Computer Vision: In Audio: In Multimodal tasks: Write With Transformer, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities. To immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts: The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%. Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image: Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right: You can learn more about the tasks supported by the pipeline API in this tutorial. In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version: And here is the equivalent code for TensorFlow: The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator. The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset. Easy-to-use state-of-the-art models: Lower compute costs, smaller carbon footprint: Choose the right framework for every part of a model's lifetime: Easily customize a model or an example to your needs: This repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+. You should install \ud83e\udd17 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide. First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform. When one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows: If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source. Since Transformers version v4.0.0, we now have a conda channel: huggingface. \ud83e\udd17 Transformers can be installed using conda as follows: Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue. All the model checkpoints provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations. Current number of checkpoints:  \ud83e\udd17 Transformers currently provides the following architectures (see here for a high-level summary of each them): To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to this table. These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation. We now have a paper you can cite for the \ud83e\udd17 Transformers library: \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.     \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.               Use Git or checkout with SVN using the web URL.           Work fast with our official CLI.      Learn more.                     Please                sign in                to use Codespaces.                   If nothing happens, download GitHub Desktop and try again.       If nothing happens, download GitHub Desktop and try again.       If nothing happens, download Xcode and try again.   Your codespace will open once ready. There was a problem preparing your codespace, please try again.   English |        \u7b80\u4f53\u4e2d\u6587 |        \u7e41\u9ad4\u4e2d\u6587 |        \ud55c\uad6d\uc5b4 |        Espa\u00f1ol |        \u65e5\u672c\u8a9e |        \u0939\u093f\u0928\u094d\u0926\u0940  State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. These models can be applied on: Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. You can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models. Here are a few examples: In Natural Language Processing: In Computer Vision: In Audio: In Multimodal tasks: Write With Transformer, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities. To immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts: The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%. Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image: Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right: You can learn more about the tasks supported by the pipeline API in this tutorial. In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version: And here is the equivalent code for TensorFlow: The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator. The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset. Easy-to-use state-of-the-art models: Lower compute costs, smaller carbon footprint: Choose the right framework for every part of a model's lifetime: Easily customize a model or an example to your needs: This repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+. You should install \ud83e\udd17 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide. First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform. When one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows: If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source. Since Transformers version v4.0.0, we now have a conda channel: huggingface. \ud83e\udd17 Transformers can be installed using conda as follows: Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue. All the model checkpoints provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations. Current number of checkpoints:  \ud83e\udd17 Transformers currently provides the following architectures (see here for a high-level summary of each them): To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to this table. These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation. We now have a paper you can cite for the \ud83e\udd17 Transformers library: \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.     AI for the Common Good None defined yet. We let innovators create the next breakthroughs in machine intelligence.         Graphcore and Hugging Face are working together to make training of Transformer models on IPUs fast and easy.Contact Graphcore to learn more about leveraging IPUs for your training needs.                 Accelerate training and inference models with high-performance optimisations across natural language processing,\u202fcomputer vision and more.                     Graphcore\u2019s IPU is powering advances in AI applications such as fraud detection for finance, drug discovery for life sciences, defect detection for manufacturing, traffic monitoring for smart cities and for all of tomorrow\u2019s new breakthroughs.                     Learn more at hf.co/hardware/graphcore You can try out Hugging Face Optimum on IPUs instantly using Paperspace Gradient. Join our Slack Community for help and support.   Advancing the state of the art: We work on computer science problems that define the technology of today and tomorrow. None defined yet.       Intel and Hugging Face are building powerful optimization tools to accelerate training and inference with Transformers.             Intel optimizes the most widely adopted and innovative AI software         tools, frameworks, and libraries for Intel\u00ae architecture. Whether         you are computing locally or deploying AI applications on a massive         scale, your organization can achieve peak performance with AI         software optimized for Intel Xeon Scalable platforms.                 Intel\u2019s engineering collaboration with Hugging Face offers state-of-the-art hardware and software acceleration to train, fine-tune and predict with Transformers.              \tUseful Resources:      ", "url": "https://huggingface.co/Intel"}