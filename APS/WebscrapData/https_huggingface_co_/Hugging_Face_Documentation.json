{"title": "Hugging Face - Documentation", "content": "Build, train and deploy state of the art models powered by the reference open source in machine learning.\t\t\t Hub Create, discover and collaborate on ML better.Join the community to start your ML journey.\t\t\t\t\t\t Tasks Thousands of creators work as a community to solve Audio, Vision, and Language with AI.\t\t\t\t\t\t 347 models 2,697 models 326 models 4,091 models 923 models 18,799 models 1,962 models Open Source Transformers is our natural language processing library and our hub is now open to all ML models, with\t\t\t\t\t\t\tsupport from libraries like\t\t\t\t\t\t\tFlair,\t\t\t\t\t\t\tAsteroid,\t\t\t\t\t\t\tESPnet,\t\t\t\t\t\t\tPyannote, and\t\t\t\t\t\t\tmore to come.\t\t\t\t\t\t On demand Serve your models directly from Hugging Face infrastructure and run large scale NLP models in\t\t\t\t\t\t\t\tmilliseconds with just a few lines of code.\t\t\t\t\t\t\t We\u2019re on a journey to advance and democratize NLP for everyone. Along the way, we contribute to the\t\t\t\t\t\tdevelopment of technology for the better.\t\t\t\t\t \ud83c\udf38 T0 Open source state-of-the-art zero-shot language model out of\t\t\t\t\t\t\tBigScience.\t\t\t\t\t\t \ud83d\udc0e DistilBERT A smaller, faster, lighter, cheaper version of BERT obtained via model distillation.\t\t\t\t\t\t \ud83d\udcda HMTL Learning embeddings from semantic tasks for multi-task learning. We have open-sourced code and a demo.\t\t\t\t\t\t \ud83d\udc38 Dynamical Language Models A meta learner is trained via gradient descent to continuously and dynamically update language model\t\t\t\t\t\t\tweights.\t\t\t\t\t\t \ud83e\udd16 State of the art Our open source coreference resolution library for coreference. You can train it on your own dataset and\t\t\t\t\t\t\tlanguage.\t\t\t\t\t\t \ud83e\udd84 Auto-complete your thoughts This web app is the official demo of the Transformers repository's text generation capabilities.\t\t\t\t\t\t Discover amazing ML apps made by the community!", "url": "https://huggingface.co/docs"}