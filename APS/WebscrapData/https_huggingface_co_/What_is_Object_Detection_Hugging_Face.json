{"title": "What is Object Detection? - Hugging Face", "content": "Build, train and deploy state of the art models powered by the reference open source in machine learning.\t\t\t Hub Create, discover and collaborate on ML better.Join the community to start your ML journey.\t\t\t\t\t\t Tasks Thousands of creators work as a community to solve Audio, Vision, and Language with AI.\t\t\t\t\t\t 347 models 2,697 models 326 models 4,091 models 923 models 18,799 models 1,962 models Open Source Transformers is our natural language processing library and our hub is now open to all ML models, with\t\t\t\t\t\t\tsupport from libraries like\t\t\t\t\t\t\tFlair,\t\t\t\t\t\t\tAsteroid,\t\t\t\t\t\t\tESPnet,\t\t\t\t\t\t\tPyannote, and\t\t\t\t\t\t\tmore to come.\t\t\t\t\t\t On demand Serve your models directly from Hugging Face infrastructure and run large scale NLP models in\t\t\t\t\t\t\t\tmilliseconds with just a few lines of code.\t\t\t\t\t\t\t We\u2019re on a journey to advance and democratize NLP for everyone. Along the way, we contribute to the\t\t\t\t\t\tdevelopment of technology for the better.\t\t\t\t\t \ud83c\udf38 T0 Open source state-of-the-art zero-shot language model out of\t\t\t\t\t\t\tBigScience.\t\t\t\t\t\t \ud83d\udc0e DistilBERT A smaller, faster, lighter, cheaper version of BERT obtained via model distillation.\t\t\t\t\t\t \ud83d\udcda HMTL Learning embeddings from semantic tasks for multi-task learning. We have open-sourced code and a demo.\t\t\t\t\t\t \ud83d\udc38 Dynamical Language Models A meta learner is trained via gradient descent to continuously and dynamically update language model\t\t\t\t\t\t\tweights.\t\t\t\t\t\t \ud83e\udd16 State of the art Our open source coreference resolution library for coreference. You can train it on your own dataset and\t\t\t\t\t\t\tlanguage.\t\t\t\t\t\t \ud83e\udd84 Auto-complete your thoughts This web app is the official demo of the Transformers repository's text generation capabilities.\t\t\t\t\t\t Discover amazing ML apps made by the community! Users and organizations already use the Hub as a collaboration platform,\t\t\t\t\twe\u2019re making it easy to seamlessly and scalably launch ML compute directly from the Hub.\t\t\t\t Collaborate on Machine Learning Forever Upgrade your Space compute Starting at Deploy models on fully managed infrastructure Starting at Show your support for the best ML community Subscribe for Create powerful AI models without code Starting at ML Solutions from research to production Contact us The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine\t\t\t\t\tLearning. Join the open source Machine Learning movement!\t\t\t\t Packed with ML features, like model eval, dataset preview and much more. Git based and designed for collaboration at its core. Learn by experimenting and sharing with our awesome community. Share your work with the world and build your own ML profile. Spaces are one of the most popular ways to share ML applications and demos with the world.\t\t\t\t\tUpgrade your Spaces with our selection of custom on-demand hardware:\t\t\t\t Building something cool as a side project? We also offer community GPU grants.\t\t\t Inference Endpoints offers a secure production solution to easily deploy any ML model on dedicated and\t\t\t\t\tautoscaling infrastructure, right from the HF Hub.\t\t\t\t Create powerful AI models without code. AutoTrain is a new way to automatically train, evaluate and deploy\t\t\t\t\tstate-of-the-art Machine Learning models by simply uploading data.\t\t\t\t A monthly subscription to access exclusive features. Don't have an account?\t\t\t\t\tSign Up  Forgot your password?  Join the community of machine learners! Already have an account?\t\t\t\t\tLog in  \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.               Use Git or checkout with SVN using the web URL.           Work fast with our official CLI.      Learn more.                     Please                sign in                to use Codespaces.                   If nothing happens, download GitHub Desktop and try again.       If nothing happens, download GitHub Desktop and try again.       If nothing happens, download Xcode and try again.   Your codespace will open once ready. There was a problem preparing your codespace, please try again.   English |        \u7b80\u4f53\u4e2d\u6587 |        \u7e41\u9ad4\u4e2d\u6587 |        \ud55c\uad6d\uc5b4 |        Espa\u00f1ol |        \u65e5\u672c\u8a9e |        \u0939\u093f\u0928\u094d\u0926\u0940  State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. These models can be applied on: Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. You can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models. Here are a few examples: In Natural Language Processing: In Computer Vision: In Audio: In Multimodal tasks: Write With Transformer, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities. To immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts: The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%. Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image: Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right: You can learn more about the tasks supported by the pipeline API in this tutorial. In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version: And here is the equivalent code for TensorFlow: The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator. The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset. Easy-to-use state-of-the-art models: Lower compute costs, smaller carbon footprint: Choose the right framework for every part of a model's lifetime: Easily customize a model or an example to your needs: This repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+. You should install \ud83e\udd17 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide. First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform. When one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows: If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source. Since Transformers version v4.0.0, we now have a conda channel: huggingface. \ud83e\udd17 Transformers can be installed using conda as follows: Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue. All the model checkpoints provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations. Current number of checkpoints:  \ud83e\udd17 Transformers currently provides the following architectures (see here for a high-level summary of each them): To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to this table. These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation. We now have a paper you can cite for the \ud83e\udd17 Transformers library: \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.     \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.               Use Git or checkout with SVN using the web URL.           Work fast with our official CLI.      Learn more.                     Please                sign in                to use Codespaces.                   If nothing happens, download GitHub Desktop and try again.       If nothing happens, download GitHub Desktop and try again.       If nothing happens, download Xcode and try again.   Your codespace will open once ready. There was a problem preparing your codespace, please try again.   English |        \u7b80\u4f53\u4e2d\u6587 |        \u7e41\u9ad4\u4e2d\u6587 |        \ud55c\uad6d\uc5b4 |        Espa\u00f1ol |        \u65e5\u672c\u8a9e |        \u0939\u093f\u0928\u094d\u0926\u0940  State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow \ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. These models can be applied on: Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. \ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. \ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. You can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models. Here are a few examples: In Natural Language Processing: In Computer Vision: In Audio: In Multimodal tasks: Write With Transformer, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities. To immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts: The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%. Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image: Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right: You can learn more about the tasks supported by the pipeline API in this tutorial. In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version: And here is the equivalent code for TensorFlow: The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator. The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset. Easy-to-use state-of-the-art models: Lower compute costs, smaller carbon footprint: Choose the right framework for every part of a model's lifetime: Easily customize a model or an example to your needs: This repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+. You should install \ud83e\udd17 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide. First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform. When one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows: If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source. Since Transformers version v4.0.0, we now have a conda channel: huggingface. \ud83e\udd17 Transformers can be installed using conda as follows: Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue. All the model checkpoints provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations. Current number of checkpoints:  \ud83e\udd17 Transformers currently provides the following architectures (see here for a high-level summary of each them): To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to this table. These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation. We now have a paper you can cite for the \ud83e\udd17 Transformers library: \ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.     AI for the Common Good None defined yet. We let innovators create the next breakthroughs in machine intelligence.         Graphcore and Hugging Face are working together to make training of Transformer models on IPUs fast and easy.Contact Graphcore to learn more about leveraging IPUs for your training needs.                 Accelerate training and inference models with high-performance optimisations across natural language processing,\u202fcomputer vision and more.                     Graphcore\u2019s IPU is powering advances in AI applications such as fraud detection for finance, drug discovery for life sciences, defect detection for manufacturing, traffic monitoring for smart cities and for all of tomorrow\u2019s new breakthroughs.                     Learn more at hf.co/hardware/graphcore You can try out Hugging Face Optimum on IPUs instantly using Paperspace Gradient. Join our Slack Community for help and support.   Advancing the state of the art: We work on computer science problems that define the technology of today and tomorrow. None defined yet.       Intel and Hugging Face are building powerful optimization tools to accelerate training and inference with Transformers.             Intel optimizes the most widely adopted and innovative AI software         tools, frameworks, and libraries for Intel\u00ae architecture. Whether         you are computing locally or deploying AI applications on a massive         scale, your organization can achieve peak performance with AI         software optimized for Intel Xeon Scalable platforms.                 Intel\u2019s engineering collaboration with Hugging Face offers state-of-the-art hardware and software acceleration to train, fine-tune and predict with Transformers.              \tUseful Resources:       Deep Learning, Speech Technologies   SpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.We released to the community models for Speech Recognition, Text-to-Speech, Speaker Recognition, Speech Enhancement, Speech Separation, Spoken Language Understanding, Language Identification,  Emotion Recognition, Voice Activity Detection, Sound Classification, Grapheme-to-Phoneme, and many others. Website: https://speechbrain.github.io/ GitHub: https://github.com/speechbrain/speechbrain HuggingFace: https://huggingface.co/speechbrain None defined yet. None defined yet. Join the community of machine learners! Already have an account?\t\t\t\t\tLog in  Hugging Face is the home for all Machine Learning tasks. Here you can find what you need to get started with a\t\t\t\ttask: demos, use cases, models, datasets, and more!\t\t\t 47 models 2,697 models 186 models 53 models 326 models 87 models 568 models 68 models 1,974 models 6,130 models 4,091 models 1,613 models 923 models 57 models 18,799 models 9,248 models 7,667 models 1,962 models 113 models 347 models 144 models 7,309 models 265 models 121 models 61 models 26 models 3,359 models 125 models 2,979 models 14 models 16,371 models Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker. Command recognition or keyword spotting classifies utterances into a predefined set of commands. This is often done on-device for fast response time. As an example, using the Google Speech Commands dataset, given an input, a model can classify which of the following commands the user is typing: Speechbrain models can easily perform this task with just a couple of lines of code! Datasets such as VoxLingua107 allow anyone to train language identification models for up to 107 languages! This can be extremely useful as a preprocessing step for other systems. Here's an example modeltrained on VoxLingua107. Emotion recognition is self explanatory. In addition to trying the widgets, you can use the Inference API to perform audio classification. Here is a simple example that uses a HuBERT model fine-tuned for this task. Speaker Identification is classifying the audio of the person speaking. Speakers are usually predefined. You can try out this task with this model. A useful dataset for this task is VoxCeleb1. We have some great news! You can do fine-tuning (transfer learning) to train a well-performing model without requiring as much data. Pretrained models such as Wav2Vec2 and HuBERT exist. Facebook's Wav2Vec2 XLS-R model is a large multilingual model trained on 128 languages and with 436K hours of speech. We suggest checking out the following example (Colab Notebook) to learn how to fine-tune a model for audio classification with a single or multiple GPUs and share it on the Hub. Note\tAn easy-to-use model for Command Recognition.\t\t\t\t\t\t\t\t\t Note\tAn Emotion Recognition model.\t\t\t\t\t\t\t\t\t Note\tA benchmark of 10 different audio tasks.\t\t\t\t\t\t\t\t\t Note\tAn application that can predict the language spoken in a given audio.\t\t\t\t\t\t\t\t\t Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to. Image classification models can be used when we are not interested in specific instances of objects with location information or their shape. Image classification models are used widely in stock photography to assign each image a keyword. Models trained in image classification can improve user experience by organizing and categorizing photo galleries on the phone or in the cloud, on multiple keywords or tags. With the transformers library, you can use the image-classification pipeline to infer with image classification models. You can initialize the pipeline with a model id from the Hub. If you do not provide a model id it will initialize with google/vit-base-patch16-224 by default. When calling the pipeline you just need to specify a path, http link or an image loaded in PIL. You can also provide a top_k parameter which determines how many results it should return. With HuggingPics, you can fine-tune Vision Transformers for anything using images found on the web. This project downloads images of classes defined by you, trains a model, and pushes it to the Hub. You even get to try out the model directly with a working widget in the browser, ready to be shared with all your friends! Note\tStrong Image Classification model trained on the ImageNet dataset.\t\t\t\t\t\t\t\t\t Note\tStrong Image Classification model trained on the ImageNet dataset.\t\t\t\t\t\t\t\t\t Note\tBenchmark dataset used for image classification with images that belong to 100 classes.\t\t\t\t\t\t\t\t\t Note\tDataset consisting of images of garments.\t\t\t\t\t\t\t\t\t Note\tAn application that classifies what a given image is about.\t\t\t\t\t\t\t\t\t Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects. Object Detection is widely used in computer vision for autonomous driving. Self-driving cars use Object Detection models to detect pedestrians, bicycles, traffic lights and road signs to decide which step to take. Object Detection models are widely used in sports where the ball or a player is tracked for monitoring and refereeing during matches. Object Detection models are widely used in image search. Smartphones use Object Detection models to detect entities (such as specific places or objects) and allow the user to search for the entity on the Internet. Object Detection models are used to count instances of objects in a given image, this can include counting the objects in warehouses or stores, or counting the number of visitors in a store. They are also used to manage crowds at events to prevent disasters. You can infer with Object Detection models through the object-detection pipeline. When calling the pipeline you just need to specify a path or http link to an image. Note\tSolid object detection model trained on the benchmark dataset COCO 2017.\t\t\t\t\t\t\t\t\t Note\tStrong object detection model trained on ImageNet-21k dataset.\t\t\t\t\t\t\t\t\t No example dataset is defined for this task. Note\tContribute by proposing a dataset for this task ! Note\tAn object detection application that can detect unseen objects out of the box.\t\t\t\t\t\t\t\t\t Note\tAn object detection application that can detect facemasks in an image.\t\t\t\t\t\t\t\t\t Note\tAn application that contains various object detection models to try from.\t\t\t\t\t\t\t\t\t Note\tUltralytics YOLOv8: State-of-the-Art YOLO Models.\t\t\t\t\t\t\t\t\t Note\tAn application that shows multiple cutting edge techniques for object detection and tracking\t\t\t\t\t\t\t\t\t", "url": "https://huggingface.co/tasks/object-detection"}