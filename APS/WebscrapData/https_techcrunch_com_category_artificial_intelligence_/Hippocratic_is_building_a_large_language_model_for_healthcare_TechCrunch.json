{"title": "Hippocratic is building a large language model for healthcare | TechCrunch", "content": "AI, specifically generative AI, has the potential to transform healthcare. At least, that sales pitch from Hippocratic AI, which emerged from stealth today with a whopping $50 million in seed financing behind it and a valuation in the \u201ctriple digit millions.\u201d The tranche, co-led by General Catalyst and Andreessen Horowitz, is a big vote of confidence in Hippocratic\u2019s technology, a text-generating model tuned specifically for healthcare applications. Hippocratic \u2014 hatched out of General Catalyst \u2014 was founded by a group of physicians, hospital administrators, Medicare professionals and AI researchers from organizations including Johns Hopkins, Stanford, Google and Nvidia. After co-founder and CEO Munjal Shah sold his previous company, Like.com, a shopping comparison site, to Google in 2010, he spent the better part of the next decade building Hippocratic. \u201cHippocratic has created the first safety-focused large language model (LLM) designed specifically for healthcare,\u201d Shah told TechCrunch in an email interview. \u201cThe company mission is to develop the safest artificial health general intelligence in order to dramatically improve healthcare accessibility and health outcomes.\u201d AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.\u2019s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led major customer partnerships to deteriorate. Elsewhere, OpenAI\u2019s GPT-3, the predecessor to GPT-4, urged at least one user to commit suicide. Shah emphasized that Hippocratic isn\u2019t focused on diagnosing. Rather, he says, the tech \u2014 which is consumer-facing \u2014 is aimed at use cases like explaining benefits and billing, providing dietary advice and medication reminders, answering pre-op questions, onboarding patients and delivering \u201cnegative\u201d test results that indicate nothing\u2019s wrong. Hippocratic\u2019s benchmark results on a range of medical exams. Hippocratic\u2019s benchmark results on a range of medical exams. The dietary advice use case gave me pause, I must say, in light of the poor diet-related suggestions AI like OpenAI\u2019s ChatGPT provides. But Shah claims that Hippocratic\u2019s AI outperforms leading language models including GPT-4 and Claude on over 100 healthcare certifications, including the NCLEX-RN for nursing, the American Board of Urology exam and the registered dietitian exam. \u201cThe language models have to be safe,\u201d Shah said. \u201cThat\u2019s why we\u2019re building a model just focused on safety, certifying it with healthcare professionals and partnering closely with the industry \u2026 This will help ensure that data retention and privacy policies will be consistent with the current norms of the healthcare industry.\u201d One of the ways Hippocratic aims to achieve this is by \u201cdetecting tone\u201d and \u201ccommunicating empathy\u201d better than rival tech, Shah says \u2014 in part by \u201cbuilding in\u201d good bedside manner (i.e. the elusive \u201chuman touch\u201d). He makes the case that bedside manner \u2014 especially interactions that leave patients with a sense of hope, even in grim circumstances \u2014 can and do affect health outcomes. To evaluate bedside manner, Hippocratic designed a benchmark to test the model for signs of humanism, if you will \u2014 things like \u201cshowing empathy\u201d and \u201ctaking a personal interest in a patient\u2019s life.\u201d (Whether a single test can accurately capture subjects that nuanced is up for debate, of course.) Unsurprisingly given the source, Hippocratic\u2019s model scored the highest across all categories of the models that Hippocratic tested, including GPT-4. But can a language model really replace a healthcare worker? Hippocratic invites the question, arguing that its models were trained under the supervision of medical professionals and, thus, highly capable. \u201cWe\u2019re only releasing each role \u2014 dietician, billing agent, genetic counselor, etc. \u2014 once the people who actually do that role today in real life agree the model is ready,\u201d Shah said. \u201cIn the pandemic, labor costs went up 30% for most health systems, but revenue didn\u2019t. Hence, most health systems in the country are financially struggling. Language models can help them reduce costs by filling their current large level of vacancies in a more cost-effective way.\u201d I\u2019m not sure healthcare practitioners would agree \u2014 particularly considering the Hippocratic model\u2019s low scores on some of the aforementioned certifications. According to Hippocratic, the model got a 71% on the certified professional coder exam, which covers knowledge of medical billing and coding, and 72.7% on a hospital safety training compliance quiz. There\u2019s the matter of potential bias, as well. Bias plagues the healthcare industry, and these effects trickle down to the models trained on biased medical records, studies and research. A 2019 study, for instance, found that an algorithm many hospitals were using to decide which patients needed care treated Black patients with less sensitivity than white patients. In any case, one would hope Hippocratic makes it clear that its models aren\u2019t infallible. In domains like healthcare, automation bias, or the propensity for people to trust AI over other sources, even if they\u2019re correct, comes with plainly high risks. Those details are among the many that Hippocratic has yet to iron out. The company isn\u2019t releasing details on its partners or customers, preferring instead to keep the focus on the funding. The model isn\u2019t even available at present \u2014 nor information about what data it was trained on, or what data it might be trained on in the future. (Hippocratic would only say that it\u2019ll use \u201cde-identified\u201d data for the model training.) If it waits too long, Hippocratic runs the risk of falling behind rivals like Truveta and Latent \u2014 some of which have a major resource advantage. For example, Google recently began previewing Med-PaLM 2, which it claims was the first language model to perform at an expert level on dozens of medical exam questions. Like Hippocratic\u2019s model, Med-PaLM 2 was evaluated by health professionals on its ability to answer medical questions accurately \u2014 and safely. But Hemant Taneja, the managing director at General Catalyst, didn\u2019t express concern. \u201cMunjal and I hatched this company on the belief that healthcare needs its own language model built specifically for healthcare applications \u2014 one that is fair, unbiased, secure and beneficial to society,\u201d he said via email. \u201cWe set forth to create a high-integrity AI application that is fed a \u2018healthy\u2019 data diet and includes a training approach that seeks to incorporate extensive human feedback from medical experts for each specialized task. In healthcare, we simply can\u2019t afford to \u2018move fast and break things.'\u201d Shah says that the bulk of the $50 million seed tranche will be put toward investing in talent, compute data and partnerships.", "url": "https://techcrunch.com/2023/05/16/hippocratic-is-building-a-large-language-model-for-healthcare/", "threshold": -0.9991576140403}