{"title": "Machine Learning is Fun! Part 2. Using Machine Learning to generate\u2026 | by Adam Geitgey | Medium", "content": "Adam Geitgey Follow -- 70 Listen Share Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Italiano, Espa\u00f1ol, Fran\u00e7ais, T\u00fcrk\u00e7e, \u0420\u0443\u0441\u0441\u043a\u0438\u0439, \ud55c\uad6d\uc5b4 Portugu\u00eas, \u0641\u0627\u0631\u0633\u06cc, Ti\u1ebfng Vi\u1ec7t or \u666e\u901a\u8bdd. Giant update: I\u2019ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven\u2019t already read part 1, read it now!). This time, we are going to see one of these generic algorithms do something really cool \u2014 create video game levels that look like they were made by humans. We\u2019ll build a neural network, feed it existing Super Mario levels and watch new ones pop out! Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone \u2014 which means that there\u2019s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished. Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this: We ended up with this simple estimation function: In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house\u2019s value. Instead of using code, let\u2019s represent that same function as a simple diagram: However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn\u2019t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn\u2019t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model? To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Now we have four different price estimates. Let\u2019s combine those four price estimates into one final estimate. We\u2019ll run them through the same algorithm again (but using another set of weights)! Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model. Let\u2019s combine our four attempts to guess into one big diagram: This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions. There\u2019s a lot that I\u2019m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click: It\u2019s just like LEGO! We can\u2019t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together: The neural network we\u2019ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it\u2019s a stateless algorithm. In many cases (like estimating the price of house), that\u2019s exactly what you want. But the one thing this kind of model can\u2019t do is respond to patterns in data over time. Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess? I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. Our model might look like this: But let\u2019s make the problem harder. Let\u2019s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem. Let\u2019s use the first few words of Ernest Hemingway\u2019s The Sun Also Rises as an example: Robert Cohn was once middleweight boxi What letter is going to come next? You probably guessed \u2019n\u2019 \u2014 the word is probably going to be boxing. We know this based on the letters we\u2019ve already seen in the sentence and our knowledge of common words in English. Also, the word \u2018middleweight\u2019 gives us an extra clue that we are talking about boxing. In other words, it\u2019s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English. To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently. Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters. This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory. Predicting the next letter in a story might seem pretty useless. What\u2019s the point? One cool use might be auto-predict for a mobile phone keyboard: But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over \u2014 forever? We\u2019d be asking it to write a complete story for us! We saw how we could guess the next letter in Hemingway\u2019s sentence. Let\u2019s try generating a whole story in the style of Hemingway. To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github. We\u2019ll create our model from the complete text of The Sun Also Rises \u2014 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway\u2019s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example. As we just start to train the RNN, it\u2019s not very good at predicting letters. Here\u2019s what it generates after a 100 loops of training: hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h\u2019e btentmuhgehi bcgdltt. gey heho grpiahe.Ddelnss.eelaishaner\u201d cot AAfhB ht ltnyehbih a\u201don bhnte ectrsnae abeahngyamo k ns aeo?cdse nh a taei.rairrhelardr er deffijha You can see that it has figured out that sometimes words have spaces between them, but that\u2019s about it. After about 1000 iterations, things are looking more promising: hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been somsinick stalling that aid. \u201cHon\u2019t me and acrained on .Hw\u2019s don\u2019t you for the roed,\u201d In\u2019s pair.\u201d \u201cAlough marith him.\u201d The model has started to identify the patterns in basic sentence structure. It\u2019s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there\u2019s also still a lot of nonsense. But after several thousand more training iterations, it looks pretty good: He went over to the gate of the caf\u00e9. It was like a country bed. \u201cDo you know it\u2019s been me.\u201d \u201cDamned us,\u201d Bill said. \u201cI was dangerous,\u201d I said. \u201cYou were she did it and think I would a fine cape you,\u201d I said. \u201cI can\u2019t look strange in the cab.\u201d \u201cYou know I was this is though,\u201d Brett said. \u201cIt\u2019s a fights no matter?\u201d \u201cIt makes to do it.\u201d \u201cYou make it?\u201d \u201cSit down,\u201d I said. \u201cI wish I wasn\u2019t do a little with the man.\u201d \u201cYou found it.\u201d \u201cI don\u2019t know.\u201d \u201cYou see, I\u2019m sorry of chatches,\u201d Bill said. \u201cYou think it\u2019s a friend off back and make you really drunk.\u201d At this point, the algorithm has captured the basic pattern of Hemingway\u2019s short, direct dialog. A few sentences even sort of make sense. Compare that with some real text from the book: There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave. \u201cSit down,\u201d said Harvey, \u201cI\u2019ve been looking for you.\u201d \u201cWhat\u2019s the matter?\u201d \u201cNothing. Just looking for you.\u201d \u201cBeen out to the races?\u201d \u201cNo. Not since Sunday.\u201d \u201cWhat do you hear from the States?\u201d \u201cNothing. Absolutely nothing.\u201d \u201cWhat\u2019s the matter?\u201d Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing! We don\u2019t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters. For fun, let\u2019s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of \u201cEr\u201d, \u201cHe\u201d, and \u201cThe S\u201d: Not bad! But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern. In 2015, Nintendo released Super Mario Maker\u2122 for the Wii U gaming system. This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It\u2019s like a virtual LEGO set for people who grew up playing Super Mario Brothers. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? First, we need a data set for training our model. Let\u2019s take all the outdoor levels from the original Super Mario Brothers game released in 1985: This game has 32 levels and about 70% of them have the same outdoor style. So we\u2019ll stick to those. To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game\u2019s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game\u2019s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime. Here\u2019s the first level from the game (which you probably remember if you ever played it): If we look closely, we can see the level is made of a simple grid of objects: We could just as easily represent this grid as a sequence of characters with one character representing each object: We\u2019ve replaced each object in the level with a letter: \u2026and so on, using a different letter for each different kind of object in the level. I ended up with text files that looked like this: Looking at the text file, you can see that Mario levels don\u2019t really have much of a pattern if you read them line-by-line: The patterns in a level really emerge when you think of the level as a series of columns: So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well. To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up: Just like we saw when creating the model of Hemingway\u2019s prose, a model improves as we train it. After a little training, our model is generating junk: It sort of has an idea that \u2018-\u2019s and \u2018=\u2019s should show up a lot, but that\u2019s about it. It hasn\u2019t figured out the pattern yet. After several thousand iterations, it\u2019s starting to look like something: The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the \u201cP\u201ds in the data should appear in 2x2 clusters. That\u2019s pretty cool! With a lot more training, the model gets to the point where it generates perfectly valid data: Let\u2019s sample an entire level\u2019s worth of data from our model and rotate it back horizontal: This data looks great! There are several awesome things to notice: Finally, let\u2019s take this level and recreate it in Super Mario Maker: Play it yourself! If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9\u20130000\u20130157-F3C3. The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a \u2018toy\u2019 instead of cutting-edge is that our model is generated from very little data. There just aren\u2019t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can\u2019t \u2014 because Nintendo won\u2019t let us have them. Big companies don\u2019t give away their data for free. As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That\u2019s why companies like Google and Facebook need your data so badly! For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate. But without Google\u2019s massive trove of data in every language, you can\u2019t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you\u2019ve ever been. In machine learning, there\u2019s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach. Readers have sent me links to other interesting approaches to generating Super Mario levels: If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I\u2019ll only email you when I have something new and awesome to share. It\u2019s the best way to find out when I write more articles like this. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I\u2019d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun Part 3! -- -- 70 Interested in computers and machine learning. Likes to write about it. Adam Geitgey -- 26 Adam Geitgey -- 195 Adam Geitgey -- 263 Adam Geitgey -- 62 The PyCoach in Artificial Corner -- 329 Alex Mathers -- 203 Matt Chapman in Towards Data Science -- 43 Bryan Ye in Better Humans -- 638 Albers Uzila in Level Up Coding -- 13 Unbecoming -- 758 Help Status Writers Blog Careers Privacy Terms About Text to speech", "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3", "threshold": -0.6415731490815757}