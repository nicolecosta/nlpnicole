{"title": "Machine Learning is Fun Part 8: How to Intentionally Trick Neural Networks | by Adam Geitgey | Medium", "content": "Adam Geitgey Follow -- 18 Listen Share This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in \u0420\u0443\u0441\u0441\u043a\u0438\u0439, Ti\u1ebfng Vi\u1ec7t, \u0641\u0627\u0631\u0633\u06cc or \ud55c\uad6d\uc5b4. Giant update: I\u2019ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! Almost as long as programmers have been writing computer programs, computer hackers have been figuring out ways to exploit those programs. Malicious hackers take advantage of the tiniest bugs in programs to break into systems, steal data and generally wreak havoc. But systems powered by deep learning algorithms should be safe from human interference, right? How is a hacker going to get past a neural network trained on terabytes of data? It turns out that even the most advanced deep neural networks can be easily fooled. With a few tricks, you can force them into predicting whatever result you want: So before you launch a new system powered by deep neural networks, let\u2019s learn exactly how to break them and what you can do to protect yourself from attackers. Let\u2019s imagine that we run an auction website like Ebay. On our website, we want to prevent people from selling prohibited items \u2014 things like live animals. Enforcing these kinds of rules are hard if you have millions of users. We could hire hundreds of people to review every auction listing by hand, but that would be expensive. Instead, we can use deep learning to automatically check auction photos for prohibited items and flag the ones that violate the rules. This is a typical image classification problem. To build this, we\u2019ll train a deep convolutional neural network to tell prohibited items apart from allowed items and then we\u2019ll run all the photos on our site through it. First, we need a data set of thousands of images from past auction listings. We need images of both allowed and prohibited items so that we can train the neural network to tell them apart: To train then neural network, we use the standard back-propagation algorithm. This is an algorithm were we pass in a training picture, pass in the expected result for that picture, and then walk back through each layer in the neural network adjusting their weights slightly to make them a little better at producing the correct output for that picture: We repeat this thousands of times with thousands of photos until the model reliably produces the correct results with an acceptable accuracy. The end result is a neural network that can reliably classify images: Note: If you want more detail on how convolution neural networks recognize objects in images, check out Part 3. Convolutional neural networks are powerful models that consider the entire image when classifying it. They can recognize complex shapes and patterns no matter where they appear in the image. In many image recognition tasks, they can equal or even beat human performance. With a fancy model like that, changing a few pixels in the image to be darker or lighter shouldn\u2019t have a big effect on the final prediction, right? Sure, it might change the final likelihood slightly, but it shouldn\u2019t flip an image from \u201cprohibited\u201d to \u201callowed\u201d. But in a famous paper in 2013 called Intriguing properties of neural networks, it was discovered that this isn\u2019t always true. If you know exactly which pixels to change and exactly how much to change them, you can intentionally force the neural network to predict the wrong output for a given picture without changing the appearance of the picture very much. That means we can intentionally craft a picture that is clearly a prohibited item but which completely fools our neural network: Why is this? A machine learning classifier works by finding a dividing line between the things it\u2019s trying to tell apart. Here\u2019s how that looks on a graph for a simple two-dimensional classifier that\u2019s learned to separate green points (acceptable) from red points (prohibited): Right now, the classifier works with 100% accuracy. It\u2019s found a line that perfectly separates all the green points from the red points. But what if we want to trick it into mis-classifying one of the red points as a green point? What\u2019s the minimum amount we could move a red point to push it into green territory? If we add a small amount to the Y value of a red point right beside the boundary, we can just barely push it over into green territory: So to trick a classifier, we just need to know which direction to nudge the point to get it over the line. And if we don\u2019t want to be too obvious about being nefarious, ideally we\u2019ll move the point as little as possible so it just looks like an honest mistake. In image classification with deep neural networks, each \u201cpoint\u201d we are classifying is an entire image made up of thousands of pixels. That gives us thousands of possible values that we can tweak to push the point over the decision line. And if we make sure that we tweak the pixels in the image in a way that isn\u2019t too obvious to a human, we can fool the classifier without making the image look manipulated. In other words, we can take a real picture of one object and change the pixels very slightly so that the image completely tricks the neural network into thinking that the picture is something else \u2014 and we can control exactly what object it detects instead: We\u2019ve already talked about the basic process of training a neural network to classify photos: But what if instead of tweaking the weights of the layers of the neural network, we instead tweaked the input image itself until we get the answer we want? So let\u2019s take the already-trained neural network and \u201ctrain\u201d it again. But let\u2019s use back-propagation to adjust the input image instead of the neural network layers: So here\u2019s the new algorithm: At end of this, we\u2019ll have an image that fools the neural network without changing anything inside the neural network itself. The only problem is that by allowing any single pixel to be adjusted without any limitations, the changes to the image can be drastic enough that you\u2019ll see them. They\u2019ll show up as discolored spots or wavy areas: To prevent these obvious distortions, we can add a simple constraint to our algorithm. We\u2019ll say that no single pixel in the hacked image can ever be changed by more than a tiny amount from the original image \u2014 let\u2019s say something like 0.01%. That forces our algorithm to tweak the image in a way that still fools the neural network without it looking too different from the original image. Here\u2019s what the generated image looks like when we add that constraint: Even though that image looks the same to us, it still fools the neural network! To code this, first we need a pre-trained neural network to fool. Instead of training one from scratch, let\u2019s use one created by Google. Keras, the popular deep learning framework, comes with several pre-trained neural networks. We\u2019ll use its copy of Google\u2019s Inception v3 deep neural network that was pre-trained to detect 1000 different kinds of objects. Here\u2019s the basic code in Keras to recognize what\u2019s in a picture using this neural network. Just make sure you have Python 3 and Keras installed before you run it: When we run it, it properly detects our image as a Persian cat: Now let\u2019s trick it into thinking that this cat is a toaster by tweaking the image until it fools the neural network. Keras doesn\u2019t have a built-in way to train against the input image instead of training the neural network layers, so I had to get a little tricky and code the training step manually. Here\u2019s the code: If we run this, it will eventually spit out an image that will fool the neural network: Note: If you don\u2019t have a GPU, this might take a few hours to run. If you do have a GPU properly configured with Keras and CUDA, it shouldn\u2019t take more than a couple of minutes to run. Now let\u2019s test the hacked image that we just made by running it through the original model again: We did it! We tricked the neural network into thinking that a cat is a toaster! Created a hacked image like this is called \u201cgenerating an adversarial example\u201d. We\u2019re intentionally crafting a piece of data so that a machine-learning model will misclassify it. It\u2019s a neat trick, but why does this matter in the real world? Research has show that these hacked images have some surprising properties: So we can potentially do a lot with these hacked images! But there is still a big limitation with how we create these images \u2014 our attack requires direct access to the neural network itself. Because we are actually \u201ctraining\u201d against the neural network to fool it, we need a copy of it. In the real world, no company is going to let you download their trained neural network\u2019s code, so that means we can\u2019t attack them\u2026 Right? Nope! Researchers have recently shown that you can train your own substitute neural network to mirror another neural network by probing it to see how it behaves. Then you can use your substitute neural network to generate hacked images that still often fool the original network! This is called a black-box attack. The applications of black-box attacks are limitless. Here are some plausible examples: And these attack methodology isn\u2019t limited to just images. You can use the same kind of approach to fool classifiers that work on other types of data. For example, you could trick virus scanners into recognizing your virus as safe code! So now that we know it\u2019s possible to trick neural networks (and all other machine learning models too), how do we defend against this? The short answer is that no one is entirely sure yet. Preventing these kinds of attacks is still an on-going area of research. The best way to keep up with the latest developments is by reading the cleverhans blog maintained by Ian Goodfellow and Nicolas Papernot, two of the most influential researchers in this area. But there are some things we do know so far: Since we don\u2019t have any final answers yet, its worth thinking about the scenarios where you are using neural networks so that you can at least lessen the risk that this kind of attack would cause damage your business. For example, if you have a single machine learning model as the only line of defense to grant access to a restricted resource and assume it can\u2019t be fooled, that\u2019s probably a bad idea. But if you use machine learning as a step in a process where there is still human verification, that\u2019s probably fine. In other words, treat machine learning models in your architecture like any other component that can potentially be bypassed. Think through the implications of what would happen if a user intentionally sets out to fool them and think of ways to mitigate those scenarios. Want to learn more about Adversarial Examples and protecting against them? If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I\u2019ll only email you when I have something new and awesome to share. It\u2019s the best way to find out when I write more articles like this. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I\u2019d love to hear from you if I can help you or your team with machine learning. -- -- 18 Interested in computers and machine learning. Likes to write about it. Adam Geitgey -- 26 Adam Geitgey -- 195 Adam Geitgey -- 263 Adam Geitgey -- 62 The PyCoach in Artificial Corner -- 327 Alex Mathers -- 203 Unbecoming -- 757 Bryan Ye in Better Humans -- 638 Matt Chapman in Towards Data Science -- 43 Aleid ter Weel in Better Advice -- 246 Help Status Writers Blog Careers Privacy Terms About Text to speech", "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196", "threshold": -0.999958891299865}