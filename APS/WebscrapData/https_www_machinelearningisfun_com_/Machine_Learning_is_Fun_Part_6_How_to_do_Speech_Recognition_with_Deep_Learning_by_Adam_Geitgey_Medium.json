{"title": "Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning | by Adam Geitgey | Medium", "content": "Adam Geitgey Follow -- 72 Listen Share Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in \u666e\u901a\u8bdd , \ud55c\uad6d\uc5b4, Ti\u1ebfng Vi\u1ec7t, \u0641\u0627\u0631\u0633\u06cc or \u0420\u0443\u0441\u0441\u043a\u0438\u0439. Giant update: I\u2019ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! Speech recognition is invading our lives. It\u2019s built into our phones, our game consoles and our smart watches. It\u2019s even automating our homes. For just $50, you can get an Amazon Echo Dot \u2014 a magic box that allows you to order pizza, get a weather report or even buy trash bags \u2014 just by speaking out loud: The Echo Dot has been so popular this holiday season that Amazon can\u2019t seem to keep them in stock! But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments. Andrew Ng has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between annoyingly unreliable and incredibly useful. Thanks to Deep Learning, we\u2019re finally cresting that peak. Let\u2019s learn how to do speech recognition with deep learning! If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text: That\u2019s the holy grail of speech recognition with deep learning, but we aren\u2019t quite there yet (at least at the time that I wrote this \u2014 I bet that we will be in a couple of years). The big problem is that speech varies in speed. One person might say \u201chello!\u201d very quickly and another person might say \u201cheeeelllllllllllllooooo!\u201d very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text \u2014 \u201chello!\u201d Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard. To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let\u2019s see how it works! The first step in speech recognition is obvious \u2014 we need to feed sound waves into a computer. In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition: But sound is transmitted as waves. How do we turn sound waves into numbers? Let\u2019s use this sound clip of me saying \u201cHello\u201d: Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let\u2019s zoom in on one tiny part of the sound wave and take a look: To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points: This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That\u2019s basically all an uncompressed .wav audio file is. \u201cCD Quality\u201d audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech. Lets sample our \u201cHello\u201d sound wave 16,000 times per second. Here\u2019s the first 100 samples: You might be thinking that sampling is only creating a rough approximation of the original sound wave because it\u2019s only taking occasional readings. There\u2019s gaps in between our readings so we must be losing data, right? But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples \u2014 as long as we sample at least twice as fast as the highest frequency we want to record. I mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality. It doesn\u2019t. </end rant> We now have an array of numbers with each number representing the sound wave\u2019s amplitude at 1/16,000th of a second intervals. We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data. Let\u2019s start by grouping our sampled audio into 20-millisecond-long chunks. Here\u2019s our first 20 milliseconds of audio (i.e., our first 320 samples): Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time: This recording is only 1/50th of a second long. But even this short recording is a complex mish-mash of different frequencies of sound. There\u2019s some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech. To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it\u2019s component parts. We\u2019ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet. Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes\u2014 C, E and G \u2014 all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea. We do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one. The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip: But this is a lot easier to see when you draw this as a chart: If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk): A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we\u2019ll actually feed into our neural network. Now that we have our audio in a format that\u2019s easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the letter that corresponds the sound currently being spoken. We\u2019ll use a recurrent neural network \u2014 that is, a neural network that has a memory that influences future predictions. That\u2019s because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said \u201cHEL\u201d so far, it\u2019s very likely we will say \u201cLO\u201d next to finish out the word \u201cHello\u201d. It\u2019s much less likely that we will say something unpronounceable next like \u201cXYZ\u201d. So having that memory of previous predictions helps the neural network make more accurate predictions going forward. After we run our entire audio clip through the neural network (one chunk at a time), we\u2019ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here\u2019s what that mapping looks like for me saying \u201cHello\u201d: Our neural net is predicting that one likely thing I said was \u201cHHHEE_LL_LLLOOO\u201d. But it also thinks that it was possible that I said \u201cHHHUU_LL_LLLOOO\u201d or even \u201cAAAUU_LL_LLLOOO\u201d. We have some steps we follow to clean up this output. First, we\u2019ll replace any repeated characters a single character: Then we\u2019ll remove any blanks: That leaves us with three possible transcriptions \u2014 \u201cHello\u201d, \u201cHullo\u201d and \u201cAullo\u201d. If you say them out loud, all of these sound similar to \u201cHello\u201d. Because it\u2019s predicting one character at a time, the neural network will come up with these very sounded-out transcriptions. For example if you say \u201cHe would not go\u201d, it might give one possible transcription as \u201cHe wud net go\u201d. The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic. Of our possible transcriptions \u201cHello\u201d, \u201cHullo\u201d and \u201cAullo\u201d, obviously \u201cHello\u201d will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we\u2019ll pick \u201cHello\u201d as our final transcription instead of the others. Done! You might be thinking \u201cBut what if someone says \u2018Hullo\u2019? It\u2019s a valid word. Maybe \u2018Hello\u2019 is the wrong transcription!\u201d Of course it is possible that someone actually said \u201cHullo\u201d instead of \u201cHello\u201d. But a speech recognition system like this (trained on American English) will basically never produce \u201cHullo\u201d as the transcription. It\u2019s just such an unlikely thing for a user to say compared to \u201cHello\u201d that it will always think you are saying \u201cHello\u201d no matter how much you emphasize the \u2018U\u2019 sound. Try it out! If your phone is set to American English, try to get your phone\u2019s digital assistant to recognize the world \u201cHullo.\u201d You can\u2019t! It refuses! It will always understand it as \u201cHello.\u201d Not recognizing \u201cHullo\u201d is a reasonable behavior, but sometimes you\u2019ll find annoying cases where your phone just refuses to understand something valid you are saying. That\u2019s why these speech recognition models are always being retrained with more data to fix these edge cases. One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop\u2019s video card\u2026 Right? That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them. Here\u2019s another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise! To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a lot of training data \u2014 far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can\u2019t skimp on this. No one wants a voice recognition system that works 80% of the time. For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That\u2019s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That\u2019s the whole game! Don\u2019t believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you\u2019ve ever said into it: So if you are looking for a start-up idea, I wouldn\u2019t recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead. If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I\u2019ll only email you when I have something new and awesome to share. It\u2019s the best way to find out when I write more articles like this. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I\u2019d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun! Part 7! -- -- 72 Interested in computers and machine learning. Likes to write about it. Adam Geitgey -- 26 Adam Geitgey -- 195 Adam Geitgey -- 263 Adam Geitgey -- 62 The PyCoach in Artificial Corner -- 332 Matt Chapman in Towards Data Science -- 44 Somnath Singh in JavaScript in Plain English -- 263 Albers Uzila in Level Up Coding -- 14 Victor Murcia -- Zach Quinn in Pipeline: Your Data Engineering Resource -- 46 Help Status Writers Blog Careers Privacy Terms About Text to speech", "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a", "threshold": 0.9875741494287691}