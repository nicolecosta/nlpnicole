{"title": "Understanding Convolutions on Graphs", "content": "Understanding the building blocks and design choices of graph neural networks. Ameya Daigavane Google Research Balaraman Ravindran Google Research Gaurav Aggarwal Google Research Sept. 2, 2021 10.23915/distill.00032         This article is one of two Distill publications about graph neural networks.        Take a look at        A Gentle Introduction to Graph Neural Networks        for a companion view on many things graph and neural network related.               Many systems and interactions - social networks, molecules, organizations, citations, physical models, transactions - can be represented quite naturally as graphs.        How can we reason about and make predictions within these systems?               One idea is to look at tools that have worked well in other domains: neural networks have shown immense predictive power in a variety of learning tasks.        However, neural networks have been traditionally used to operate on fixed-size and/or regular-structured inputs (such as sentences, images and video).        This makes them unable to elegantly process graph-structured data.             Graph neural networks (GNNs) are a family of neural networks that can operate naturally on graph-structured data.       By extracting and utilizing features from the underlying graph,      GNNs can make more informed predictions about entities in these interactions,      as compared to models that consider individual entities in isolation.           GNNs are not the only tools available to model graph-structured data:      graph kernels       and random-walk methods       were some of the most popular ones.      Today, however, GNNs have largely replaced these techniques      because of their inherent flexibility to model the underlying systems      better.           In this article, we will illustrate      the challenges of computing over graphs,       describe the origin and design of graph neural networks,      and explore the most popular GNN variants in recent times.      Particularly, we will see that many of these variants      are composed of similar building blocks.           First, let\u2019s discuss some of the complications that graphs come with.             Graphs are extremely flexible mathematical models; but this means they lack consistent structure across instances.        Consider the task of predicting whether a given chemical molecule is toxic \u00a0:               Looking at a few examples, the following issues quickly become apparent:               Representing graphs in a format that can be computed over is non-trivial,        and the final representation chosen often depends significantly on the actual problem.             Extending the point above: graphs often have no inherent ordering present amongst the nodes.      Compare this to images, where every pixel is uniquely determined by its absolute position within the image!           As a result, we would like our algorithms to be node-order equivariant:      they should not depend on the ordering of the nodes of the graph.      If we permute the nodes in some way, the resulting representations of       the nodes as computed by our algorithms should also be permuted in the same way.             Graphs can be really large! Think about social networks like Facebook and Twitter, which have over a billion users.         Operating on data this large is not easy.               Luckily, most naturally occuring graphs are \u2018sparse\u2019:        they tend to have their number of edges linear in their number of vertices.        We will see that this allows the use of clever methods        to efficiently compute representations of nodes within the graph.        Further, the methods that we look at here will have significantly fewer parameters        in comparison to the size of the graphs they operate on.               There are many useful problems that can be formulated over graphs:               A common precursor in solving many of these problems is node representation learning:        learning to map individual nodes to fixed-size real-valued vectors (called \u2018representations\u2019 or \u2018embeddings\u2019).               In Learning GNN Parameters, we will see how the learnt embeddings can be used for these tasks.               Different GNN variants are distinguished by the way these representations are computed.        Generally, however, GNNs compute node representations in an iterative process.        We will use the notation hv(k)h_v^{(k)}hv(k)\u200b to indicate the representation of node vvv after the kthk^{\\text{th}}kth iteration.        Each iteration can be thought of as the equivalent of a \u2018layer\u2019 in standard neural networks.               We will define a graph GGG as a set of nodes, VVV, with a set of edges EEE connecting them.        Nodes can have individual features as part of the input: we will denote by xvx_vxv\u200b the individual feature for node v\u2208Vv \\in Vv\u2208V.        For example, the \u2018node features\u2019 for a pixel in a color image        would be the red, green and blue channel (RGB) values at that pixel.               For ease of exposition, we will assume GGG is undirected, and all nodes are of the same type.        These kinds of graphs are called \u2018homogeneous\u2019.        Many of the same ideas we will see here         apply to other kinds of graphs:        we will discuss this later in Different Kinds of Graphs.               Sometimes we will need to denote a graph property by a matrix MMM,        where each row MvM_vMv\u200b represents a property corresponding to a particular vertex vvv.             Convolutional Neural Networks have been seen to be quite powerful in extracting features from images.      However, images themselves can be seen as graphs with a very regular grid-like structure,      where the individual pixels are nodes, and the RGB channel values at each pixel as the node features.           A natural idea, then, is to consider generalizing convolutions to arbitrary graphs. Recall, however, the challenges      listed out in the previous section: in particular, ordinary convolutions are not node-order invariant, because      they depend on the absolute positions of pixels.      It is initially unclear as how to generalize convolutions over grids to convolutions over general graphs,      where the neighbourhood structure differs from node to node.              The curious reader may wonder if performing some sort of padding and ordering        could be done to ensure the consistency of neighbourhood structure across nodes.        This has been attempted with some success ,        but the techniques we will look at here are more general and powerful.             We begin by introducing the idea of constructing polynomial filters over node neighbourhoods,      much like how CNNs compute localized filters over neighbouring pixels.      Then, we will see how more recent approaches extend on this idea with more powerful mechanisms.      Finally, we will discuss alternative methods      that can use \u2018global\u2019 graph-level information for computing node representations.             Given a graph GGG, let us fix an arbitrary ordering of the nnn nodes of GGG.        We denote the 0\u221210-10\u22121 adjacency matrix of GGG by AAA, we can construct the diagonal degree matrix DDD of GGG as:                     where AvuA_{vu}Avu\u200b denotes the entry in the row corresponding to vvv and the column corresponding to uuu        in the matrix AAA. We will use this notation throughout this section.               Then, the graph Laplacian LLL is the square n\u00d7nn \\times nn\u00d7n matrix defined as:        L=D\u2212A.          L = D - A.        L=D\u2212A.         The graph Laplacian gets its name from being the discrete analog of the        Laplacian operator        from calculus.               Although it encodes precisely the same information as the adjacency matrix AAA          In the sense that given either of the matrices AAA or LLL, you can construct the other.        ,        the graph Laplacian has many interesting properties of its own.                  The graph Laplacian shows up in many mathematical problems involving graphs:          random walks,          spectral clustering,          and          diffusion, to name a few.                We will see some of these properties        in a later section,        but will instead point readers to        this tutorial        for greater insight into the graph Laplacian.               Now that we have understood what the graph Laplacian is,        we can build polynomials  of the form:        pw(L)=w0In+w1L+w2L2+\u2026+wdLd=\u2211i=0dwiLi.          p_w(L) = w_0 I_n + w_1 L + w_2 L^2 + \\ldots + w_d L^d = \\sum_{i = 0}^d w_i L^i.        pw\u200b(L)=w0\u200bIn\u200b+w1\u200bL+w2\u200bL2+\u2026+wd\u200bLd=i=0\u2211d\u200bwi\u200bLi.        Each polynomial of this form can alternately be represented by        its vector of coefficients w=[w0,\u2026,wd]w = [w_0, \\ldots, w_d]w=[w0\u200b,\u2026,wd\u200b].        Note that for every www, pw(L)p_w(L)pw\u200b(L) is an n\u00d7nn \\times nn\u00d7n matrix, just like LLL.               These polynomials can be thought of as the equivalent of \u2018filters\u2019 in CNNs,        and the coefficients www as the weights of the \u2018filters\u2019.               For ease of exposition, we will focus on the case where nodes have one-dimensional features:        each of the xvx_vxv\u200b for v\u2208Vv \\in Vv\u2208V is just a real number.         The same ideas hold when each of the xvx_vxv\u200b are higher-dimensional vectors, as well.               Using the previously chosen ordering of the nodes,        we can stack all of the node features xvx_vxv\u200b        to get a vector x\u2208Rnx \\in \\mathbb{R}^nx\u2208Rn.               Once we have constructed the feature vector xxx,        we can define its convolution with a polynomial filter pwp_wpw\u200b as:        x\u2032=pw(L)\u00a0x          x\u2019 = p_w(L) \\ x        x\u2032=pw\u200b(L)\u00a0x        To understand how the coefficients www affect the convolution,        let us begin by considering the \u2018simplest\u2019 polynomial:        when w0=1w_0 = 1w0\u200b=1 and all of the other coefficients are 000.        In this case, x\u2032x\u2019x\u2032 is just xxx:        x\u2032=pw(L)\u00a0x=\u2211i=0dwiLix=w0Inx=x.          x\u2019 = p_w(L) \\ x = \\sum_{i = 0}^d w_i L^ix = w_0 I_n x = x.        x\u2032=pw\u200b(L)\u00a0x=i=0\u2211d\u200bwi\u200bLix=w0\u200bIn\u200bx=x.        Now, if we increase the degree, and consider the case where        instead w1=1w_1 = 1w1\u200b=1 and and all of the other coefficients are 000.        Then, x\u2032x\u2019x\u2032 is just LxLxLx, and so:        xv\u2032=(Lx)v=Lvx=\u2211u\u2208GLvuxu=\u2211u\u2208G(Dvu\u2212Avu)xu=Dv\u00a0xv\u2212\u2211u\u2208N(v)xu          \\begin{aligned}           x\u2019_v = (Lx)_v &= L_v x \\\\                          &= \\sum_{u \\in G} L_{vu} x_u \\\\                          &= \\sum_{u \\in G} (D_{vu} - A_{vu}) x_u \\\\                          &= D_v \\ x_v - \\sum_{u \\in \\mathcal{N}(v)} x_u          \\end{aligned}        xv\u2032\u200b=(Lx)v\u200b\u200b=Lv\u200bx=u\u2208G\u2211\u200bLvu\u200bxu\u200b=u\u2208G\u2211\u200b(Dvu\u200b\u2212Avu\u200b)xu\u200b=Dv\u200b\u00a0xv\u200b\u2212u\u2208N(v)\u2211\u200bxu\u200b\u200b        We see that the features at each node vvv are combined        with the features of its immediate neighbours u\u2208N(v)u \\in \\mathcal{N}(v)u\u2208N(v).                  For readers familiar with          Laplacian filtering of images,          this is the exact same idea. When xxx is an image,           x\u2032=Lxx\u2019 = Lxx\u2032=Lx is exactly the result of applying a \u2018Laplacian filter\u2019 to xxx.                 At this point, a natural question to ask is:        How does the degree ddd of the polynomial influence the behaviour of the convolution?        Indeed, it is not too hard to show that:        This is Lemma 5.2 from .distG(v,u)>i\u27f9Lvui=0.          \\text{dist}_G(v, u) > i \\quad \\Longrightarrow \\quad L_{vu}^i = 0.        distG\u200b(v,u)>i\u27f9Lvui\u200b=0.              This implies, when we convolve xxx with pw(L)p_w(L)pw\u200b(L) of degree ddd to get x\u2032x\u2019x\u2032:        xv\u2032=(pw(L)x)v=(pw(L))vx=\u2211i=0dwiLvix=\u2211i=0dwi\u2211u\u2208GLvuixu=\u2211i=0dwi\u2211u\u2208GdistG(v,u)\u2264iLvuixu.          \\begin{aligned}          x\u2019_v = (p_w(L)x)_v  &= (p_w(L))_v x \\\\              &= \\sum_{i = 0}^d w_i L_v^i x \\\\              &= \\sum_{i = 0}^d w_i \\sum_{u \\in G} L_{vu}^i x_u \\\\              &= \\sum_{i = 0}^d w_i \\sum_{u \\in G \\atop \\text{dist}_G(v, u) \\leq i} L_{vu}^i x_u.          \\end{aligned}        xv\u2032\u200b=(pw\u200b(L)x)v\u200b\u200b=(pw\u200b(L))v\u200bx=i=0\u2211d\u200bwi\u200bLvi\u200bx=i=0\u2211d\u200bwi\u200bu\u2208G\u2211\u200bLvui\u200bxu\u200b=i=0\u2211d\u200bwi\u200bdistG\u200b(v,u)\u2264iu\u2208G\u200b\u2211\u200bLvui\u200bxu\u200b.\u200b         Effectively, the convolution at node vvv occurs only with nodes uuu which are not more than ddd hops away.        Thus, these polynomial filters are localized. The degree of the localization is governed completely by ddd.               To help you understand these \u2018polynomial-based\u2019 convolutions better, we have created the visualization below.        Vary the polynomial coefficients and the input grid xxx to see how the result x\u2032x\u2019x\u2032 of the convolution changes.        The grid under the arrow shows the equivalent convolutional kernel applied at the highlighted pixel in xxx to get        the resulting pixel in x\u2032x\u2019x\u2032.        The kernel corresponds to the row of pw(L)p_w(L)pw\u200b(L) for the highlighted pixel.        Note that even after adjusting for position,        this kernel is different for different pixels, depending on their position within the grid.                     Hover over a pixel in the input grid (left, representing xxx)              to highlight it and see the equivalent convolutional kernel              for that pixel under the arrow.              The result x\u2032x\u2019x\u2032 of the convolution is shown on the right:              note that different convolutional kernels are applied at different pixels,              depending on their location.                           Click on the input grid to toggle pixel values between 000 (white) and 111 (blue).              To randomize the input grid, press \u2018Randomize Grid\u2019. To reset all pixels to 000, press \u2018Reset Grid\u2019.              Use the sliders at the bottom to change the coefficients www.              To reset all coefficients www to 000, press \u2018Reset Coefficients.\u2019                     What is the motivation behind these choices?               The polynomial filters we considered here are actually independent of the ordering of the nodes.        This is particularly easy to see when the degree of the polynomial pwp_wpw\u200b is 111:        where each node\u2019s feature is aggregated with the sum of its neighbour\u2019s features.        Clearly, this sum does not depend on the order of the neighbours.        A similar proof follows for higher degree polynomials:        the entries in the powers of LLL are equivariant to the ordering of the nodes.                 As above, let\u2019s assume an arbitrary node-order over the nnn nodes of our graph.          Any other node-order can be thought of as a permutation of this original node-order.          We can represent any permutation by a          permutation matrix PPP.          PPP will always be an orthogonal 0\u221210-10\u22121 matrix:          PPT=PTP=In.            PP^T = P^TP = I_n.          PPT=PTP=In\u200b.          Then, we call a function fff node-order equivariant iff for all permutations PPP:          f(Px)=Pf(x).            f(Px) = P f(x).          f(Px)=Pf(x).          When switching to the new node-order using the permutation PPP,          the quantities below transform in the following way:          x\u2192PxL\u2192PLPTLi\u2192PLiPT            \\begin{aligned}              x &\\to Px \\\\              L &\\to PLP^T \\\\              L^i &\\to PL^iP^T            \\end{aligned}          xLLi\u200b\u2192Px\u2192PLPT\u2192PLiPT\u200b          and so, for the case of polynomial filters where f(x)=pw(L)\u00a0xf(x) = p_w(L) \\ xf(x)=pw\u200b(L)\u00a0x, we can see that:          f(Px)=\u2211i=0dwi(PLiPT)(Px)=P\u2211i=0dwiLix=Pf(x).            \\begin{aligned}              f(Px) & = \\sum_{i = 0}^d w_i (PL^iP^T) (Px) \\\\                    & = P \\sum_{i = 0}^d w_i L^i x \\\\                    & = P f(x).            \\end{aligned}          f(Px)\u200b=i=0\u2211d\u200bwi\u200b(PLiPT)(Px)=Pi=0\u2211d\u200bwi\u200bLix=Pf(x).\u200b           as claimed.                   We now describe how we can build a graph neural network          by stacking ChebNet (or any polynomial filter) layers          one after the other with non-linearities,          much like a standard CNN.          In particular, if we have KKK different polynomial filter layers,          the kthk^{\\text{th}}kth of which has its own learnable weights w(k)w^{(k)}w(k),          we would perform the following computation:                   Note that these networks          reuse the same filter weights across different nodes,          exactly mimicking weight-sharing in Convolutional Neural Networks (CNNs)          which reuse weights for convolutional filters across a grid.                 ChebNet was a breakthrough in learning localized filters over graphs,        and it motivated many to think of graph convolutions from a different perspective.                As we noted before, this is a 111-hop localized convolution.        But more importantly, we can think of this convolution as arising of two steps:       Key Idea:        What if we consider different kinds of \u2018aggregation\u2019 and \u2018combination\u2019 steps,        beyond what are possible using polynomial filters?               By ensuring that the aggregation is node-order equivariant,        the overall convolution becomes node-order equivariant.               These convolutions can be thought of as \u2018message-passing\u2019 between adjacent nodes:        after each step, every node receives some \u2018information\u2019 from its neighbours.               By iteratively repeating the 111-hop localized convolutions KKK times (i.e., repeatedly \u2018passing messages\u2019),        the receptive field of the convolution effectively includes all nodes upto KKK hops away.                 Message-passing forms the backbone of many GNN architectures today.          We describe the most popular ones in depth below:                 An interesting point is to assess different aggregation functions: are some better and others worse?         demonstrates that aggregation functions indeed can be compared on how well        they can uniquely preserve node neighbourhood features;        we recommend the interested reader take a look at the detailed theoretical analysis there.               Here, we\u2019ve talk about GNNs where the computation only occurs at the nodes.        More recent GNN models        such as Message-Passing Neural Networks         and Graph Networks         perform computation over the edges as well;        they compute edge embeddings together with node embeddings.        This is an even more general framework -        but the same \u2018message passing\u2019 ideas from this section apply.               Below is an interactive visualization of these GNN models on small graphs.        For clarity, the node features are just real numbers here, shown inside the squares next to each node,        but the same equations hold when the node features are vectors.               In practice, each iteration above is generally thought of as a single \u2018neural network layer\u2019.        This ideology is followed by many popular Graph Neural Network libraries,                For example: PyTorch Geometric        and StellarGraph.                allowing one to compose different types of graph convolutions in the same model.               The methods we\u2019ve seen so far perform \u2018local\u2019 convolutions:        every node\u2019s feature is updated using a function of its local neighbours\u2019 features.               While performing enough steps of message-passing will eventually ensure that        information from all nodes in the graph is passed,        one may wonder if there are more direct ways to perform \u2018global\u2019 convolutions.               The answer is yes; we will now describe an approach that was actually first put forward        in the context of neural networks by ,        much before any of the GNN models we looked at above.               As before, we will focus on the case where nodes have one-dimensional features.        After choosing an arbitrary node-order, we can stack all of the node features to get a        \u2018feature vector\u2019 x\u2208Rnx \\in \\mathbb{R}^nx\u2208Rn.       Key Idea:        Given a feature vector xxx,         the Laplacian LLL allows us to quantify how smooth xxx is, with respect to GGG.               How?               After normalizing xxx such that \u2211i=1nxi2=1\\sum_{i = 1}^n x_i^2 = 1\u2211i=1n\u200bxi2\u200b=1,        if we look at the following quantity involving LLL:        RLR_LRL\u200b is formally called the Rayleigh quotient.        RL(x)=xTLxxTx=\u2211(i,j)\u2208E(xi\u2212xj)2\u2211ixi2=\u2211(i,j)\u2208E(xi\u2212xj)2.          R_L(x) = \\frac{x^T L x}{x^T x} = \\frac{\\sum_{(i, j) \\in E} (x_i - x_j)^2}{\\sum_i x_i^2} = \\sum_{(i, j) \\in E} (x_i - x_j)^2.        RL\u200b(x)=xTxxTLx\u200b=\u2211i\u200bxi2\u200b\u2211(i,j)\u2208E\u200b(xi\u200b\u2212xj\u200b)2\u200b=(i,j)\u2208E\u2211\u200b(xi\u200b\u2212xj\u200b)2.        we immediately see that feature vectors xxx that assign similar values to         adjacent nodes in GGG (hence, are smooth) would have smaller values of RL(x)R_L(x)RL\u200b(x).       LLL is a real, symmetric matrix, which means it has all real eigenvalues \u03bb1\u2264\u2026\u2264\u03bbn\\lambda_1 \\leq \\ldots \\leq \\lambda_{n}\u03bb1\u200b\u2264\u2026\u2264\u03bbn\u200b.                  An eigenvalue \u03bb\\lambda\u03bb of a matrix AAA is a value          satisfying the equation Au=\u03bbuAu = \\lambda uAu=\u03bbu for a certain vector uuu, called an eigenvector.          For a friendly introduction to eigenvectors,          please see this tutorial.                Further, the corresponding eigenvectors u1,\u2026,unu_1, \\ldots, u_{n}u1\u200b,\u2026,un\u200b can be taken to be orthonormal:        uk1Tuk2={1\u00a0if\u00a0k1=k2.0\u00a0if\u00a0k1\u2260k2.          u_{k_1}^T u_{k_2} =          \\begin{cases}            1 \\quad \\text{ if } {k_1} = {k_2}. \\\\            0 \\quad \\text{ if } {k_1} \\neq {k_2}.          \\end{cases}        uk1\u200bT\u200buk2\u200b\u200b={1\u00a0if\u00a0k1\u200b=k2\u200b.0\u00a0if\u00a0k1\u200b\u2260k2\u200b.\u200b        It turns out that these eigenvectors of LLL are successively less smooth, as RLR_LRL\u200b indicates:        This is the min-max theorem for eigenvalues.argminx,\u00a0x\u22a5{u1,\u2026,ui\u22121}RL(x)=ui.minx,\u00a0x\u22a5{u1,\u2026,ui\u22121}RL(x)=\u03bbi.          \\underset{x, \\ x \\perp \\{u_1, \\ldots, u_{i - 1}\\}}{\\text{argmin}} R_L(x) = u_i.          \\qquad          \\qquad          \\qquad          \\min_{x, \\ x \\perp \\{u_1, \\ldots, u_{i - 1}\\}} R_L(x) = \\lambda_i.        x,\u00a0x\u22a5{u1\u200b,\u2026,ui\u22121\u200b}argmin\u200bRL\u200b(x)=ui\u200b.x,\u00a0x\u22a5{u1\u200b,\u2026,ui\u22121\u200b}min\u200bRL\u200b(x)=\u03bbi\u200b.        The set of eigenvalues of LLL are called its \u2018spectrum\u2019, hence the name!        We denote the \u2018spectral\u2019 decomposition of LLL as:        L=U\u039bUT.          L = U \\Lambda U^T.        L=U\u039bUT.        where \u039b\\Lambda\u039b is the diagonal matrix of sorted eigenvalues,        and UUU denotes the matrix of the eigenvectors (sorted corresponding to increasing eigenvalues):        \u039b=[\u03bb1\u22f1\u03bbn]U=[u1\u00a0\u22ef\u00a0un].          \\Lambda = \\begin{bmatrix}                        \\lambda_{1} &        & \\\\                                    & \\ddots & \\\\                                    &        & \\lambda_{n}                    \\end{bmatrix}          \\qquad          \\qquad          \\qquad          \\qquad          U = \\begin{bmatrix}  \\\\ u_1 \\ \\cdots \\ u_n \\\\ \\end{bmatrix}.        \u039b=\u23a3\u23a1\u200b\u03bb1\u200b\u200b\u22f1\u200b\u03bbn\u200b\u200b\u23a6\u23a4\u200bU=\u23a3\u23a1\u200bu1\u200b\u00a0\u22ef\u00a0un\u200b\u200b\u23a6\u23a4\u200b.        The orthonormality condition between eigenvectors gives us that UTU=IU^T U = IUTU=I, the identity matrix.        As these nnn eigenvectors form a basis for Rn\\mathbb{R}^nRn,        any feature vector xxx can be represented as a linear combination of these eigenvectors:        x=\u2211i=1nxi^ui=Ux^.          x = \\sum_{i = 1}^n \\hat{x_i} u_i = U \\hat{x}.        x=i=1\u2211n\u200bxi\u200b^\u200bui\u200b=Ux^.        where x^\\hat{x}x^ is the vector of coefficients [x0,\u2026xn][x_0, \\ldots x_n][x0\u200b,\u2026xn\u200b].        We call x^\\hat{x}x^ as the spectral representation of the feature vector xxx.        The orthonormality condition allows us to state:        x=Ux^\u27faUTx=x^.          x = U \\hat{x} \\quad \\Longleftrightarrow \\quad U^T x = \\hat{x}.        x=Ux^\u27faUTx=x^.        This pair of equations allows us to interconvert        between the \u2018natural\u2019 representation xxx and the \u2018spectral\u2019 representation x^\\hat{x}x^        for any vector x\u2208Rnx \\in \\mathbb{R}^nx\u2208Rn.               As discussed before, we can consider any image as a grid graph, where each pixel is a node,        connected by edges to adjacent pixels.        Thus, a pixel can have either 3,5,3, 5,3,5, or 888 neighbours, depending on its location within the image grid.        Each pixel gets a value as part of the image. If the image is grayscale, each value will be a single         real number indicating how dark the pixel is. If the image is colored, each value will be a 333-dimensional        vector, indicating the values for the red, green and blue (RGB) channels.        We use the alpha channel as well in the visualization below, so this is actually RGBA.         This construction allows us to compute the graph Laplacian and the eigenvector matrix UUU.        Given an image, we can then investigate what its spectral representation looks like.               To shed some light on what the spectral representation actually encodes,        we perform the following experiment over each channel of the image independently:                 Finally, we stack the resulting channels back together to get back an image.        We can now see how the resulting image changes with choices of mmm.        Note that when m=nm = nm=n, the resulting image is identical to the original image,        as we can reconstruct each channel exactly.               As mmm decreases, we see that the output image xmx_mxm\u200b gets blurrier.        If we decrease mmm to 111, the output image xmx_mxm\u200b is entirely the same color throughout.        We see that we do not need to keep all nnn components;        we can retain a lot of the information in the image with significantly fewer components.        We can relate this to the Fourier decomposition of images:        the more eigenvectors we use, the higher frequencies we can represent on the grid.               To complement the visualization above,        we additionally visualize the first few eigenvectors on a smaller 8\u00d788 \\times 88\u00d78 grid below.        We change the coefficients of the first 101010 out of 646464 eigenvectors        in the spectral representation        and see how the resulting image changes:               These visualizations should convince you that the first eigenvectors are indeed smooth,        and the smoothness correspondingly decreases as we consider later eigenvectors.               For any image xxx, we can think of        the initial entries of the spectral representation x^\\hat{x}x^        as capturing \u2018global\u2019 image-wide trends, which are the low-frequency components,        while the later entries as capturing \u2018local\u2019 details, which are the high-frequency components.               We now have the background to understand spectral convolutions        and how they can be used to compute embeddings/feature representations of nodes.               As before, the model we describe below has KKK layers:        each layer kkk has learnable parameters w^(k)\\hat{w}^{(k)}w^(k),        called the \u2018filter weights\u2019.        These weights will be convolved with the spectral representations of the node features.        As a result, the number of weights needed in each layer is equal to mmm, the number of         eigenvectors used to compute the spectral representations.        We had shown in the previous section that we can take m\u226anm \\ll nm\u226an        and still not lose out on significant amounts of information.               Thus, convolution in the spectral domain enables the use of significantly fewer parameters        than just direct convolution in the natural domain.        Further, by virtue of the smoothness of the Laplacian eigenvectors across the graph,        using spectral representations automatically enforces an inductive bias for        neighbouring nodes to get similar representations.               Assuming one-dimensional node features for now,        the output of each layer is a vector of node representations h(k)h^{(k)}h(k),        where each node\u2019s representation corresponds to a row        of the vector.               We fix an ordering of the nodes in GGG. This gives us the adjacency matrix AAA and the graph Laplacian LLL,        allowing us to compute UmU_mUm\u200b.        Finally, we can describe the computation that the layers perform, one after the other:               The method above generalizes easily to the case where each h(k)\u2208Rdkh^{(k)} \\in \\mathbb{R}^{d_k}h(k)\u2208Rdk\u200b, as well:        see  for details.               With the insights from the previous section, we see that convolution in the spectral-domain of graphs        can be thought of as the generalization of convolution in the frequency-domain of images.               We can show spectral convolutions are node-order equivariant using a similar approach        as for Laplacian polynomial filters.                  As in our proof before,          let\u2019s fix an arbitrary node-order.          Then, any other node-order can be represented by a          permutation of this original node-order.          We can associate this permutation with its permutation matrix PPP.          Under this new node-order,          the quantities below transform in the following way:          x\u2192PxA\u2192PAPTL\u2192PLPTUm\u2192PUm            \\begin{aligned}              x &\\to Px \\\\              A &\\to PAP^T \\\\              L &\\to PLP^T \\\\              U_m &\\to PU_m            \\end{aligned}          xALUm\u200b\u200b\u2192Px\u2192PAPT\u2192PLPT\u2192PUm\u200b\u200b          which implies that, in the embedding computation:          x^\u2192(PUm)T(Px)=UmTx=x^w^\u2192(PUm)T(Pw)=UmTw=w^g^\u2192g^g\u2192(PUm)g^=P(Umg^)=Pg            \\begin{aligned}              \\hat{x} &\\to \\left(PU_m\\right)^T (Px) = U_m^T x = \\hat{x} \\\\              \\hat{w} &\\to \\left(PU_m\\right)^T (Pw) = U_m^T w = \\hat{w} \\\\              \\hat{g} &\\to \\hat{g} \\\\              g &\\to (PU_m)\\hat{g} = P(U_m\\hat{g}) = Pg            \\end{aligned}          x^w^g^\u200bg\u200b\u2192(PUm\u200b)T(Px)=UmT\u200bx=x^\u2192(PUm\u200b)T(Pw)=UmT\u200bw=w^\u2192g^\u200b\u2192(PUm\u200b)g^\u200b=P(Um\u200bg^\u200b)=Pg\u200b          Hence, as \u03c3\\sigma\u03c3 is applied elementwise:          f(Px)=\u03c3(Pg)=P\u03c3(g)=Pf(x)            f(Px) = \\sigma(Pg) = P \\sigma(g) = P f(x)          f(Px)=\u03c3(Pg)=P\u03c3(g)=Pf(x)          as required.          Further, we see that the spectral quantities x^,w^\\hat{x}, \\hat{w}x^,w^ and g^\\hat{g}g^\u200b          are unchanged by permutations of the nodes.                    Formally, they are what we would call node-order invariant.                   The theory of spectral convolutions is mathematically well-grounded;        however, there are some key disadvantages that we must talk about:               While spectral convolutions have largely been superseded by        \u2018local\u2019 convolutions for the reasons discussed above,        there is still much merit to understanding the ideas behind them.        Indeed, a recently proposed GNN model called Directional Graph Networks                actually uses the Laplacian eigenvectors        and their mathematical properties        extensively.               A simpler way to incorporate graph-level information        is to compute embeddings of the entire graph by pooling node        (and possibly edge) embeddings,        and then using the graph embedding to update node embeddings,        following an iterative scheme similar to what we have looked at here.        This is an approach used by Graph Networks        .        We will briefly discuss how graph-level embeddings        can be constructed in Pooling.        However, such approaches tend to ignore the underlying        topology of the graph that spectral convolutions can capture.               All of the embedding computations we\u2019ve described here, whether spectral or spatial, are completely differentiable.        This allows GNNs to be trained in an end-to-end fashion, just like a standard neural network,        once a suitable loss function L\\mathcal{L}L is defined:               The broad success of pre-training for natural language processing models        such as ELMo  and BERT         has sparked interest in similar techniques for GNNs        .        The key idea in each of these papers is to train GNNs to predict        local (eg. node degrees, clustering coefficient, masked node attributes)        and/or global graph properties (eg. pairwise distances, masked global attributes).               Another self-supervised technique is to enforce that neighbouring nodes get similar embeddings,        mimicking random-walk approaches such as node2vec  and DeepWalk :               where NR(v)N_R(v)NR\u200b(v) is a multi-set of nodes visited when random walks are started from vvv.        For large graphs, where computing the sum over all nodes may be computationally expensive,        techniques such as Noise Contrastive Estimation  are especially useful.               While we have looked at many techniques and ideas in this article,        the field of Graph Neural Networks is extremely vast.        We have been forced to restrict our discussion to a small subset of the entire literature,        while still communicating the key ideas and design principles behind GNNs.        We recommend the interested reader take a look at         for a more comprehensive survey.               We end with pointers and references for additional concepts readers might be interested in:               It turns out that accomodating the different structures of graphs is often hard to do efficiently,        but we can still represent many GNN update equations using        as sparse matrix-vector products (since generally, the adjacency matrix is sparse for most real-world graph datasets.)        For example, the GCN variant discussed here can be represented as:        h(k)=D\u22121A\u22c5h(k\u22121)W(k)T+h(k\u22121)B(k)T.          h^{(k)} = D^{-1} A \\cdot h^{(k - 1)} {W^{(k)}}^T + h^{(k - 1)} {B^{(k)}}^T.        h(k)=D\u22121A\u22c5h(k\u22121)W(k)T+h(k\u22121)B(k)T.        Restructuring the update equations in this way allows for efficient vectorized implementations of GNNs on accelerators        such as GPUs.               Regularization techniques for standard neural networks,        such as Dropout ,        can be applied in a straightforward manner to the parameters        (for example, zero out entire rows of W(k)W^{(k)}W(k) above).        However, there are graph-specific techniques such as DropEdge         that removes entire edges at random from the graph,        that also boost the performance of many GNN models.               Here, we have focused on undirected graphs, to avoid going into too many unnecessary details.        However, there are some simple variants of spatial convolutions for:               There do exist more sophisticated techniques that can take advantage of the different structures of these graphs:        see  for more discussion.                 This article discusses how GNNs compute useful representations of nodes.          But what if we wanted to compute representations of graphs for graph-level tasks (for example, predicting the toxicity of a molecule)?                   A simple solution is to just aggregate the final node embeddings and pass them through another neural network PREDICTG\\text{PREDICT}_GPREDICTG\u200b:          hG=PREDICTG(AGGv\u2208G({hv}))            h_G = \\text{PREDICT}_G \\Big( \\text{AGG}_{v \\in G}\\left(\\{ h_v \\} \\right) \\Big)          hG\u200b=PREDICTG\u200b(AGGv\u2208G\u200b({hv\u200b}))          However, there do exist more powerful techniques for \u2018pooling\u2019 together node representations:                 The experiments from        Spectral Representations of Natural Images        can be reproduced using the following        Colab  notebook:        Spectral Representations of Natural Images.               To aid in the creation of future interactive articles,        we have created ObservableHQ                notebooks for each of the interactive visualizations here:             We are deeply grateful to ObservableHQ, a wonderful platform for developing interactive visualizations.      The static visualizations would not have been possible without Inkscape      and Alexander Lenail\u2019s Neural Network SVG Generator.      The molecule diagrams depicted above were obtained and modified from      Wikimedia Commons,      available in the public domain.           We would like to acknowledge the following Distill articles for inspiration on article design:           We would like to thank Thomas Kipf for his valuable feedback on the technical content within this article.           We would like to thank David Nichols for creating Coloring for Colorblindness      which helped us improve the accessibility of this article\u2019s color scheme.           We would also like to acknowledge CS224W: Machine Learning with Graphs as an excellent reference from which the authors benefitted significantly.           Ashish Tendulkar from Google Research India provided significant feedback on the content within this article, helping its readability.      He also helped with identifying the topics this article should cover, and with brainstorming the experiments here.           Adam Pearce from Google Research helped us immensely with article hosting and rendering.           Finally, we would like to thank Anirban Santara, Sujoy Paul and Ansh Khurana from Google Research India for their help with setting up and running experiments.     Ameya Daigavane drafted most of the text, designed experiments and created the interactive visualizations in this article.      Balaraman Ravindran and Gaurav Aggarwal extensively guided the overall direction of the article,      deliberated over the design and scope of experiments, provided much feedback on the interactive visualizations,      edited the text, and described improvements to make the article more accessible to readers.     Review 1 - Chaitanya K. JoshiReview 2 - Nick MoranReview 3 - Anonymous     If you see mistakes or want to suggest changes, please create an issue on GitHub.  Diagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don\u2019t fall under this license and can be recognized by a note in their caption: \u201cFigure from \u2026\u201d. For attribution in academic contexts, please cite this work as BibTeX citation", "url": "https://distill.pub/2021/understanding-gnns", "threshold": -0.9997393306045992}