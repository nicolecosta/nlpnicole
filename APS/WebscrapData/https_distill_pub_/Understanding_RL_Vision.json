{"title": "Understanding RL Vision", "content": "With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Jacob Hilton OpenAI Nick Cammarata OpenAI Shan Carter Observable Gabriel Goh OpenAI Chris Olah OpenAI Nov. 17, 2020 10.23915/distill.00029         In this article, we apply interpretability techniques to a reinforcement learning (RL) model trained to play the video game CoinRun . Using attribution  combined with dimensionality reduction as in , we build an interface for exploring the objects detected by the model, and how they influence its value function and policy. We leverage this interface in several ways.               Our results depend on levels in CoinRun being procedurally-generated, leading us to formulate a diversity hypothesis for interpretability. If it is correct, then we can expect RL models to become more interpretable as the environments they are trained on become more diverse. We provide evidence for our hypothesis by measuring the relationship between interpretability and generalization.               Finally, we provide a thorough investigation of several interpretability techniques in the context of RL vision, and pose a number of questions for further research.             CoinRun is a side-scrolling platformer in which the agent must dodge enemies and other traps and collect the coin at the end of the level.           CoinRun is procedurally-generated, meaning that each new level encountered by the agent is randomly generated from scratch. This incentivizes the model to learn how to spot the different kinds of objects in the game, since it cannot get away with simply memorizing a small number of specific trajectories .We use the original version of CoinRun , not the version from Procgen Benchmark , which is slightly different. To play CoinRun yourself, please follow the instructions here.       Here are some examples of the objects used, along with walls and floors, to generate CoinRun levels.           There are 9 actions available to the agent in CoinRun:           We trained a convolutional neural network on CoinRun for around 2 billion timesteps, using PPO , an actor-critic algorithm.We used the standard PPO hyperparameters for CoinRun , except that we used twice as many copies of the environment per worker and twice and many workers. The effect of these changes was to increase the effective batch size, which seemed to be necessary to reach the same performance with our smaller architecture. The architecture of our network is described in Appendix C. We used a non-recurrent network, to avoid any need to visualize multiple frames at once. Thus our model observes a single downsampled 64x64 image, and outputs a value function (an estimate of the total future time-discounted reward) and a policy (a probability distribution over the actions, from which the next action is sampled).           Since the only available reward is a fixed bonus for collecting the coin, the value function estimates the time-discountedWe use a discount rate of 0.999 per timestep. probability that the agent will successfully complete the level.           Having trained a strong RL agent, we were curious to see what it had learned. Following , we developed an interface for examining trajectories of the agent playing the game. This incorporates attribution from a hidden layer that recognizes objects, which serves to highlight objects that positively or negatively influence a particular network output. By applying dimensionality reduction, we obtain attribution vectors whose components correspond to different types of object, which we indicate using different colors.           Here is our interface for a typical trajectory, with the value function as the network output. It reveals the model using obstacles, coins, enemies and more to compute the value function.           Our fully-trained model fails to complete around 1 in every 200 levels. We explored a few of these failures using our interface, and found that we were usually able to understand why they occurred.           The failure often boils down to the fact that the model has no memory, and must therefore choose its action based only on the current observation. It is also common for some unlucky sampling of actions from the agent\u2019s policy to be partly responsible.           Here are some cherry-picked examples of failures, carefully analyzed step-by-step.           We searched for errors in the model using generalized advantage estimation (GAE) ,We use the same GAE hyperparameters as in training, namely \u03b3=0.999\\gamma=0.999\u03b3=0.999 and \u03bb=0.95\\lambda=0.95\u03bb=0.95. which measures how successful each action turned out relative to the agent\u2019s expectations. An unusually high or low GAE indicates that either something unexpected occurred, or the agent\u2019s expectations were miscalibrated. Filtering for such timesteps can therefore find problems with the value function or policy.           Using our interface, we found a couple of cases in which the model \u201challucinated\u201d a feature not present in the observation, causing the value function to spike.           Our analysis so far has been mostly qualitative. To quantitatively validate our analysis, we hand-edited the model to make the agent blind to certain features identified by our interface: buzzsaw obstacles in one case, and left-moving enemies in another. Our method for this can be thought of as a primitive form of circuit-editing , and we explain it in detail in Appendix A.           We evaluated each edit by measuring the percentage of levels that the new agent failed to complete, broken down by the object that the agent collided with to cause the failure. Our results show that our edits were successful and targeted, with no statistically measurable effects on the agent\u2019s other abilities.The data for this plot are as follows.Percentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:- Original model: 0.37% / 0.16% / 0.12% / 0.08%- Buzzsaw obstacle blindness: 12.76% / 0.16% / 0.08% / 0.05%- Enemy moving left blindness: 0.36% / 4.69% / 0.97% / 0.07%Each model was tested on 10,000 levels.       We did not manage to achieve complete blindness, however: the buzzsaw-edited model still performed significantly better than the original model did when we made the buzzsaws completely invisible.Our results on the version of the game with invisible buzzsaws are as follows.Percentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:Original model, invisible buzzsaws: 32.20% / 0.05% / 0.05% / 0.05%We tested the model on 10,000 levels.We experimented briefly with iterating the editing procedure, but were not able to achieve more than around 50% buzzsaw blindness by this metric without affecting the model\u2019s other abilities. This implies that the model has other ways of detecting buzzsaws than the feature identified by our interface.           Here are the original and edited models playing some cherry-picked levels.           All of the above analysis uses the same hidden layer of our network, the third of five convolutional layers, since it was much harder to find interpretable features at other layers. Interestingly, the level of abstraction at which this layer operates \u2013 finding the locations of various in-game objects \u2013 is exactly the level at which CoinRun levels are randomized using procedural generation. Furthermore, we found that training on many randomized levels was essential for us to be able to find any interpretable features at all.           This led us to suspect that the diversity introduced by CoinRun\u2019s randomization is linked to the formation of interpretable features. We call this the diversity hypothesis:           Our explanation for this hypothesis is as follows. For the forward implication (\u201conly if\u201d), we only expect features to be interpretable if they are general enough, and when the training distribution is not diverse enough, models have no incentive to develop features that generalize instead of overfitting. For the reverse implication (\u201cif\u201d), we do not expect it to hold in a strict sense: diversity on its own is not enough to guarantee the development of interpretable features, since they must also be relevant to the task. Rather, our intention with the reverse implication is to hypothesize that it holds very often in practice, as a result of generalization being bottlenecked by diversity.           In CoinRun, procedural generation is used to incentivize the model to learn skills that generalize to unseen levels . However, only the layout of each level is randomized, and correspondingly, we were only able to find interpretable features at the level of abstraction of objects. At a lower level, there are only a handful of visual patterns in the game, and the low-level features of our model seem to consist mostly of memorized color configurations used for picking these out. Similarly, the game\u2019s high-level dynamics follow a few simple rules, and accordingly the high-level features of our model seem to involve mixtures of combinations of objects that are hard to decipher. To explore the other convolutional layers, see the interface here.           To test our hypothesis, we made the training distribution less diverse, by training the agent on a fixed set of 100 levels. This dramatically reduced our ability to interpret the model\u2019s features. Here we display an interface for the new model, generated in the same way as the one above. The smoothly increasing value function suggests that the model has memorized the number of timesteps until the end of the level, and the features it uses for this focus on irrelevant background objects. Similar overfitting occurs for other video games with a limited number of levels .           We attempted to quantify this effect by varying the number of levels used to train the agent, and evaluating the 8 features identified by our interface on how interpretable they were.The interfaces used for this evaluation can be found here. Features were scored based on how consistently they focused on the same objects, and whether the value function attribution made sense \u2013 for example, background objects should not be relevant. This process was subjective and noisy, but that may be unavoidable. We also measured the generalization ability of each model, by testing the agent on unseen levels .The data for this plot are as follows.- Number of training levels: 100 / 300 / 1000 / 3,000 / 10,000 / 30,000 / 100,000- Percentage of levels completed (train, run 1): 99.96% / 99.82% / 99.67% / 99.65% / 99.47% / 99.55% / 99.57%- Percentage of levels completed (train, run 2): 99.97% / 99.86% / 99.70% / 99.46% / 99.39% / 99.50% / 99.37%- Percentage of levels completed (test, run 1): 61.81% / 66.95% / 74.93% / 89.87% / 97.53% / 98.66% / 99.25%- Percentage of levels completed (test, run 2): 64.13% / 67.64% / 73.46% / 90.36% / 97.44% / 98.89% / 99.35%- Percentage of features interpretable (researcher 1, run 1): 52.5% / 22.5% / 11.25% / 45% / 90% / 75% / 91.25%- Percentage of features interpretable (researcher 2, run 1): 8.75% / 8.75% / 10% / 26.25% / 56.25% / 90% / 70%- Percentage of features interpretable (researcher 1, run 2): 15% / 13.75% / 15% / 23.75% / 53.75% / 90% / 96.25%- Percentage of features interpretable (researcher 2, run 2): 3.75% / 6.25% / 21.25% / 45% / 72.5% / 83.75% / 77.5%Percentages of levels completed are estimated by sampling 10,000 levels with replacement.       Our results illustrate how diversity may lead to interpretable features via generalization, lending support to the diversity hypothesis. Nevertheless, we still consider the hypothesis to be highly unproven.     Feature visualization  answers questions about what certain parts of a network\u2009are looking for by generating examples. This can be done by applying gradient descent to the input image, starting from random noise, with the objective of activating a particular neuron or group of neurons. While this method works well for an image classifier trained on ImageNet , for our CoinRun model it yields only featureless clouds of color. Only for the first layer, which computes simple convolutions of the input, does the method produce comparable visualizations for the two models.           Gradient-based feature visualization has previously been shown to struggle with RL models trained on Atari games . To try to get it to work for CoinRun, we varied the method in a number of ways. Nothing we tried had any noticeable effect on the quality of the visualizations.           As shown below, we were able to use dataset examples to identify a number of channels that pick out human-interpretable features. It is therefore striking how resistant gradient-based methods were to our efforts. We believe that this is because solving CoinRun does not ultimately require much visual ability. Even with our modifications, it is possible to solve the game using simple visual shortcuts, such as picking out certain small configurations of pixels. These shortcuts work well on the narrow distribution of images on which the model is trained, but behave unpredictably in the full space of images in which gradient-based optimization takes place.           Our analysis here provides further insight into the diversity hypothesis. In support of the hypothesis, we have examples of features that are hard to interpret in the absence of diversity. But there is also evidence that the hypothesis may need to be refined. Firstly, it seems to be a lack of diversity at a low level of abstraction that harms our ability to interpret features at all levels of abstraction, which could be due to the fact that gradient-based feature visualization needs to back-propagate through earlier layers. Secondly, the failure of our efforts to increase low-level visual diversity suggests that diversity may need to be assessed in the context of the requirements of the task.           As an alternative to gradient-based feature visualization, we use dataset examples. This idea has a long history, and can be thought of as a heavily-regularized form of feature visualization . In more detail, we sample a few thousand observations infrequently from the agent playing the game, and pass them through the model. We then apply a dimensionality reduction method known as non-negative matrix factorization (NMF) to the activation channels .More precisely, we find a non-negative approximate low-rank factorization of the matrix obtained by flattening the spatial dimensions of the activations into the batch dimension. This matrix has one row per observation per spatial position and one column per channel: thus the dimensionality reduction does not use spatial information. For each of the resulting channels (which correspond to weighted combinations of the original channels), we choose the observations and spatial positions with the strongest activation (with a limited number of examples per position, for diversity), and display a patch from the observation at that position.           Unlike gradient-based feature visualization, this method finds some meaning to the different directions in activation space. However, it may still fail to provide a complete picture for each direction, since it only shows a limited number of dataset examples, and with limited context.           CoinRun observations differ from natural images in that they are much less spatially invariant. For example, the agent always appears in the center, and the agent\u2019s velocity is always encoded in the top left. As a result, some features detect unrelated things at different spatial positions, such as reading the agent\u2019s velocity in the top left while detecting an unrelated object elsewhere. To account for this, we developed a spatially-aware version of dataset example-based feature visualization, in which we fix each spatial position in turn, and choose the observation with the strongest activation at that position (with a limited number of reuses of the same observation, for diversity). This creates a spatial correspondence between visualizations and observations.           Here is such a visualization for a feature that responds strongly to coins. The white squares in the top left show that the feature also responds strongly to the horizontal velocity info when it is white, corresponding to the agent moving right at full speed.           Attribution  answers questions about the relationships between neurons. It is most commonly used to see how the input to a network affects a particular output \u2013 for example, in RL  \u2013 but it can also be applied to the activations of hidden layers . Although there are many approaches to attribution we could have used, we chose the method of integrated gradients . We explain in Appendix B how we applied this method a hidden layer, and how positive value function attribution can be thought of as \u201cgood news\u201d and negative value function attribution can as \u201cbad news\u201d.           We showed above that a dimensionality reduction method known as non-negative matrix factorization (NMF) could be applied to the channels of activations to produce meaningful directions in activation space . We found that it is even more effective to apply NMF not to activations, but to value function attributionsAs before, we obtain the NMF directions by sampling a few thousand observations infrequently from the agent playing the game, computing the attributions, flattening the spatial dimensions into the batch dimension, and applying NMF. (working around the fact that NMF can only be applied to non-negative matricesOur workaround is to separate out the positive and negative parts of the attributions and concatenate them along the batch dimension. We could also have concatenated them along the channel dimension.). Both methods tend to produce NMF directions that are close to one-hot, and so can be thought of as picking out the most relevant channels. However, when reducing to a small number of dimensions, using attributions usually picks out more salient features, because attribution takes into account not just what neurons respond to but also whether their response matters.           Following , after applying NMF to attributions, we visualize them by assigning a different color to each of the resulting channels. We overlay these visualizations over the observation  and contextualize each channel using feature visualization , making use of dataset example-based feature visualization. This gives a basic version of our interface, which allows us to see the effect of the main features at different spatial positions.           For the full version of our interface, we simply repeat this for an entire trajectory of the agent playing the game. We also incorporate video controls, a timeline view of compressed observations , and additional information, such as model outputs and sampled actions. Together these allow the trajectory to be easily explored and understood.           Attributions for our CoinRun model have some interesting properties that would be unusual for an ImageNet model.           These considerations suggest that some care may be required when interpreting attributions.           We are motivated to study interpretability for RL for two reasons.           We think that large neural networks are currently the most likely type of model to be used in highly capable and influential AI systems in the future. Contrary to the traditional perception of neural networks as black boxes, we think that there is a fighting chance that we will be able to clearly and thoroughly understand the behavior even of very large networks. We are therefore most excited by neural network interpretability research that scores highly according to the following criteria.           Our proposed questions reflect this perspective. One of the reasons we emphasize diversity relates to exhaustiveness. If \u201cnon-diverse features\u201d remain when diversity is present, then our current techniques are not exhaustive and could end up missing important features of more capable models. Developing tools to understand non-diverse features may shed light on whether this is likely to be a problem.           We think there may be significant mileage in simply applying existing interpretability techniques, with attention to detail, to more models. Indeed, this was the mindset with which we initially approached this project. If the diversity hypothesis is correct, then this may become easier as we train our models to perform more complex tasks. Like early biologists encountering a new species, there may be a lot we can glean from taking a magnifying glass to the creatures in front of us.           Here we explain our method for editing the model to make the agent blind to certain features.           The features in our interface correspond to directions in activation space obtained by applying attribution-based NMF to layer 2b of our model. To blind the agent to a feature, we edit the weights to make them project out the corresponding NMF direction.           More precisely, let v\\mathbf vv be the NMF direction corresponding to the feature we wish to blind the model to. This is a vector of length ccc, the number of channels in activation space. Using this we construct the orthogonal projection matrix P:=I\u22121\u2225v\u22252vvTP:=I-\\frac 1{\\|\\mathbf v\\|^2}\\mathbf v\\mathbf v^{\\mathsf T}P:=I\u2212\u2225v\u222521\u200bvvT, which projects out the direction of v\\mathbf vv from activation vectors. We then take the convolutional kernel of the following layer, which has shape height\u00d7width\u00d7c\u00d7d\\text{height}\\times\\text{width}\\times c\\times dheight\u00d7width\u00d7c\u00d7d, where ddd is the number of output channels. Broadcasting across the height and width dimensions, we left-multiply each c\u00d7dc\\times dc\u00d7d matrix in the kernel by PPP. The effect of the new kernel is to project out the direction of v\\mathbf vv from activations before applying the original kernel.           As it turned out, the NMF directions were close to one-hot, so this procedure is approximately equivalent to zeroing out the slice of the kernel corresponding to a particular in-channel.           Here we explain the application of integrated gradients  to a hidden layer for the purpose of attribution. This method can be applied to any of the network\u2019s outputs, but we focus here on the value function. Recall that this is the model\u2019s estimate of the time-discounted probability that the agent will successfully complete the level.             Let V:R64\u00d764\u00d73\u2192RV:\\mathbb R^{64\\times 64\\times 3}\\to\\mathbb RV:R64\u00d764\u00d73\u2192R be the value function computed by our network, which accepts a 64x64 RGB observation. Given any layer in the network, we may write VVV as V(x)=F(A(x))V\\left(\\mathbf x\\right)=F\\left(\\mathbf A\\left(\\mathbf x\\right)\\right)V(x)=F(A(x)), where A\\mathbf AA computes the layer\u2019s activations. Given an observation x\\mathbf xx, a simple method of attribution is to compute \u2207aF(a)\u2299a\\nabla_{\\mathbf a}F\\left(\\mathbf a\\right)\\odot\\mathbf a\u2207a\u200bF(a)\u2299a, where a=A(x)\\mathbf a=\\mathbf A\\left(\\mathbf x\\right)a=A(x) and \u2299\\odot\u2299 denotes the pointwise product. This tells us the sensitivity of the value function to each activation, multiplied by the strength of that activation. However, it uses the sensitivity of the value function at the activation itself, which does not account for the fact that this sensitivity may change as the activation is increased from zero.               To account for this, the integrated gradients method instead chooses a path P\\mathcal PP in activation space from some starting point a0\\mathbf a_0a0\u200b to the ending point a1:=A(x)\\mathbf a_1:=\\mathbf A\\left(\\mathbf x\\right)a1\u200b:=A(x). We then compute the integrated gradient of FFF along P\\mathcal PP, which is defined as the path integral \u222bP\u2207aF(a)\u2299da.\\int_{\\mathcal P}\\nabla_{\\mathbf a}F\\left(\\mathbf a\\right)\\odot\\mathrm d\\mathbf a.\u222bP\u200b\u2207a\u200bF(a)\u2299da. Note the use of the pointwise product rather than the usual dot product here, which makes the integral vector-valued. By the fundamental theorem of calculus for line integrals, when the components of the vector produced by this integral are summed, the result depends only on the endpoints a0\\mathbf a_0a0\u200b and a1\\mathbf a_1a1\u200b, equaling F(a1)\u2212F(a0)F\\left(\\mathbf a_1\\right)-F\\left(\\mathbf a_0\\right)F(a1\u200b)\u2212F(a0\u200b). Thus the components of this vector provide a true decomposition of this difference, \u201cattributing\u201d it across the activations.               For our purposes, we take P\\mathcal PP to be the straight line from 0\\mathbf 00 to A(x)\\mathbf A\\left(\\mathbf x\\right)A(x).In theory, we could choose any point in activation space as the starting point of our path, but in practice, 0\\mathbf 00 tends to be a good baseline against which to compare other activations, with F(0)F\\left(\\mathbf 0\\right)F(0) being on the same order as the average value function. Sundararajan, Taly and Yan  discuss the choice of this baseline in more depth. In other words, given an observation x\\mathbf xx, we define the value function attribution asIn practice, we numerically approximate the integral by evaluating the integrand at \u03b1=0.1,0.2,\u2026,1\\alpha=0.1,0.2,\\ldots,1\u03b1=0.1,0.2,\u2026,1.         This has the same dimensions as A(x)\\mathbf A\\left(\\mathbf x\\right)A(x), and its components sum to V(x)\u2212F(0)V\\left(\\mathbf x\\right)-F\\left(\\mathbf 0\\right)V(x)\u2212F(0). So for a convolutional layer, this method allows us to attribute the value function (in excess of the baseline F(0)F\\left(\\mathbf 0\\right)F(0)) across the horizontal, vertical and channel dimensions of activation space. Positive value function attribution can be thought of as \u201cgood news\u201d, components that cause the agent to think it is more likely to collect the coin at the end of the level. Similarly, negative value function attribution can be thought of as \u201cbad news\u201d.             Our architecture consists of the following layers in the order given, together with ReLU activations for all except the final layer.           We designed this architecture by starting with the architecture from IMPALA , and making the following modifications in an attempt to aid interpretability without noticeably sacrificing performance.           The choice that seemed to make the most difference was using 5 rather than 12 convolutional layers, resulting in the object-identifying features (which were the most interpretable, as discussed above) being concentrated in a single layer (layer 2b), rather than being spread over multiple layers and mixed in with less interpretable features.           We would like to thank our reviewers Jonathan Uesato, Joel Lehman and one anonymous reviewer for their detailed and thoughtful feedback. We would also like to thank Karl Cobbe, Daniel Filan, Sam Greydanus, Christopher Hesse, Jacob Jackson, Michael Littman, Ben Millwood, Konstantinos Mitsopoulos, Mira Murati, Jorge Orbay, Alex Ray, Ludwig Schubert, John Schulman, Ilya Sutskever, Nevan Wichers, Liang Zhang and Daniel Ziegler for research discussions, feedback, follow-up work, help and support that have greatly benefited this project.     Jacob Hilton was the primary contributor.     Nick Cammarata developed the model editing methodology and suggested applying it to CoinRun models.     Shan Carter (while working at OpenAI) advised on interface design throughout the project, and worked on many of the diagrams in the article.     Gabriel Goh provided evaluations of feature interpretability for the section Interpretability and generalization.     Chris Olah guided the direction of the project, performing initial exploratory research on the models, coming up with many of the research ideas, and helping to construct the article\u2019s narrative.     Review 1 - AnonymousReview 2 - Jonathan UesatoReview 3 - Joel Lehman     If you see mistakes or want to suggest changes, please create an issue on GitHub.  Diagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don\u2019t fall under this license and can be recognized by a note in their caption: \u201cFigure from \u2026\u201d. For attribution in academic contexts, please cite this work as BibTeX citation", "url": "https://distill.pub/2020/understanding-rl-vision", "threshold": -0.9999981673319689}