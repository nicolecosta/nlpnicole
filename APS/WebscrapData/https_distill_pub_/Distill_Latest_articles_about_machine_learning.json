{"title": "Distill \u2014 Latest articles about machine learning", "content": "Ameya\u00a0Daigavane, Balaraman\u00a0Ravindran, and Gaurav\u00a0Aggarwal Understanding the building blocks and design choices of graph neural networks. Benjamin\u00a0Sanchez-Lengeling, Emily\u00a0Reif, Adam\u00a0Pearce, and Alexander\u00a0B. Wiltschko What components are needed for building learning algorithms that leverage the structure and properties of graphs? Editorial\u00a0Team After five years, Distill will be taking a break. Gabriel\u00a0Goh, Nick\u00a0Cammarata \u2020, Chelsea\u00a0Voss \u2020, Shan\u00a0Carter, Michael\u00a0Petrov, Ludwig\u00a0Schubert, Alec\u00a0Radford, and Chris\u00a0Olah We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain. Jacob\u00a0Hilton, Nick\u00a0Cammarata, Shan\u00a0Carter, Gabriel\u00a0Goh, and Chris\u00a0Olah With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Fred\u00a0Hohman, Matthew\u00a0Conlen, Jeffrey\u00a0Heer, and Duen\u00a0Horng (Polo) Chau Examining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization. Alexander\u00a0Mordvintsev, Ettore\u00a0Randazzo, Eyvind\u00a0Niklasson, Michael\u00a0Levin, and Sam\u00a0Greydanus A collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems. Apoorv\u00a0Agnihotri and Nipun\u00a0Batra How to tune hyperparameters for your machine learning model using Bayesian optimization. Mingwei\u00a0Li, Zhenge\u00a0Zhao, and Carlos\u00a0Scheidegger By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks. Nick\u00a0Cammarata, Shan\u00a0Carter, Gabriel\u00a0Goh, Chris\u00a0Olah, Michael\u00a0Petrov, Ludwig\u00a0Schubert, Chelsea\u00a0Voss, Ben\u00a0Egan, and Swee\u00a0Kiat Lim What can we learn if we invest heavily in reverse engineering a single neural network? Pascal\u00a0Sturmfels, Scott\u00a0Lundberg, and Su-In\u00a0Lee Exploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior. Andr\u00e9\u00a0Araujo, Wade\u00a0Norris, and Jack\u00a0Sim Detailed derivations and open-source code to analyze the receptive fields of convnets. Sam\u00a0Greydanus and Chris\u00a0Olah A closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency Logan\u00a0Engstrom, Justin\u00a0Gilmer, Gabriel\u00a0Goh, Dan\u00a0Hendrycks, Andrew\u00a0Ilyas, Aleksander\u00a0Madry, Reiichiro\u00a0Nakano, Preetum\u00a0Nakkiran, Shibani\u00a0Santurkar, Brandon\u00a0Tran, Dimitris\u00a0Tsipras, and Eric\u00a0Wallace Six comments from the community and responses from the original authors Augustus\u00a0Odena What we\u2019d like to find out about GANs that we don\u2019t know yet. Jochen\u00a0G\u00f6rtler, Rebecca\u00a0Kehlbeck, and Oliver\u00a0Deussen How to turn a collection of small building blocks into a versatile tool for solving regression problems. Andreas\u00a0Madsen Inspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding. Shan\u00a0Carter, Zan\u00a0Armstrong, Ludwig\u00a0Schubert, Ian\u00a0Johnson, and Chris\u00a0Olah By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents. Geoffrey\u00a0Irving and Amanda\u00a0Askell If we want to train AI to do what humans want, we need to study humans. Distill\u00a0Editors An Update from the Editorial Team Alexander\u00a0Mordvintsev, Nicola\u00a0Pezzotti, Ludwig\u00a0Schubert, and Chris\u00a0Olah A powerful, under-explored tool for neural network visualizations and art. Vincent\u00a0Dumoulin, Ethan\u00a0Perez, Nathan\u00a0Schucher, Florian\u00a0Strub, Harm\u00a0de Vries, Aaron\u00a0Courville, and Yoshua\u00a0Bengio A simple and surprisingly effective family of conditioning mechanisms. Chris\u00a0Olah, Arvind\u00a0Satyanarayan, Ian\u00a0Johnson, Shan\u00a0Carter, Ludwig\u00a0Schubert, Katherine\u00a0Ye, and Alexander\u00a0Mordvintsev Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them\u2009\u2014\u2009and the rich structure of this combinatorial space. Shan\u00a0Carter and Michael\u00a0Nielsen By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. Awni\u00a0Hannun A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems. Chris\u00a0Olah, Alexander\u00a0Mordvintsev, and Ludwig\u00a0Schubert How neural networks build up their understanding of images Gabriel\u00a0Goh We often think of optimization with momentum as a ball rolling down a hill. This isn\u2019t wrong, but there is much more to the story. Chris\u00a0Olah and Shan\u00a0Carter Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt... Shan\u00a0Carter, David\u00a0Ha, Ian\u00a0Johnson, and Chris\u00a0Olah Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious. Augustus\u00a0Odena, Vincent\u00a0Dumoulin, and Chris\u00a0Olah When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. Martin\u00a0Wattenberg, Fernanda\u00a0Vi\u00e9gas, and Ian\u00a0Johnson Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. Chris\u00a0Olah and Shan\u00a0Carter A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.", "url": "https://distill.pub/", "threshold": 0.8664969844260098}