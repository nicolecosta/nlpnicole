{"title": "From Discrimination in Machine Learning to Discrimination in Law, Part 1: Disparate Treatment | SAIL Blog", "content": "Fereshte Khani,  Percy LiangDecember 5, 2022  Around 60 years ago, the U.S. Department of Justice Civil Rights Division was established for prohibiting discrimination based on protected attributes. Over these 60 years, they established a set of policies and guidelines to identify and penalize those who discriminate1. The widespread use of machine learning (ML) models in routine life has prompted researchers to begin studying the extent to which these models are discriminatory. However, some researcher are unaware that the legal system already has well established procedures for describing and proving discrimination in law. In this series of blog posts, we\u2019ll try to bridge this gap. We give a brief overview of the procedures to prove discrimination in law, focusing on employment, and discuss its analogy to discrimination in machine learning. Our goal is to help ML researchers assess discrimination in machine learning more effectively and facilitate the process of auditing algorithms and mitigating discrimination. This series of blog posts is based on CM-604 Theories of Discrimination (Title VII) and chapters 6 and 7 of TITLE VI Legal Manual.  In this first blog post, we discuss the first type of illegal discrimination known as disparate treatment, and in the second blog post, we discuss the second type of illegal discrimination known as disparate impact.  Legal procedure (a) Comparative Evidence: (b) Statistical Evidence: (c) Direct Evidence of Motive: (a) Charging Party's Allegations Are Factually Incorrect. (c) Respondent\u2019s Actions were Based on an Act of Favoritism (d) Charging Party's Statistical Proof Is Not Meaningful: (e) Statistical Proof To Rebut an Inference of Discriminatory Motive: Age Discrimination Sex Discrimination Race Discrimination Anti-discrimination laws are designed to prevent discrimination based on one of the following protected attributes: Disparate treatment occurs when an employer treats some individuals less favorably than other similarly situated individuals because of their protected attributes. \u201cSimilarly situated individuals\u201d is specific to each case and cannot be defined precisely, intuitively it means individuals who are situated in a way that it is reasonable to expect that they would receive the same treatment. During the legal proceddings, the charging party  (the party that believes it has suffered from disparate treatment, e.g., employee) accuses the respondent (the party that is accused of treating the charging party less favorably because of their protected attributes, e.g., employer) of disparate treatment. Although historically, the charging party  has to establish that the respondent  deliberately discriminated against them, it has been recognized that it is difficult and often impossible to obtain direct evidence of discriminatory motive; therefore, the discriminatory motive can be inferred from the fact of differences in treatment3.  The legal process for proving disparate treatment comprises three steps: We now expand each step and briefly mention the related work in ML.  Evidence for disparate treatment discrimination can be presented in three main ways: The disparate treatment theory is based on differences in the treatment of similarly situated individuals. \u201cSimilarly situated individuals\u201d cannot be precisely defined, and it is context-dependent. Generally, similarly situated individuals are the ones that are expected to receive the same treatment for a particular employment decision. For example, when there are some predefined qualifications for promotion, similarly situated individuals are those who meet these qualifications. Or, in the case of discharge (firing), the employer provides a reason for the termination, and people who have committed the same misconduct are similarly situated. Comparative evidence is a piece of evidence that shows that two similarly situated individuals are treated differently due to their protected attributes. \u201cFor example, an employer\u2019s collective bargaining agreement may contain a rule that any employee charged with theft of company property is automatically discharged.  If a Black employee who is charged with theft of company property is discharged, the discharge is consistent with the rule and the agreement.  However, the analysis does not end there.  To determine whether there was disparate treatment, we should ascertain whether White employees who have been charged with the same offense are also discharged.  If they are merely suspended, disparate treatment has occurred.  The key to the analysis is that they are similarly situated employees, yet the employer failed to apply the same criteria for discharge to all of them. They are similarly situated because they are respondent\u2019s employees and were charged with the same misconduct.  The difference in discipline could be attributable to race, unless respondent produces evidence to the contrary.\u201d5  Statistical evidence can also be used to demonstrate discriminatory motives. Charging party uses statistical evidence to prove that the respondent uses protected attributes in its decision9. For example, Alice believes that she is not hired for a secretarial position because she is Black. She can buttress her allegation with statistical evidence indicating that the respondent employs no Black secretaries despite the many applicants in its Metropolitan Statistical Areas. The statistical data shows that the respondent refused to hire Blacks as secretaries; thus, Alice\u2019s rejection was pursuant to this practice. Note that in statistical evidence unlinke the comparative evidence Alice does not need to find a similarly situated individual to herself. It is important to note that statistics alone will not normally prove an individual case of disparate treatment 10. Direct evidence of motive can be demonstrated by: In the second step, the respondent can bring some evidence to show that the evidence presented by the charging party is not valid. There are six types of rebuttals the respondent can provide: This evidence can usually be in the form of (1) Individuals compared are not similarly situated, or the hired individual is more qualified, and (2) Not all similarly situated individuals were compared Title VII only prohibits discrimination based on protected attributes.  If in isolated instances a respondent discriminates against the charging party in favor of a relative or friend, no violation of Title VII has occurred. However, if there are indications that the respondent hired their relative to avoid hiring people from some protected groups, there should be an investigation to determine if the respondent\u2019s actions were a pretext to hide discrimination. In this case, the respondent\u2019s workforce composition and their past hiring practices would be important pieces of evidence. The respondent can show that statistical proof is not meaningful e.g., it considers the pool of applicants in the state instead of the city. The respondent can provide statistical data showing that they have not discriminated against protected groups. For example, showing that they have employed a high proportion of a protected group.  Even though these kinds of evidence serve as support, they are not conclusive proof that discrimination did not occur.  There may not be a pattern and practice of discrimination, but an individual case of disparate treatment may have occurred. \u201cAffirmative action under the Guidelines is not a type of discrimination but a justification for a policy or practice based on race, sex, or national origin. An affirmative action plan must be designed to break down old patterns of segregation and hierarchy and to overcome the effects of past or present practices, policies, or other barriers to equal employment opportunity. _It must be a concerted, reasoned program rather than one or more isolated events. It should be in effect only as long as necessary to achieve its objectives and should avoid unnecessary restrictions on opportunities for the workforce as a whole. For more details, see the affirmative action manual.\u201d Once the respondent states a legitimate justification for the decision, the charging party can rebut the argument and claim that it\u2019s a pretext for discrimination. For instance, the charging party might present evidence or witnesses that contradict those submitted by the respondent. Or the charging party can show that the respondent gives different justifications at different times for its decision. \u201cJPL systemically laid off employees over the age of 40 in favor of retaining younger employees. The complaint also alleges that older employees were passed over for rehire in favor of less qualified, younger employees. Such conduct violates the Age Discrimination in Employment Act (ADEA),\u201d according to the EEOC.\u201d Ramifications: Jet Propulsion Laboratory had to pay $10 million to settle the EEOC age discrimination lawsuit. \u201cPruittHealth-Raleigh LLC, (PruittHealth) operates a skilled nursing and rehabilitation facility in Raleigh, N.C. Allegedly, PruittHealth subjected Dominque Codrington, a certified nursing assistant, to disparate treatment by refusing to accommodate her pregnancy-related lifting restriction, while accommodating the restrictions of other non-pregnant employees who were injured on the job and who were similar in their ability or inability to work. The EEOC alleged that PruittHealth refused to accommodate Codrington and required her to involuntarily resign in lieu of termination.\u201d Ramifications: \u201cPruittHealth-Raleigh, LLC paid $25,000 and provide other relief to settle a pregnancy discrimination lawsuit brought by the U.S. Equal Employment Opportunity Commission (EEOC). The EEOC charged that PruittHealth violated Title VII when it denied a reasonable accommodation to a pregnant employee.\u201d \u201cKoch subjected individual plaintiff/intervenors and classes of Hispanic employees and female employees to a hostile work environment and disparate treatment based on their race/national origin (Hispanic), sex (female), and further retaliated against those who engaged in protected activity. Allegedly, supervisors touched and/or made sexually suggestive comments to female Hispanic employees, hit Hispanic employees and charged many of them money for normal everyday work activities. Further, a class of Hispanic employees was subject to retaliation in the form of discharge and other adverse actions after complaining.\u201d Ramifications: \u201cKoch Foods, one of the largest poultry suppliers in the world, paid $3,750,000 and furnish other relief to settle a class employment discrimination lawsuit filed by the U.S. Equal Employment Opportunity Commission (EEOC). The EEOC charged the company with sexual harassment, national origin and race discrimination as well as retaliation against a class of Hispanic workers.\u201d We would like to thank Alex Tamkin, Jacob Schreiber, Neel Guha, Peter Henderson, Megha Srivastava, and Michael Zhang for their useful feedback on this blog post. For example, the federal agency that administers and enforces civil rights laws against workplace discrimination charges around 90,000 cases each year, of which around 15% result in monetary benefit (~300 million per year) for the charging party.\u00a0\u21a9 https://www.nytimes.com/2020/06/15/us/gay-transgender-workers-supreme-court.html\u00a0\u21a9 Teamsters, supra; Commission Decision No. 71-1683, CCH EEOC Decisions (1973) \u00b6 6262.\u00a0\u21a9 Khani, Fereshte, and Percy Liang. \u201cFeature noise induces loss discrepancy across groups.\u201d International Conference on Machine Learning. PMLR, 2020.\u00a0\u21a9 Example from 604.3 (a) of CM-604 Theories of Discrimination\u00a0\u21a9 Rudin, Cynthia. \u201cStop explaining black box machine learning models for high stakes decisions and use interpretable models instead.\u201d Nature Machine Intelligence 1.5 (2019): 206-215.\u00a0\u21a9\u00a0\u21a92 Garg, Sahaj, et al. \u201cCounterfactual fairness in text classification through robustness.\u201d Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019.\u00a0\u21a9 Sweeney, Latanya. \u201cDiscrimination in online ad delivery.\u201d Communications of the ACM 56.5 (2013): 44-54.\u00a0\u21a9 International Brotherhood of Teamsters v. U.S., 431 U.S. 324, 14 EPD \u00b6 7579 (1977)\u00a0\u21a9 Bolton v. Murray Envelope Corp., 493 F.2d 191, 7 EPD \u00b6 9289 (5th Cir. 1974)\u00a0\u21a9 Datta, Amit, Michael Carl Tschantz, and Anupam Datta. \u201cAutomated experiments on ad privacy settings: A tale of opacity, choice, and discrimination.\u201d arXiv preprint arXiv:1408.6491 (2014).\u00a0\u21a9\u00a0\u21a92\u00a0\u21a93 Wilson, Benjamin, Judy Hoffman, and Jamie Morgenstern. \u201cPredictive inequity in object detection.\u201d arXiv preprint arXiv:1902.11097 (2019).\u00a0\u21a9 Note that, there are many objections to this method as there might be some relevant features that are not reported and not considering them causes false evidence of disparate treatment (see Jung, Jongbin, et al. \u201cOmitted and included variable bias in tests for disparate impact.\u201d arXiv preprint arXiv:1809.05651 (2018) for more details).\u00a0\u21a9 Interestingly, while ML researchers have been doing many studies on complicated ways to infer and mitigate discrimination since 2011, up until 2020, HEC advertisers could directly target users with their gender!\u00a0\u21a9 Datta, Amit, Michael Carl Tschantz, and Anupam Datta. \u201cAutomated experiments on ad privacy settings: A tale of opacity, choice, and discrimination.\u201d arXiv preprint arXiv:1408.6491 (2014).\u00a0\u21a9 He, Kaiming, et al. \u201cDeep residual learning for image recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\u00a0\u21a9 Devlin, Jacob, et al. \u201cBert: Pre-training of deep bidirectional transformers for language understanding.\u201d arXiv preprint arXiv:1810.04805 (2018).\u00a0\u21a9 Brown, Tom B., et al. \u201cLanguage models are few-shot learners.\u201d arXiv preprint arXiv:2005.14165 (2020).\u00a0\u21a9 Bolukbasi, Tolga, et al. \u201cMan is to computer programmer as woman is to homemaker? debiasing word embeddings.\u201d Advances in neural information processing systems 29 (2016): 4349-4357.\u00a0\u21a9 Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. \u201cSemantics derived automatically from language corpora contain human-like biases.\u201d Science 356.6334 (2017): 183-186.\u00a0\u21a9 Steed, Ryan, and Aylin Caliskan. \u201cImage representations learned with unsupervised pre-training contain human-like biases.\u201d Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021.\u00a0\u21a9\u00a0\u21a92 Abid, Abubakar, Maheen Farooqi, and James Zou. \u201cLarge language models associate Muslims with violence.\u201d Nature Machine Intelligence 3.6 (2021): 461-463.\u00a0\u21a9 Oliva, Thiago Dias, Dennys Marcelo Antonialli, and Alessandra Gomes. \u201cFighting hate speech, silencing drag queens? artificial intelligence in content moderation and risks to LGBTQ voices online.\u201d Sexuality & Culture 25.2 (2021): 700-732.\u00a0\u21a9 This is our analogy of direct evidence of motive to machine learning, and this kind of reasoning has not yet been successfully used in courts. In addition, as we see in the next section, the respondent can rebut such evidence with a protected attribute-neutral explanation (see Hernandez v. New York in which jurors were struck because of their Spanish speaking ability and the explanation was that the prosecutor wanted all jurors to hear the same story from Spanish-speaking witnesses through a translator, not through their own spanish language knowledge)\u00a0\u21a9 Ramchand, Rajeev, Rosalie Liccardo Pacula, and Martin Y. Iguchi. \u201cRacial differences in marijuana-users\u2019 risk of arrest in the United States.\u201d Drug and alcohol dependence 84.3 (2006): 264-272.\u00a0\u21a9 Denton, Emily, et al. \u201cDetecting bias with generative counterfactual face attribute augmentation.\u201d arXiv e-prints (2019): arXiv-1906.\u00a0\u21a9 Bolukbasi, Tolga, et al. \u201cMan is to computer programmer as woman is to homemaker? debiasing word embeddings.\u201d Advances in neural information processing systems 29 (2016): 4349-4357.\u00a0\u21a9 Kusner, Matt J., et al. \u201cCounterfactual fairness.\u201d arXiv preprint arXiv:1703.06856 (2017).\u00a0\u21a9 Nabi, Razieh, and Ilya Shpitser. \u201cFair inference on outcomes.\u201d Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.\u00a0\u21a9 Holland, Paul W. \u201cStatistics and causal inference.\u201d Journal of the American statistical Association 81.396 (1986): 945-960.\u00a0\u21a9 Freedman, David A. \u201cGraphical models for causation, and the identification problem.\u201d Evaluation Review 28.4 (2004): 267-293.\u00a0\u21a9 As a simple example, consider the effect of race on the salary of people. We cannot compare the salary of two people of different races at the same job level because race might have led one of them to get mis-leveled.\u00a0\u21a9 Rosenbaum, Paul R. \u201cThe consequences of adjustment for a concomitant variable that has been affected by the treatment.\u201d Journal of the Royal Statistical Society: Series A (General) 147.5 (1984): 656-666.\u00a0\u21a9 Examples from https://www.digitalhrtech.com/disparate-treatment/\u00a0\u21a9  ai  discrimination  disparate treatment  fairness  fairness in machine learning  law  machine learning  ml Previous post Next post \u00a9 2021 Stanford AI Lab", "url": "https://ai.stanford.edu/blog/discrimination_in_ML_and_law/", "threshold": -0.9999999989017243}