{"title": "Stanford AI Lab Papers and Talks at ICLR 2023 | SAIL Blog", "content": "    Compiled by Drew A. HudsonMay 1, 2023  The International Conference on Learning Representations (ICLR) 2023 is being hosted in Kigali, Rwanda from May 1st - May 5th. We\u2019re excited to share all the work from SAIL that\u2019s being presented, and you\u2019ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that\u2019s happening at Stanford! Authors: Stephen Tian, Chelsea Finn, Jiajun WuContact: tians@stanford.eduLinks: Paper | WebsiteKeywords: benchmarking, video prediction, visual mpc, manipulation Authors: Simran Arora*, Avanika Narayan*, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher R\u00e9Contact: avanikan@stanford.eduAward nominations: Spotlight, top 25% of acceptancesLinks: Paper | WebsiteKeywords: prompting, foundation models Authors: Kefan Dong, Tengyu MaContact: kefandong@stanford.eduLinks: PaperKeywords: instance optimality, reinforcement learning theory Authors: Beyond Confidence: Reliable Models Should Also Quantify AtypicalityContact: merty@stanford.eduAward nominations: Oral PresentationLinks: PaperKeywords: trustworthy machine learning, reliable machine learning, uncertainty, calibration Authors: Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D ManningContact: smurty@cs.stanford.eduLinks: PaperKeywords: compositionality, emergent syntax, generalization Authors: Yuhui Zhang, Jeff Z. HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, Serena YeungContact: yuhuiz@stanford.eduLinks: Paper | Video | WebsiteKeywords: model diagnosis, multi-modal contrastive learning, vision and language Authors: Div Garg*, Joey Hejna*, Matthieu Geist, Stefano ErmonContact: jhejna@cs.stanford.eduAward nominations: notable top 5%Links: Paper | WebsiteKeywords: reinforcement learning, offline reinforcement learning, statistical learning, extreme value analysis, maximum entropy rl, gumbel Authors: Kefan Dong, Tengyu MaContact: kefandong@stanford.eduLinks: PaperKeywords: extrapolation of nonlinear models, theory, structured domain shift, gaussian kernel Authors: Patricia Suriana, Joseph M. Paggi, Ron O. DrorContact: psuriana@stanford.eduLinks: PaperKeywords: deep learning, structural biology, protein ligand docking, protein flexibility Authors: Daniel Y. Fu*, Tri Dao*, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9Contact: danfu@cs.stanford.eduAward nominations: Notable top-25% (spotlight)Links: Paper | Blog Post | WebsiteKeywords: language modeling, state space models, convolution, fft, io-aware Authors: Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, Tatsunori B HashimotoContact: rtaori@stanford.eduLinks: PaperKeywords: clip, transfer learning, contrastive learning, multi-modal Authors: Tailin Wu, Takashi Maruyama, Qingqing Zhao, Gordon Wetzstein, Jure LeskovecContact: tailin@cs.stanford.eduAward nominations: notable-top-25% (spotlight)Links: Paper | WebsiteKeywords: learned simulation, adaptive, multi-scale, error vs. computation, controllable Authors: Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, Diyi YangContact: jchen896@gatech.eduLinks: PaperKeywords: parameter-efficient fine-tuning, design spaces Authors: Holden Lee, Chirag Pabbaraju, Anish Sevekari, Andrej RisteskiContact: cpabbara@stanford.eduLinks: PaperKeywords: nce, noise contrastive estimation, generative models, statistical efficiency, theory Authors: Mert Yuksekgonul, Maggie Wang, James ZouContact: merty@stanford.eduAward nominations: Spotlight (Top 25%)Links: PaperKeywords: concepts, interpretability, concept bottleneck models, model editing Authors: Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa SadighContact: minae@cs.stanford.eduLinks: Paper | VideoKeywords: alignment, reinforcement learning, foundation models, reward design Authors: Jimmy T.H. Smith*, Andrew Warrington*, Scott W. LindermanContact: jsmith14@stanford.eduAward nominations: Notable-top-5% (In-person Oral Presentation)Links: Paper | WebsiteKeywords: deep learning, sequence model, state space model, s4 Authors: Yoonho Lee*, Annie S. Chen*, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea FinnContact: asc8@stanford.eduLinks: PaperKeywords: transfer learning, fine-tuning, parameter freezing, distortion of pre-trained models Authors: Daniel Kunin, Atsushi Yamamura, Chao Ma, Surya GanguliContact: kunin@stanford.edu, atsushi3@stanford.eduAward nominations: ICLR 2023 notable top 25%Links: PaperKeywords: margin, maximum-margin, implicit regularization, neural networks, neural collapse, gradient flow, implicit bias, robustness, homogeneous, symmetry, classification Authors: Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James ZouContact: merty@stanford.eduAward nominations: Oral (Top 5%)Links: Paper | Blog PostKeywords: vision-language models, clip, contrastive learning, retrieval, vision-language pretraining, multimodal representation learning We look forward to seeing you at ICLR!  ICLR  conference  publication  video Previous post \u00a9 2021 Stanford AI Lab", "url": "https://ai.stanford.edu/blog/iclr-2023/", "threshold": -0.9999809875368361}