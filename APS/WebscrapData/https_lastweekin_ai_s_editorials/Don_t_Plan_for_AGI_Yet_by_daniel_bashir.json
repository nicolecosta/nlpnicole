{"title": "Don't \"Plan for AGI\" Yet - by daniel bashir", "content": "TL;DR: Transformer models, and therefore ChatGPT, Bing/Sydney, and their ilk still have fundamental limitations; I think these are important barriers to something we might call \u201cAGI\u201d\u2014but I also think we might consider jettisoning the term and just saying what we mean. We should be impressed by what\u2019s possible now, but be aware of limitations, think clearly about what these limitations mean, and better conceptualize our own flourishing.\u00a0 On February 24, OpenAI\u2019s CEO Sam Altman released a post titled \u201cPlanning for AGI and Beyond.\u201d Given OpenAI\u2019s charter to bring safe general intelligence to the world, it\u2019s not much of a surprise that we\u2019d see Altman write something like this. The article first lays out three principles they care about: AGI should \u201cempower humanity to maximally flourish in the universe\u201d Benefits of, access to, and governance of AGI should be widely shared Successfully navigating massive risks will require iterative learning and adaptation by deploying less powerful versions of the technology [ \u201cthe technology\u201d here is presumably \u201cAGI\u201d ] Altman then discusses how we should prepare for AGI in the short term: this section re-iterates principle 3 above and expands on the notion that we should learn to navigate AI deployment challenges and that a gradual approach to releasing more powerful technologies will give people, policymakers, and institutions time to understand and adapt to new advances. First-hand experience with these technologies will allow the greater public to understand their downsides, \u201cadapt our economy,\u201d and put regulations in place.\u00a0 In planning for the long term, Altman thinks that \u201cthe future of humanity should be determined by humanity\u201d and that there should be great scrutiny of any effort attempting to build AGI; accordingly, major decisions should require public consultation.\u00a0 In this article, I want to do a few things\u2014broadly, I want to answer the question \u201cShould we be \u2018planning for AGI\u2019 right now?\u201d Answering that question requires two things first: Articulating what we mean by \u201cAGI.\u201d And, in turn, Articulating what we mean by \u201cintelligence.\u201d\u00a0 Along the way, we should realize that the precise nature of what we\u2019re talking about matters a lot more than the terms we use. Overloading terms without much explanation often allows us to make vague statements that sound important, but say little. I will center much of this discussion around terms like AGI because we use them so frequently, but we should try to remember that actions like \u201cachieving AGI\u201d can mean very different things to different people.  TL;DR: This section attempts to clarify the terms \u201cintelligence\u201d and \u201cAGI,\u201d drawing from a motley of perspectives on what those might mean. I consider a notion of intelligence, and of AGI, that defines a threshold for intelligence based on the capacity to act and perceive/associate contexts for action, and a gradation of intelligence based on the efficiency with which one learns new skills.\u00a0 Subscribe to Last Week in AI to keep reading this post and get 7 days of free access to the full post archives.", "url": "https://lastweekin.ai/p/dont-plan-for-agi-yet", "threshold": 0.9887314490102532}