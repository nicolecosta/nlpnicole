{"title": "Robotics \u2013 Google Research", "content": "We work to make robots useful in the real world through machine learning. Our goal is to improve robotics via machine learning, and improve machine learning via robotics. We foster close collaborations between machine learning researchers and roboticists to enable learning at scale on real and simulated robotic systems. We're exploring how to teach robots transferable skills, by learning in parallel across many manipulation arms in our one-of-a-kind lab purpose-built for machine learning research. We're enabling robots to predict what happens when they move objects around, to learn about the world around them and make better, safer decisions without supervision. We are sharing our training data publicly to help advance the state-of-the-art in this field. We're also bringing advances in deep learning to robot motion planning, navigation, and the exciting and demanding world of self-driving cars to improve their safety and reliability. Our Human Robot Interaction (HRI) team works to enable safe and useful interactive robots. We focus on understanding, designing, and evaluating robotic systems for use by or with humans. Example projects include: social-navigation, intuitive interfaces for natural human-robot communication, enabling robots to learn from humans as teachers and from human feedback, human robot collaboration. Our Robot Mobility team works to enable safe, autonomous, and agile mobile robots in human centered environments. Projects include: navigation, locomotion, supporting infrastructure, and robot safety. Our Robot Vision team works to leverage intermediate visual representations for generalizable robotics. Projects include 3D representations for robotics, sim2real, object trajectory-tracking, human perception, scene understanding. The Robot Manipulation team works to leverage machine learning to teach robots to act by interacting with the world. The Robot Control team works on machine learning, perception and control for advancing Robotics and Alphabet moonshots. Our Reasoning research focuses on helping robots to perform more complex tasks by breaking down long-horizon tasks into actions that robots can safely complete. Our Agility and Precision research focuses on using machine learning to make robots move in a dynamic fashion while executing precise, dexterous movements. Projects include robotics table tennis, catching, agile locomotion, and bi-manual assembly. The Robotics Infrastructure team works to build world-class shared robot, data and ML platforms to support and scale the ambitious needs of all Robotics at Google researchers, engineers, and operators. Introducing PaLM-SayCan, a robotics algorithm that combines the understanding of language models with the real-world capabilities of a helper robot. There are few large libraries with high-quality models of 3D objects. We describe our efforts to address this need by creating the Scanned Objects dataset, a curated collection of over 1000 3D-scanned common household items. In Safe Reinforcement Learning for Legged Locomotion, we introduce a safe RL framework for learning legged locomotion while satisfying safety constraints during training. In Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning, presented at ICLR 2022, we address the task of learning suitable state and action abstractions for long-range problems. In Jump-Start Reinforcement Learning (JSRL), we introduce a meta-algorithm that can use a pre-existing policy of any form to initialize any type of RL algorithm. In XIRL: Cross-Embodiment Inverse RL, presented as an oral paper at CoRL 2021, we introduce a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL). In BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning, published at CoRL 2021, we present new research that studies how robots can generalize to new tasks that they were not trained to do. We are sharing our training data publicly to help advance the state-of-the-art in this field. Our system enables us to study problems that arise from robotic learning in a challenging, multi-player, dynamic and interactive setting. Two projects illustrate the problems we have been investigating so far. i-S2R enables a robot to hold rallies of over 300 hits with a human player, while Goal\u2019s-Eye enables learning goal-conditioned policies that match the precision of amateur humans. Sim-to-real transfer is a powerful paradigm for robotic reinforcement learning. The ability to train policies in simulation enables safe exploration and large-scale data collection quickly at low cost. However, prior works in sim-to-real transfer of robotic policies typically do not involve any human-robot interaction because accurately simulating human behavior is an open problem. In this... Saminda Wishwajith Abeyruwan,  Laura Graesser,  David Bryan D'Ambrosio,  Avi Singh,  Anish Shankar,  Alex Bewley,  Deepali Jain,  Krzysztof Marcin Choromanski,  Pannag Raghunath Sanketi  Conference on Robot Learning (Oral) (2022) Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could in principle be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack contextual grounding, which makes it difficult to leverage them for decision making... Alex Irpan,  Alexander Herzog,  Alexander Toshkov Toshev,  Andy Zeng,  Anthony Brohan,  Brian Andrew Ichter,  Byron David,  Carolina Parada,  Chelsea Finn,  Clayton Tan,  Diego Reyes,  Dmitry Kalashnikov,  Eric Victor Jang,  Fei Xia,  Jarek Liam Rettinghouse,  Jasmine Chiehju Hsu,  Jornell Lacanlale Quiambao,  Julian Ibarz,  Kanishka Rao,  Karol Hausman,  Keerthana Gopalakrishnan,  Kuang-Huei Lee,  Kyle Alan Jeffrey,  Linda Luu,  Mengyuan Yan,  Michael Soogil Ahn,  Nicolas Sievers,  Nikhil J Joshi,  Noah Brown,  Omar Eduardo Escareno Cortes,  Peng Xu,  Peter Pastor Sampedro,  Pierre Sermanet,  Rosario Jauregui Ruano,  Ryan Christopher Julian,  Sally Augusta Jesmonth,  Sergey Levine,  Steve Xu,  Ted Xiao,  Vincent Olivier Vanhoucke,  Yao Lu,  Yevgen Chebotar,  Yuheng Kuang  Conference on Robot Learning 2022 (2022) Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT... Andy Zeng,  Maria Attarian,  Brian Ichter,  Krzysztof Marcin Choromanski,  Adrian Wong,  Stefan Welker,  Federico Tombari,  Aveek Purohit,  Michael Ryoo,  Vikas Sindhwani,  Johnny Chung Lee,  Vincent Olivier Vanhoucke,  Pete Florence  arXiv (2022) We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random... Krzysztof Marcin Choromanski,  Valerii Likhosherstov,  David Martin Dohan,  Xingyou Song,  Andreea Gane,  Tamas Sarlos,  Peter Hawkins,  Jared Quincy Davis,  Afroz Mohiuddin,  Lukasz Kaiser,  David Belanger,  Lucy Colwell,  Adrian Weller  accepted to ICLR 2021 (oral presentation) (to appear) Indirect trajectory optimization methods such as Differential Dynamic Programming (DDP) have found considerable success when only planning under dynamic feasibility constraints. Meanwhile, nonlinear programming (NLP) has been the state-of-the-art approach when faced with additional constraints (e.g., control bounds, obstacle avoidance). However, a na{\\\"i}ve implementation of NLP algorithms,... Sumeet Singh,  Jean-Jacques Slotine,  Vikas Sindhwani  ICRA (2022) We focus on the problem of developing energy efficient controllers for quadrupedal robots. Animals can actively switch gaits at different speeds to lower their energy consumption. In this paper, we devise a hierarchical learning framework, in which distinctive locomotion gaits and natural gait transitions emerge automatically with a simple reward of energy minimization. We use evolutionary... Yuxiang Yang,  Tingnan Zhang,  Erwin Coumans,  Jie Tan,  Byron Boots  Conference on Robot Learning (2021) We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit... Pete Florence,  Corey Lynch,  Andy Zeng,  Oscar Ramirez,  Ayzaan Wahid,  Laura Downs,  Adrian Wong,  Johnny Chung Lee,  Igor Mordatch,  Jonathan James Richard Tompson  CoRL (2021) Robots trained via reinforcement-learning (RL) requirecollecting and labeling many real-world episodes, whichmay be costly and time-consuming. Training models with alarge amount of simulation is a cheaper alternative. How-ever, simulations are not perfect and such models may nottransfer to the real world. Techniques developed to closethis simulation-to-reality (Sim2Real) gap typically... Alex Irpan,  Chris Harris,  Julian Ibarz,  Kanishka Rao,  Mohi Khansari,  Sergey Levine  CVPR 2020 (2020) Throwing is a means to increase the capabilities of a manipulator by exploiting dynamics, a form of dynamic extrinsic dexterity. In the case of pick-and-place for example, throwing can enable a robot arm to rapidly place objects into selected boxes outside its maximum kinematic range \u2014 improving its physical reachability and picking speed. However, precisely throwing arbitrary objects in... Andy Zeng,  Shuran Song,  Johnny Lee,  Alberto Rodriguez,  Thomas Funkhouser  Robotics: Science and Systems (RSS) (2019) We propose a self-supervised approach to learning a wide variety of manipulation skills from unlabeled data collected through playing in and interacting within a playground environment. Learning by playing offers three main advantages: 1) Collecting large amounts of play data is cheap and fast as it does not require staging the scene nor labeling data, 2) It relaxes the need to have a discrete... Corey Harrison Lynch,  Mohi Khansari,  Ted Xiao,  Vikash Kumar,  Jonathan James Richard Tompson,  Sergey Levine,  Pierre Sermanet  RSS (2019) Andy Zeng Krzysztof Choromanski Jasmine Hsu Carolina Parada Pierre Sermanet Vikas Sindhwani Jie Tan Chelsea Finn Sergey Levine Karol Hausman Jonathan Tompson Sherry Moore Vincent Vanhoucke", "url": "https://research.google/teams/robotics/", "threshold": -0.2545484169502873}