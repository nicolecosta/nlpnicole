{"title": "Eric Wallace", "content": "Hello! I am a fourth-year PhD student at UC Berkeley working on machine learning and NLP. I am advised by Dan Klein and Dawn Song, and I am fortunate to also work with external collaborators such as Sameer Singh, Nicholas Carlini, and Colin Raffel. My research is supported by the Apple Scholars in AI Fellowship. Outside of Berkeley, I am a student researcher at Google Brain for 2023, and I have previously interned at FAIR and AI2.    Starting Fall 2023, I'm on the job market for academia and industry! Hello! I am a fourth-year PhD student at UC Berkeley working on machine learning and NLP. I am advised by Dan Klein and Dawn Song, and I am fortunate to also work with external collaborators such as Sameer Singh, Nicholas Carlini, and Colin Raffel. My research is supported by the Apple Scholars in AI Fellowship. Outside of Berkeley, I am a student researcher at Google Brain for 2023, and I have previously interned at FAIR and AI2.    Starting Fall 2023, I'm on the job market for academia and industry! Starting Fall 2023, I'm on the job market for academia and industry! Hello! I am a fourth-year PhD student at UC Berkeley working on machine learning and NLP. I am advised by Dan Klein and Dawn Song, and I am fortunate to also work with external collaborators such as Sameer Singh, Nicholas Carlini, and Colin Raffel. My research is supported by the Apple Scholars in AI Fellowship. Outside of Berkeley, I am a student researcher at Google Brain for 2023, and I have previously interned at FAIR and AI2.  Starting Fall 2023, I'm on the job market for academia and industry! Hello! I am a fourth-year PhD student at UC Berkeley working on machine learning and NLP. I am advised by Dan Klein and Dawn Song, and I am fortunate to also work with external collaborators such as Sameer Singh, Nicholas Carlini, and Colin Raffel. My research is supported by the Apple Scholars in AI Fellowship. Outside of Berkeley, I am a student researcher at Google Brain for 2023, and I have previously interned at FAIR and AI2.  Starting Fall 2023, I'm on the job market for academia and industry! Starting Fall 2023, I'm on the job market for academia and industry! I focus on improving language models, enhancing the security/privacy/reliability of ML, and the intersection of these topics. Some of my recent research directions include: Here are some of my representative papers. See my Google Scholar page for a complete list.       Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\u00e8r, Borja Balle, Daphne Ippolito, Eric Wallace     arXiv preprint TLDR |       Twitter |       Paper|      CitationTLDR: We show how to extract hundreds of memorized images from popular diffusion models like Imagen and Stable Diffusion.@article{carlini2023extracting,          title={Extracting training data from diffusion models},          author={Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tram{\\`e}r, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},          journal={arXiv preprint arXiv:2301.13188},          year={2023}} Eric Wallace*, Nicholas Tomlin*, Albert Xu*, Kevin Yang*, Eshaan Pathak*, Matt Ginsberg, Dan Klein ACL 2022 TLDR|  Blog|  Demo|  Twitter|  Paper|  Code|  Slides|  Poster|  CitationTLDR: We create an AI for solving crossword puzzles that outperforms the world's best human players.@inproceedings{Wallace2022Crosswords,      title={Automated Crossword Solving},    author={Wallace, Eric and Tomlin, Nicholas and Xu, Albert and Yang, Kevin and Pathak, Eshaan and Ginsberg, Matthew L. and Klein, Dan},     booktitle={Association for Computational Linguistics},    year={2022}} Tony Zhao*, Eric Wallace*, Shi Feng, Dan Klein, Sameer Singh ICML 2021. Oral Presentation, top 3% TLDR|     Twitter #1#2|    Paper|    Code|     Slides|     CitationTLDR: We are the first to show that GPT-3's accuracy has high variance across different choices of the prompt. We propose a calibration procedure that reduces this variance and substantially improves average accuracy.@inproceedings{Zhao2021Calibrate,            Title = {Calibrate Before Use: Improving Few-shot Performance of Language Models},          Author = {Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},           booktitle={International Conference on Machine Learning},          Year = {2021}} Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, Colin Raffel USENIX Security 2021 TLDR| Blog| Twitter #1#2| Paper| Code| CitationTLDR: We create a method for extracting verbatim training examples from a language model.@inproceedings{carlini2020extracting,            title={Extracting Training Data from Large Language Models},            author={Nicholas Carlini and Florian Tram\\`er and Eric Wallace and Matthew Jagielski              and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown             and Dawn Song and \\'Ulfar Erlingsson and Alina Oprea and Colin Raffel},            booktitle={USENIX Security Symposium},            year={2021}} Taylor Shin*, Yasaman Razeghi*, Robert L Logan IV*, Eric Wallace, Sameer Singh EMNLP 2020 TLDR| Twitter|Paper| Code| CitationTLDR: We propose a method for automatically designing prompts for large language models.@inproceedings{Shin2020Autoprompt,          Author = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},              BookTitle={Empirical Methods in Natural Language Processing},          Year = {2020},          Title = {{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts}} Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh EMNLP 2019 TLDR| Video| Blog| Twitter| Paper| Code| Slides| CitationTLDR: We create phrases that cause a model to produce a specific prediction when concatenated to any input. Triggers reveal egregious and insightful errors for text classification, reading comprehension, and text generation. @inproceedings{Wallace2019Triggers,    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},    Booktitle = {Empirical Methods in Natural Language Processing},    Year = {2019},    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}} Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh EMNLP 2019. Best Demo Award TLDR|            Landing Page|            Twitter|            Demo|            Paper|            Poster|            CitationTLDR: We build an open-source toolkit on top of AllenNLP that makes it easy to interpret NLP models. @inproceedings{Wallace2019AllenNLP,                     Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},                     Booktitle = {Empirical Methods in Natural Language Processing},                     Year = {2019},                     Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}} Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber EMNLP 2018 TLDR|             Video|             Paper|             Slides|             Code|             CitationTLDR: Saliency maps, a popular interpretation technique, can be negatively impacted by certain pathological behavior present in neural models, such as prediction overconfidence. @inproceedings{Feng2018Pathological,                    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},                    Booktitle = {Empirical Methods in Natural Language Processing},                    Year = {2018},                    Title = {Pathologies of Neural Models Make Interpretations Difficult}} I am passionate about mentoring and teaching. At the moment, I currently don't have capacity to advise new research projects, but I happy to answer questions via email about my work and research at Berkeley. Spring 2023      Co-taught with Dan Klein and Kevin Lin. In the second half of the course, I covered many cutting-edge topics including LM scaling, systems, risks, RLHF, retrieval, and more.      Slides EMNLP 2020     Sameer Singh, Matt Gardner, and I gave a tutorial on methods for interpreting the predictions and behavior of NLP and language models.      Slides|       Website Here are a few news articles that feature my work, including interviews with me or my colleagues.", "url": "https://www.ericswallace.com/", "threshold": 0.9595866037952592}