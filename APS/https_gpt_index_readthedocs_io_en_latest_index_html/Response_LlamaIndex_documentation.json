{"title": "Response \u2014 LlamaIndex  documentation", "content": "Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM\u2019s with external data. Github: https://github.com/jerryjliu/llama_index LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM\u2019s reasoning capabilities to generate a response. To perform LLM\u2019s data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing That\u2019s where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API\u2019s, PDF\u2019s, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM\u2019s. These indices help to abstract away common boilerplate and pain points for in-context learning: Storing context in an easy-to-access format for prompt insertion. Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big. Dealing with text splitting. Provides users an interface to query the index (feed in an input prompt) and obtain a knowledge-augmented output. Offers you a comprehensive toolset trading off cost and performance. Getting Started Guides Use Cases Key Components Reference Gallery \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery You can simply do: Git clone this repository: git clone git@github.com:jerryjliu/gpt_index.git. Then do: pip install -e . if you want to do an editable install (you can modify source files) of just the package itself. pip install -r requirements.txt if you want to install optional dependencies + dependencies used for development (e.g. unit testing). By default, we use the OpenAI GPT-3 text-davinci-003 model. In order to use this, you must have an OPENAI_API_KEY setup.You can register an API key by logging into OpenAI\u2019s page and creating a new API token. You can customize the underlying LLM in the Custom LLMs How-To (courtesy of Langchain). You mayneed additional environment keys + tokens setup depending on the LLM provider. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Here is a starter example for using LlamaIndex. Make sure you\u2019ve followed the installation steps first. LlamaIndex examples can be found in the examples folder of the LlamaIndex repository.We first want to download this examples folder. An easy way to do this is to just clone the repo: Next, navigate to your newly-cloned repository, and verify the contents: We now want to navigate to the following folder: This contains LlamaIndex examples around Paul Graham\u2019s essay, \u201cWhat I Worked On\u201d. A comprehensive set of examples are already provided in TestEssay.ipynb. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running. Create a new .py file with the following: This builds an index over the documents in the data folder (which in this case just consists of the essay text). We then run the following You should get back a response similar to the following: The author wrote short stories and tried to program on an IBM 1401. In a Jupyter notebook, you can view info and/or debugging logging using the following snippet: You can set the level to DEBUG for verbose output, or use level=logging.INFO for less. To save to disk and load from disk, do That\u2019s it! For more information on LlamaIndex features, please check out the numerous \u201cGuides\u201d to the left.If you are interested in further exploring how LlamaIndex works, check out our Primer Guide. Additionally, if you would like to play around with Example Notebooks, check out this link. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At its core, LlamaIndex contains a toolkit designed to easily connect LLM\u2019s with your external data.LlamaIndex helps to provide the following: A set of data structures that allow you to index your data for various LLM tasks, and remove concerns over prompt size limitations. Data connectors to your common data sources (Google Docs, Slack, etc.). Cost transparency + tools that reduce cost while increasing performance. Each data structure offers distinct use cases and a variety of customizable parameters. These indices can then bequeried in a general purpose manner, in order to achieve any task that you would typically achieve with an LLM: Question-Answering Summarization Text Generation (Stories, TODO\u2019s, emails, etc.) and more! The guides below are intended to help you get the most out of LlamaIndex. It gives a high-level overview of the following: The general usage pattern of LlamaIndex. Mapping Use Cases to LlamaIndex data Structures How Each Index Works General Guides \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This section contains a list of in-depth tutorials on how to best utilize different capabilitiesof LlamaIndex within your end-user application. They include a broad range of LlamaIndex concepts: Semantic search Structured data support Composability/Query Transformation They also showcase a variety of application settings that LlamaIndex can be used, from a simpleJupyter notebook to a chatbot to a full-stack web application. Tutorials \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery We offer a wide variety of example notebooks. They are referenced throughout the documentation. Example notebooks are found here. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,whether it\u2019s question-answering, summarization, or a component in a chatbot. This section describes the different ways you can query your data with LlamaIndex, roughly in orderof simplest (top-k semantic search), to more advanced capabilities. The most basic example usage of LlamaIndex is through semantic search. We providea simple in-memory vector store for you to get started, but you can also chooseto use any one of our vector store integrations: Relevant Resources: Quickstart Example notebook A summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.For instance, a summarization query could look like one of the following: \u201cWhat is a summary of this collection of text?\u201d \u201cGive me a summary of person X\u2019s experience with the company.\u201d In general, a list index would be suited for this use case. A list index by default goes through all the data. Empirically, setting response_mode=\"tree_summarize\" also leads to better summarization results. LlamaIndex supports queries over structured data, whether that\u2019s a Pandas DataFrame or a SQL Database. Here are some relevant resources: Guide on Text-to-SQL SQL Demo Notebook 1 SQL Demo Notebook 2 (Context) SQL Demo Notebook 3 (Big tables) Pandas Demo Notebook. LlamaIndex supports synthesizing across heterogenous data sources. This can be done by composing a graph over your existing data.Specifically, compose a list index over your subindices. A list index inherently combines information for each node; thereforeit can synthesize information across your heteregenous data sources. Here are some relevant resources: Composability City Analysis Demo. LlamaIndex also supports routing over heteregenous data sources - for instance, if you want to \u201croute\u201d a query to anunderlying Document or a subindex.Here you have three options: GPTTreeIndex, GPTKeywordTableIndex, or aVector Store Index. A GPTTreeIndex uses the LLM to select the child node(s) to send the query down to.A GPTKeywordTableIndex uses keyword matching, and a GPTVectorStoreIndex usesembedding cosine similarity. Here are some relevant resources: Composability Composable Keyword Table Graph. LlamaIndex can support compare/contrast queries as well. It can do this in the following fashion: Composing a graph over your data Adding in query transformations. You can perform compare/contrast queries by just composing a graph over your data. Here are some relevant resources: Composability SEC 10-k Analysis Example notebook. You can also perform compare/contrast queries with a query transformation module. This module will help break down a complex query into a simpler one over your existing index structure. Here are some relevant resources: Query Transformations City Analysis Example Notebook LlamaIndex can also support multi-step queries. Given a complex query, break it down into subquestions. For instance, given a question \u201cWho was in the first batch of the accelerator program the author started?\u201d,the module will first decompose the query into a simpler initial question \u201cWhat was the accelerator program the author started?\u201d,query the index, and then ask followup questions. Here are some relevant resources: Query Transformations Multi-Step Query Decomposition Notebook \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex modules provide plug and play data loaders, data structures, and query interfaces. They can be used in your downstream LLM Application. Some of these applications are described below. Chatbots are an incredibly popular use case for LLM\u2019s. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents. Relevant Resources: Building a Chatbot Using with a LangChain Agent LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit. We provide tutorials and resources to help you get started in this area. Relevant Resources: Fullstack Application Guide LlamaIndex Starter Pack \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Our data connectors are offered through LlamaHub \ud83e\udd99.LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  Some sample data connectors: local file directory (SimpleDirectoryReader). Can support parsing a wide range of file types: .pdf, .jpg, .png, .docx, etc. Notion (NotionPageReader) Google Docs (GoogleDocsReader) Slack (SlackReader) Discord (DiscordReader) Each data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a download_loader function, whichdownloads the loader file into a module that you can use within your application. Example usage: \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At the core of LlamaIndex is a set of index data structures. You can choose to use them on their own,or you can choose to compose a graph over these data structures. In the following sections, we detail how each index structure works, as well as some of the keycapabilities our indices/graphs provide. Index Structures \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a query interface over your index or graph structure. This query interfaceallows you to both retrieve the set of relevant documents, as well as synthesize a response. The basic query interface is found in our usage pattern guide. The guidedetails how to specify parameters for a basic query over a single index structure. A more advanced query interface is found in our composability guide. The guidedescribes how to specify a graph over multiple index structures. Finally, we provide a guide to our Query Transformations module. Query Interface \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides the ability to customize the following components: LLM Prompts Embedding model These are described in their respective guides below. Customizable Modules \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a variety of tools for analysis and optimizationof your indices and queries. Some of our tools involve the analysis/optimization of token usage and cost. We also offer a Playground module, giving you a visual means of analyzingthe token usage of various index structures + performance. Analysis and Optimization \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LLM output/validation capabilities are crucial to LlamaIndex in the following areas: Document retrieval: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \u201cANSWER: (number)\u201d. Response synthesis: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.) LlamaIndex supports integrations with output parsing modules offeredby other frameworks. These output parsing modules can be used in the following ways: To provide formatting instructions for any prompt / query (through output_parser.format) To provide \u201cparsing\u201d for LLM outputs (through output_parser.parse) Guardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example. Output: Langchain also offers output parsing modules that you can use within LlamaIndex. Output: \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex provides a diverse range of integrations with other toolsets and storage providers. Some of these integrations are provided in more detailed guides below. Integrations \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This doc shows both the overarching class used to represent an index. Theseclasses allow for index creation, insertion, and also querying.We first show the different index subclasses.We then show the base class that all indices inherit from, which containsparameters and methods common to all indices. Index Data Structures Base index classes. Base LlamaIndex. nodes (List[Node]) \u2013 List of nodes to index service_context (ServiceContext) \u2013 Service context container (containscomponents like LLMPredictor, PromptHelper, etc.). Asynchronously answer a query. When query is called, we query the index with the given mode andquery_kwargs. The mode determines the type of query to run, andquery_kwargs are parameters that are specific to the query type. For a comprehensive documentation of available mode and query_kwargs toquery a given index, please visit Querying an Index. Delete a document from the index. All nodes in the index related to the index will be deleted. doc_id (str) \u2013 document id Get the docstore corresponding to the index. Create index from documents. documents (Optional[Sequence[BaseDocument]]) \u2013 List of documents tobuild the index from. Get query map. Get the index struct. Load index from dict. Load index from disk. This method loads the index from a JSON file stored on disk. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). NOTE: load_from_disk should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_disk and load_from_disk on that instead. save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Load index from string (in JSON-format). This method loads the index from a JSON string. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). NOTE: load_from_string should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_string and load_from_string on that instead. index_string (str) \u2013 The index string (in JSON-format). The loaded index. BaseGPTIndex Answer a query. When query is called, we query the index with the given mode andquery_kwargs. The mode determines the type of query to run, andquery_kwargs are parameters that are specific to the query type. For a comprehensive documentation of available mode and query_kwargs toquery a given index, please visit Querying an Index. Refresh an index with documents that have changed. This allows users to save LLM and Embedding model calls, while onlyupdating documents that have any changes in text or extra_info. Itwill also insert any documents that previously were not stored. Save to dict. Save to file. This method stores the index into a JSON file stored on disk. NOTE: save_to_disk should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_disk and load_from_disk on that instead. save_path (str) \u2013 The save_path of the file. encoding (str) \u2013 The encoding of the file. Save to string. This method stores the index into a JSON string. NOTE: save_to_string should not be used for indices composed on topof other indices. Please define a ComposableGraph and usesave_to_string and load_from_string on that instead. The JSON string of the index. str Update a document. This is equivalent to deleting the document and then inserting it again. document (Union[BaseDocument, BaseGPTIndex]) \u2013 document to update insert_kwargs (Dict) \u2013 kwargs to pass to insert delete_kwargs (Dict) \u2013 kwargs to pass to delete \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This doc shows the classes that are used to query indices.We first show index-specific query subclasses.We then show how to define a query config in order to recursively querymultiple indices that are composed together.We then show the base query class, which contains parameters that are sharedamong all queries.Lastly, we show how to customize the string(s) used for an embedding-based query. Index-specific Query Subclasses This section shows how to define a query config in order to recursively querymultiple indices that are composed together. Query Configs for Composed Indices Base query classes. Base LlamaIndex Query. Helper class that is used to query an index. Can be called within querymethod of a BaseGPTIndex object, or instantiated independently. service_context (ServiceContext) \u2013 service context container (contains componentslike LLMPredictor, PromptHelper). required_keywords (List[str]) \u2013 Optional list of keywords that must be presentin nodes. Can be used to query most indices (tree index is an exception). exclude_keywords (List[str]) \u2013 Optional list of keywords that must not bepresent in nodes. Can be used to query most indices (tree index is anexception). response_mode (ResponseMode) \u2013 Optional ResponseMode. If not provided, willuse the default ResponseMode. text_qa_template (QuestionAnswerPrompt) \u2013 Optional QuestionAnswerPrompt object.If not provided, will use the default QuestionAnswerPrompt. refine_template (RefinePrompt) \u2013 Optional RefinePrompt object. If not provided,will use the default RefinePrompt. include_summary (bool) \u2013 Optional bool. If True, will also use the summarytext of the index when generating a response (the summary text can be setthrough index.set_text(\u201c<text>\u201d)). similarity_cutoff (float) \u2013 Optional float. If set, will filter out nodes withsimilarity below this cutoff threshold when computing the response streaming (bool) \u2013 Optional bool. If True, will return a StreamingResponseobject. If False, will return a Response object. Get the index struct. Get list of tuples of node and similarity for response. First part of the tuple is the node.Second part of tuple is the distance from query to the node.If not applicable, it\u2019s None. Query bundle enables user to customize the string(s) used for embedding-based query. Query Configuration Schema. This schema is used under the hood for all queries, but is primarilyexposed for recursive queries over composable indices. Query bundle. This dataclass contains the original query string and associated transformations. query_str (str) \u2013 the original user-specified query string.This is currently used by all non embedding-based queries. embedding_strs (list[str]) \u2013 list of strings used for embedding the query.This is currently used by all embedding-based queries. embedding (list[float]) \u2013 the stored embedding for the query. Use custom embedding strs if specified, otherwise use query str. Query config. Used under the hood for all queries.The user must explicitly specify a list of query config objects is passed duringa query call to define configurations for each individual subindex within anoverall composed index. The user may choose to specify either the query config objects directly,or as a list of JSON dictionaries. For instance, the following are equivalent: index_struct_id (Optional[str]) \u2013 The index struct id. This can be obtainedby calling\u201cget_doc_id\u201d on the original index class. This can be set by calling\u201cset_doc_id\u201d on the original index class. index_struct_type (IndexStructType) \u2013 The type of index struct. query_mode (QueryMode) \u2013 The query mode. query_kwargs (Dict[str, Any], optional) \u2013 The query kwargs. Defaults to {}. Query mode enum. Can be passed as the enum struct, or as the underlying string. Default query mode. \u201cdefault\u201d Retrieve mode. \u201cretrieve\u201d Embedding mode. \u201cembedding\u201d Summarize mode. Used for hierarchicalsummarization in the tree index. \u201csummarize\u201d Simple mode. Used for keyword extraction. \u201csimple\u201d RAKE mode. Used for keyword extraction. \u201crake\u201d Recursive mode. Used to recursively queryover composed indices. \u201crecursive\u201d Query Transform Query transform augments a raw query string with associated transformationsto improve index querying. Query Transforms. Decompose query transform. Decomposes query into a subquery given the current index struct.Performs a single step transformation. llm_predictor (Optional[LLMPredictor]) \u2013 LLM for generatinghypothetical documents Run query transform. Hypothetical Document Embeddings (HyDE) query transform. It uses an LLM to generate hypothetical answer(s) to a given query,and use the resulting documents as embedding strings. As described in [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496) Run query transform. Step decompose query transform. Decomposes query into a subquery given the current index structand previous reasoning. NOTE: doesn\u2019t work yet. llm_predictor (Optional[LLMPredictor]) \u2013 LLM for generatinghypothetical documents Run query transform. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Below we show the API reference for composable data structures. Init composability. Composable graph. Query the index. Create composable graph using this index class as the root. NOTE: this is mostly syntactic sugar,roughly equivalent to directly calling ComposableGraph.from_indices. Get index from index struct id. Load index from disk. This method loads the index from a JSON file stored on disk. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Load index from string (in JSON-format). This method loads the index from a JSON string. The index datastructure itself is preserved completely. If the index is defined oversubindices, those subindices will also be preserved (and subindices ofthose subindices, etc.). save_path (str) \u2013 The save_path of the file. The loaded index. BaseGPTIndex Query the index. Save to file. This method stores the index into a JSON file stored on disk. save_path (str) \u2013 The save_path of the file. Save to string. This method stores the index into a JSON file stored on disk. save_path (str) \u2013 The save_path of the file. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery NOTE: Our data connectors are now offered through LlamaHub \ud83e\udd99.LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application. The following data connectors are still available in the core repo. Data Connectors for LlamaIndex. This module contains the data connectors for LlamaIndex. Each connector inheritsfrom a BaseReader class, connects to a data source, and loads Document objectsfrom that data source. You may also choose to construct Document objects manually, for instancein our Insert How-To Guide. See below for the APIdefinition of a Document - the bare minimum is a text property. BeautifulSoup web page reader. Reads pages from the web.Requires the bs4 and urllib packages. file_extractor (Optional[Dict[str, Callable]]) \u2013 A mapping of websitehostname (e.g. google.com) to a function that specifies how toextract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR. Load data from the urls. urls (List[str]) \u2013 List of URLs to scrape. custom_hostname (Optional[str]) \u2013 Force a certain hostname in the casea website is displayed under custom URLs (e.g. Substack blogs) List of documents. List[Document] Load data in LangChain document format. ChatGPT Retrieval Plugin reader. Load data from ChatGPT Retrieval Plugin. Load data in LangChain document format. Chroma reader. Retrieve documents from existing persisted Chroma collections. collection_name \u2013 Name of the peristed collection. persist_directory \u2013 Directory where the collection is persisted. Create documents from the results. results \u2013 Results from the query. List of documents. Load data from the collection. limit \u2013 Number of results to return. where \u2013 Filter results by metadata. {\u201cmetadata_field\u201d: \u201cis_equal_to_this\u201d} where_document \u2013 Filter results by document. {\u201c$contains\u201d:\u201dsearch_string\u201d} List of documents. Load data in LangChain document format. Discord reader. Reads conversations from channels. discord_token (Optional[str]) \u2013 Discord token. If not provided, weassume the environment variable DISCORD_TOKEN is set. Load data from the input directory. channel_ids (List[int]) \u2013 List of channel ids to read. limit (Optional[int]) \u2013 Maximum number of messages to read. oldest_first (bool) \u2013 Whether to read oldest messages first.Defaults to True. List of documents. List[Document] Load data in LangChain document format. Generic interface for a data document. This document connects to data sources. Extra info string. Convert struct from LangChain document format. Get doc_hash. Get doc_id. Get embedding. Errors if embedding is None. Get text. Get Document type. Get Document type. Check if doc_id is None. Check if text is None. Convert struct to LangChain document format. Read documents from an Elasticsearch/Opensearch index. These documents can then be used in a downstream Llama Index data structure. endpoint (str) \u2013 URL (http/https) of cluster index (str) \u2013 Name of the index (required) httpx_client_args (dict) \u2013 Optional additional args to pass to the httpx.Client Read data from the Elasticsearch index. field (str) \u2013 Field in the document to retrieve text from query (Optional[dict]) \u2013 Elasticsearch JSON query DSL object.For example:{\u201cquery\u201d: {\u201cmatch\u201d: {\u201cmessage\u201d: {\u201cquery\u201d: \u201cthis is a test\u201d}}}} embedding_field (Optional[str]) \u2013 If there are embeddings stored inthis index, this field can be usedto set the embedding field on the returned Document list. A list of documents. List[Document] Load data in LangChain document format. Faiss reader. Retrieves documents through an existing in-memory Faiss index.These documents can then be used in a downstream LlamaIndex data structure.If you wish use Faiss itself as an index to to organize documents,insert documents, and perform queries on them, please use GPTFaissIndex. faiss_index (faiss.Index) \u2013 A Faiss Index object (required) Load data from Faiss. query (np.ndarray) \u2013 A 2D numpy array of query vectors. id_to_text_map (Dict[str, str]) \u2013 A map from ID\u2019s to text. k (int) \u2013 Number of nearest neighbors to retrieve. Defaults to 4. separate_documents (Optional[bool]) \u2013 Whether to return separatedocuments. Defaults to True. A list of documents. List[Document] Load data in LangChain document format. Github repository reader. Retrieves the contents of a Github repository and returns a list of documents.The documents are either the contents of the files in the repository or the textextracted from the files using the parser. Examples Load data from a commit or a branch. Loads github repository data from a specific commit sha or a branch. commit \u2013 commit sha branch \u2013 branch name list of documents Load data in LangChain document format. Google Docs reader. Reads a page from Google Docs Load data from the input directory. document_ids (List[str]) \u2013 a list of document ids. Load data in LangChain document format. JSON reader. Reads JSON documents with options to help suss out relationships between nodes. levels_back (int) \u2013 the number of levels to go back in the JSON tree, 0 None (if you want all levels. If levels_back is) \u2013  the (then we just format) \u2013  embedding (JSON and make each line an) \u2013  collapse_length (int) \u2013 the maximum number of characters a JSON fragment output (would be collapsed in the) \u2013  ex \u2013 if collapse_length = 10, and {a (input is) \u2013 [1, 2, 3], b: {\u201chello\u201d: \u201cworld\u201d, \u201cfoo\u201d: \u201cbar\u201d}} line (then a would be collapsed into one) \u2013  not. (while b would) \u2013  there. (Recommend starting around 100 and then adjusting from) \u2013  Load data from the input file. Load data in LangChain document format. Make reader. Load data from the input directory. NOTE: This is not implemented. Load data in LangChain document format. Pass response object to webhook. webhook_url (str) \u2013 Webhook URL. response (Response) \u2013 Response object. query (Optional[str]) \u2013 Query. Defaults to None. Mbox e-mail reader. Reads a set of e-mails saved in the mbox format. Load data from the input directory. max_count (int): Maximum amount of messages to read.message_format (str): Message format overriding default. Load data in LangChain document format. Notion Page reader. Reads a set of Notion pages. integration_token (str) \u2013 Notion integration token. Load data from the input directory. page_ids (List[str]) \u2013 List of page ids to load. List of documents. List[Document] Load data in LangChain document format. Get all the pages from a Notion database. Read a page. Search Notion page given a text query. Utilities for loading data from an Obsidian Vault. input_dir (str) \u2013 Path to the vault. Load data from the input directory. Load data in LangChain document format. Pinecone reader. api_key (str) \u2013 Pinecone API key. environment (str) \u2013 Pinecone environment. Load data from Pinecone. index_name (str) \u2013 Name of the index. id_to_text_map (Dict[str, str]) \u2013 A map from ID\u2019s to text. separate_documents (Optional[bool]) \u2013 Whether to return separatedocuments per retrieved entry. Defaults to True. vector (List[float]) \u2013 Query vector. top_k (int) \u2013 Number of results to return. include_values (bool) \u2013 Whether to include the embedding in the response.Defaults to True. **query_kwargs \u2013 Keyword arguments to pass to the query.Arguments are the exact same as those found inPinecone\u2019s reference documentation for thequery method. A list of documents. List[Document] Load data in LangChain document format. Qdrant reader. Retrieve documents from existing Qdrant collections. host \u2013 Host name of Qdrant service. port \u2013 Port of the REST API interface. Default: 6333 grpc_port \u2013 Port of the gRPC interface. Default: 6334 prefer_grpc \u2013 If true - use gPRC interface whenever possible in custom methods. https \u2013 If true - use HTTPS(SSL) protocol. Default: false api_key \u2013 API key for authentication in Qdrant Cloud. Default: None prefix \u2013 If not None - add prefix to the REST URL path.Example: service/v1 will result inhttp://localhost:6333/service/v1/{qdrant-endpoint} for REST API.Default: None timeout \u2013 Timeout for REST and gRPC API requests.Default: 5.0 seconds for REST and unlimited for gRPC Load data from Qdrant. collection_name (str) \u2013 Name of the Qdrant collection. query_vector (List[float]) \u2013 Query vector. limit (int) \u2013 Number of results to return. A list of documents. List[Document] Load data in LangChain document format. RSS reader. Reads content from an RSS feed. Load data from RSS feeds. urls (List[str]) \u2013 List of RSS URLs to load. List of documents. List[Document] Load data in LangChain document format. Simple directory reader. Can read files into separate documents, or concatenatesfiles into one document text. input_dir (str) \u2013 Path to the directory. input_files (List) \u2013 List of file paths to read(Optional; overrides input_dir, exclude) exclude (List) \u2013 glob of python file paths to exclude (Optional) exclude_hidden (bool) \u2013 Whether to exclude hidden files (dotfiles). errors (str) \u2013 how encoding and decoding errors are to be handled,see https://docs.python.org/3/library/functions.html#open recursive (bool) \u2013 Whether to recursively search in subdirectories.False by default. required_exts (Optional[List[str]]) \u2013 List of required extensions.Default is None. file_extractor (Optional[Dict[str, BaseParser]]) \u2013 A mapping of fileextension to a BaseParser class that specifies how to convert that fileto text. See DEFAULT_FILE_EXTRACTOR. num_files_limit (Optional[int]) \u2013 Maximum number of files to read.Default is None. file_metadata (Optional[Callable[str, Dict]]) \u2013 A function that takesin a filename and returns a Dict of metadata for the Document.Default is None. Load data from the input directory. concatenate (bool) \u2013 whether to concatenate all text docs into a single doc.If set to True, file metadata is ignored. False by default.This setting does not apply to image docs (always one doc per image). A list of documents. List[Document] Load data in LangChain document format. Simple mongo reader. Concatenates each Mongo doc into Document used by LlamaIndex. host (str) \u2013 Mongo host. port (int) \u2013 Mongo port. max_docs (int) \u2013 Maximum number of documents to load. Load data from the input directory. db_name (str) \u2013 name of the database. collection_name (str) \u2013 name of the collection. query_dict (Optional[Dict]) \u2013 query to filter documents.Defaults to None A list of documents. List[Document] Load data in LangChain document format. Simple web page reader. Reads pages from the web. html_to_text (bool) \u2013 Whether to convert HTML to text.Requires html2text package. Load data from the input directory. urls (List[str]) \u2013 List of URLs to scrape. List of documents. List[Document] Load data in LangChain document format. Slack reader. Reads conversations from channels. If an earliest_date is provided, anoptional latest_date can also be provided. If no latest_date is provided,we assume the latest date is the current timestamp. slack_token (Optional[str]) \u2013 Slack token. If not provided, weassume the environment variable SLACK_BOT_TOKEN is set. ssl (Optional[str]) \u2013 Custom SSL context. If not provided, it is assumedthere is already an SSL context available. earliest_date (Optional[datetime]) \u2013 Earliest date from whichto read conversations. If not provided, we read all messages. latest_date (Optional[datetime]) \u2013 Latest date from which toread conversations. If not provided, defaults to current timestampin combination with earliest_date. Load data from the input directory. channel_ids (List[str]) \u2013 List of channel ids to read. List of documents. List[Document] Load data in LangChain document format. Reads persistent Steamship Files and converts them to Documents. api_key \u2013 Steamship API key. Defaults to STEAMSHIP_API_KEY value if not provided. Note Requires install of steamship package and an active Steamship API Key.To get a Steamship API Key, visit: https://steamship.com/account/api.Once you have an API Key, expose it via an environment variable namedSTEAMSHIP_API_KEY or pass it as an init argument (api_key). Load data from persistent Steamship Files into Documents. workspace \u2013 the handle for a Steamship workspace(see: https://docs.steamship.com/workspaces/index.html) query \u2013 a Steamship tag query for retrieving files(ex: \u2018filetag and value(\u201cimport-id\u201d)=\u201dimport-001\u201d\u2019) file_handles \u2013 a list of Steamship File handles(ex: smooth-valley-9kbdr) collapse_blocks \u2013 whether to merge individual File Blocks into asingle Document, or separate them. join_str \u2013 when collapse_blocks is True, this is how the block textswill be concatenated. Note The collection of Files from both query and file_handles will becombined. There is no (current) support for deconflicting the collections(meaning that if a file appears both in the result set of the query andas a handle in file_handles, it will be loaded twice). Load data in LangChain document format. String Iterable Reader. Gets a list of documents, given an iterable (e.g. list) of strings. Example Load the data. Load data in LangChain document format. Trafilatura web page reader. Reads pages from the web.Requires the trafilatura package. Load data from the urls. urls (List[str]) \u2013 List of URLs to scrape. List of documents. List[Document] Load data in LangChain document format. Twitter tweets reader. Read tweets of user twitter handle. Check \u2018https://developer.twitter.com/en/docs/twitter-api/        getting-started/getting-access-to-the-twitter-api\u2019         on how to get access to twitter API. bearer_token (str) \u2013 bearer_token that you get from twitter API. num_tweets (Optional[int]) \u2013 Number of tweets for each user twitter handle.            Default is 100 tweets. Load tweets of twitter handles. twitterhandles (List[str]) \u2013 List of user twitter handles to read tweets. Load data in LangChain document format. Weaviate reader. Retrieves documents from Weaviate through vector lookup. Allows optionto concatenate retrieved documents into one Document, or to returnseparate Document objects per document. host (str) \u2013 host. auth_client_secret (Optional[weaviate.auth.AuthCredentials]) \u2013 auth_client_secret. Load data from Weaviate. If graphql_query is not found in load_kwargs, we assume thatclass_name and properties are provided. class_name (Optional[str]) \u2013 class_name to retrieve documents from. properties (Optional[List[str]]) \u2013 properties to retrieve from documents. graphql_query (Optional[str]) \u2013 Raw GraphQL Query.We assume that the query is a Get query. separate_documents (Optional[bool]) \u2013 Whether to return separatedocuments. Defaults to True. A list of documents. List[Document] Load data in LangChain document format. Wikipedia reader. Reads a page. Load data from the input directory. pages (List[str]) \u2013 List of pages to read. Load data in LangChain document format. Youtube Transcript reader. Load data from the input directory. pages (List[str]) \u2013 List of youtube links                 for which transcripts are to be read. Load data in LangChain document format. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery These are the reference prompt templates. We first show links to default prompts.We then document all core prompts, with their required variables. We then show the base prompt class,derived from Langchain. The list of default prompts can be found here. NOTE: we\u2019ve also curated a set of refine prompts for ChatGPT use cases.The list of ChatGPT refine prompts can befound here. Subclasses from base prompt. Keyword extract prompt. Prompt to extract keywords from a text text with a maximum ofmax_keywords keywords. Required template variables: text, max_keywords template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Define the knowledge graph triplet extraction prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Pandas prompt. Convert query to python code. Required template variables: query_str, df_str, instruction_str. template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Query keyword extract prompt. Prompt to extract keywords from a query query_str with a maximumof max_keywords keywords. Required template variables: query_str, max_keywords template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Question Answer prompt. Prompt to answer a question query_str given a context context_str. Required template variables: context_str, query_str template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Refine prompt. Prompt to refine an existing answer existing_answer given a context context_msg,and a query query_str. Required template variables: query_str, existing_answer, context_msg template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Refine Table context prompt. Prompt to refine a table context given a table schema schema,as well as unstructured text context context_msg, anda task query_str.This includes both a high-level description of the tableas well as a description of each column in the table. template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Schema extract prompt. Prompt to extract schema from unstructured text text. Required template variables: text, schema template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Simple Input prompt. Required template variables: query_str. template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Summary prompt. Prompt to summarize the provided context_str. Required template variables: context_str template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Table context prompt. Prompt to generate a table context given a table schema schema,as well as unstructured text context context_str, anda task query_str.This includes both a high-level description of the tableas well as a description of each column in the table. template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Text to SQL prompt. Prompt to translate a natural language query into SQL in the dialectdialect given a schema schema. Required template variables: query_str, schema, dialect template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Tree Insert prompt. Prompt to insert a new chunk of text new_chunk_text into the tree index.More specifically, this prompt has the LLM select the relevant candidatechild node to continue tree traversal. Required template variables: num_chunks, context_list, new_chunk_text template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Tree select multiple prompt. Prompt to select multiple candidate child nodes out of allchild nodes provided in context_list, given a query query_str.branching_factor refers to the number of child nodes to select, andnum_chunks is the number of child nodes in context_list. branching_factor template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Tree select prompt. Prompt to select a candidate child node out of all child nodesprovided in context_list, given a query query_str. num_chunks isthe number of child nodes in context_list. Required template variables: num_chunks, context_list, query_str template (str) \u2013 Template for the prompt. **prompt_kwargs \u2013 Keyword arguments for the prompt. Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. Prompt class. Prompt class for LlamaIndex. enforce certain prompt types partially fill values define stop token Format the prompt. Load prompt from LangChain prompt. Load prompt from LangChain prompt. Create a prompt from an existing prompt. Use case: If the existing prompt is already partially filled,and the remaining fields satisfy the requirements of theprompt class, then we can create a new prompt from the existingpartially filled prompt. Get langchain prompt. Format the prompt partially. Return an instance of itself. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery The service context container is a utility container for LlamaIndexindex and query classes. The container contains the followingobjects that are commonly used for configuring every index andquery, such as the LLMPredictor (for configuring the LLM),the PromptHelper (for configuring input size/chunk size),the BaseEmbedding (for configuring the embedding model), and more. Service Context Classes Service Context container. The service context container is a utility container for LlamaIndexindex and query classes. It contains the following:- llm_predictor: LLMPredictor- prompt_helper: PromptHelper- embed_model: BaseEmbedding- node_parser: NodeParser- llama_logger: LlamaLogger- chunk_size_limit: chunk size limit Create a ServiceContext from defaults.If an argument is specified, then use the argument value provided for thatparameter. If an argument is not specified, then use the default value. llm_predictor (Optional[LLMPredictor]) \u2013 LLMPredictor prompt_helper (Optional[PromptHelper]) \u2013 PromptHelper embed_model (Optional[BaseEmbedding]) \u2013 BaseEmbedding node_parser (Optional[NodeParser]) \u2013 NodeParser llama_logger (Optional[LlamaLogger]) \u2013 LlamaLogger chunk_size_limit (Optional[int]) \u2013 chunk_size_limit \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Optimization. Optimization of a text chunk given the query by shortening the input text. Optimize a text chunk given the query by shortening the input text. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Our structured indices are documented in Structured Store Index.Below, we provide a reference of the classes that are used to configure our structured indices. SQL wrapper around SQLDatabase in langchain. SQL Database. Wrapper around SQLDatabase object from langchain. Offerssome helper utilities for insertion and querying.See langchain documentation for more details: *args \u2013 Arguments to pass to langchain SQLDatabase. **kwargs \u2013 Keyword arguments to pass to langchain SQLDatabase. Return string representation of dialect to use. Return SQL Alchemy engine. Construct a SQLAlchemy engine from URI. Get table info for a single table. Get table columns. Get information about specified tables. Follows best practices as specified in: Rajkumar et al, 2022(https://arxiv.org/abs/2204.00498) If sample_rows_in_table_info, the specified number of sample rows will beappended to each table description. This can increase performance asdemonstrated in the paper. Get information about specified tables. Follows best practices as specified in: Rajkumar et al, 2022(https://arxiv.org/abs/2204.00498) If sample_rows_in_table_info, the specified number of sample rows will beappended to each table description. This can increase performance asdemonstrated in the paper. Get names of tables available. Insert data into a table. Execute a SQL command and return a string representing the results. If the statement returns rows, a string of the results is returned.If the statement returns no rows, an empty string is returned. Execute a SQL command and return a string representing the results. If the statement returns rows, a string of the results is returned.If the statement returns no rows, an empty string is returned. If the statement throws an error, the error message is returned. Execute a SQL statement and return a string representing the results. If the statement returns rows, a string of the results is returned.If the statement returns no rows, an empty string is returned. Information about all tables in the database. SQL Container builder. SQLContextContainerBuilder. Build a SQLContextContainer that can be passed to the SQL indexduring index construction or during queryt-time. NOTE: if context_str is specified, that will be used as contextinstead of context_dict sql_database (SQLDatabase) \u2013 SQL database context_dict (Optional[Dict[str, str]]) \u2013 context dict Build index structure. Derive index from context. Build context from documents. Query index for context. A simple wrapper around the index.query call whichinjects a query template to specifically fetch table information,and can store a context_str. index (BaseGPTIndex) \u2013 index data structure query_str (Union[str, QueryBundle]) \u2013 query string query_tmpl (Optional[str]) \u2013 query template store_context_str (bool) \u2013 store context_str Common classes for structured operations. Extracts datapoints from a structured document. Extract datapoint from a document and insert it. Builder that builds context for a given set of SQL tables. sql_database (Optional[SQLDatabase]) \u2013 SQL database to use, llm_predictor (Optional[LLMPredictor]) \u2013 LLM Predictor to use. prompt_helper (Optional[PromptHelper]) \u2013 Prompt Helper to use. text_splitter (Optional[TextSplitter]) \u2013 Text Splitter to use. table_context_prompt (Optional[TableContextPrompt]) \u2013 ATable Context Prompt (see Prompt Templates). refine_table_context_prompt (Optional[RefineTableContextPrompt]) \u2013 A Refine Table Context Prompt (see Prompt Templates). table_context_task (Optional[str]) \u2013 The query to performon the table context. A default query string is usedif none is provided by the user. Build context for all tables in the database. Build context from documents for a single table. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Response schema. Response object. Returned if streaming=False during the index.query() call. The response text. Optional[str] Get formatted sources text. Source node. User-facing class containing the source text and the corresponding document id. Create a SourceNode from a Node. Create a list of SourceNodes from a list of Nodes. StreamingResponse object. Returned if streaming=True during the index.query() call. The response generator. Optional[Generator] Get formatted sources text. Get a standard response object. Print the response stream. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.      ", "url": "https://gpt-index.readthedocs.io/en/latest/reference/response.html"}