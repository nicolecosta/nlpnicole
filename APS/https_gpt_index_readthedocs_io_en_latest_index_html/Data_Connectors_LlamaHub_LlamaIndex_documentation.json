{"title": "Data Connectors (LlamaHub \ud83e\udd99) \u2014 LlamaIndex  documentation", "content": "Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM\u2019s with external data. Github: https://github.com/jerryjliu/llama_index LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM\u2019s reasoning capabilities to generate a response. To perform LLM\u2019s data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing That\u2019s where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API\u2019s, PDF\u2019s, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM\u2019s. These indices help to abstract away common boilerplate and pain points for in-context learning: Storing context in an easy-to-access format for prompt insertion. Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big. Dealing with text splitting. Provides users an interface to query the index (feed in an input prompt) and obtain a knowledge-augmented output. Offers you a comprehensive toolset trading off cost and performance. Getting Started Guides Use Cases Key Components Reference Gallery \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery You can simply do: Git clone this repository: git clone git@github.com:jerryjliu/gpt_index.git. Then do: pip install -e . if you want to do an editable install (you can modify source files) of just the package itself. pip install -r requirements.txt if you want to install optional dependencies + dependencies used for development (e.g. unit testing). By default, we use the OpenAI GPT-3 text-davinci-003 model. In order to use this, you must have an OPENAI_API_KEY setup.You can register an API key by logging into OpenAI\u2019s page and creating a new API token. You can customize the underlying LLM in the Custom LLMs How-To (courtesy of Langchain). You mayneed additional environment keys + tokens setup depending on the LLM provider. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Here is a starter example for using LlamaIndex. Make sure you\u2019ve followed the installation steps first. LlamaIndex examples can be found in the examples folder of the LlamaIndex repository.We first want to download this examples folder. An easy way to do this is to just clone the repo: Next, navigate to your newly-cloned repository, and verify the contents: We now want to navigate to the following folder: This contains LlamaIndex examples around Paul Graham\u2019s essay, \u201cWhat I Worked On\u201d. A comprehensive set of examples are already provided in TestEssay.ipynb. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running. Create a new .py file with the following: This builds an index over the documents in the data folder (which in this case just consists of the essay text). We then run the following You should get back a response similar to the following: The author wrote short stories and tried to program on an IBM 1401. In a Jupyter notebook, you can view info and/or debugging logging using the following snippet: You can set the level to DEBUG for verbose output, or use level=logging.INFO for less. To save to disk and load from disk, do That\u2019s it! For more information on LlamaIndex features, please check out the numerous \u201cGuides\u201d to the left.If you are interested in further exploring how LlamaIndex works, check out our Primer Guide. Additionally, if you would like to play around with Example Notebooks, check out this link. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At its core, LlamaIndex contains a toolkit designed to easily connect LLM\u2019s with your external data.LlamaIndex helps to provide the following: A set of data structures that allow you to index your data for various LLM tasks, and remove concerns over prompt size limitations. Data connectors to your common data sources (Google Docs, Slack, etc.). Cost transparency + tools that reduce cost while increasing performance. Each data structure offers distinct use cases and a variety of customizable parameters. These indices can then bequeried in a general purpose manner, in order to achieve any task that you would typically achieve with an LLM: Question-Answering Summarization Text Generation (Stories, TODO\u2019s, emails, etc.) and more! The guides below are intended to help you get the most out of LlamaIndex. It gives a high-level overview of the following: The general usage pattern of LlamaIndex. Mapping Use Cases to LlamaIndex data Structures How Each Index Works General Guides \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery This section contains a list of in-depth tutorials on how to best utilize different capabilitiesof LlamaIndex within your end-user application. They include a broad range of LlamaIndex concepts: Semantic search Structured data support Composability/Query Transformation They also showcase a variety of application settings that LlamaIndex can be used, from a simpleJupyter notebook to a chatbot to a full-stack web application. Tutorials \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery We offer a wide variety of example notebooks. They are referenced throughout the documentation. Example notebooks are found here. \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery At a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,whether it\u2019s question-answering, summarization, or a component in a chatbot. This section describes the different ways you can query your data with LlamaIndex, roughly in orderof simplest (top-k semantic search), to more advanced capabilities. The most basic example usage of LlamaIndex is through semantic search. We providea simple in-memory vector store for you to get started, but you can also chooseto use any one of our vector store integrations: Relevant Resources: Quickstart Example notebook A summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.For instance, a summarization query could look like one of the following: \u201cWhat is a summary of this collection of text?\u201d \u201cGive me a summary of person X\u2019s experience with the company.\u201d In general, a list index would be suited for this use case. A list index by default goes through all the data. Empirically, setting response_mode=\"tree_summarize\" also leads to better summarization results. LlamaIndex supports queries over structured data, whether that\u2019s a Pandas DataFrame or a SQL Database. Here are some relevant resources: Guide on Text-to-SQL SQL Demo Notebook 1 SQL Demo Notebook 2 (Context) SQL Demo Notebook 3 (Big tables) Pandas Demo Notebook. LlamaIndex supports synthesizing across heterogenous data sources. This can be done by composing a graph over your existing data.Specifically, compose a list index over your subindices. A list index inherently combines information for each node; thereforeit can synthesize information across your heteregenous data sources. Here are some relevant resources: Composability City Analysis Demo. LlamaIndex also supports routing over heteregenous data sources - for instance, if you want to \u201croute\u201d a query to anunderlying Document or a subindex.Here you have three options: GPTTreeIndex, GPTKeywordTableIndex, or aVector Store Index. A GPTTreeIndex uses the LLM to select the child node(s) to send the query down to.A GPTKeywordTableIndex uses keyword matching, and a GPTVectorStoreIndex usesembedding cosine similarity. Here are some relevant resources: Composability Composable Keyword Table Graph. LlamaIndex can support compare/contrast queries as well. It can do this in the following fashion: Composing a graph over your data Adding in query transformations. You can perform compare/contrast queries by just composing a graph over your data. Here are some relevant resources: Composability SEC 10-k Analysis Example notebook. You can also perform compare/contrast queries with a query transformation module. This module will help break down a complex query into a simpler one over your existing index structure. Here are some relevant resources: Query Transformations City Analysis Example Notebook LlamaIndex can also support multi-step queries. Given a complex query, break it down into subquestions. For instance, given a question \u201cWho was in the first batch of the accelerator program the author started?\u201d,the module will first decompose the query into a simpler initial question \u201cWhat was the accelerator program the author started?\u201d,query the index, and then ask followup questions. Here are some relevant resources: Query Transformations Multi-Step Query Decomposition Notebook \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery LlamaIndex modules provide plug and play data loaders, data structures, and query interfaces. They can be used in your downstream LLM Application. Some of these applications are described below. Chatbots are an incredibly popular use case for LLM\u2019s. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents. Relevant Resources: Building a Chatbot Using with a LangChain Agent LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit. We provide tutorials and resources to help you get started in this area. Relevant Resources: Fullstack Application Guide LlamaIndex Starter Pack \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.       Getting Started Guides Use Cases Key Components Reference Gallery Our data connectors are offered through LlamaHub \ud83e\udd99.LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  Some sample data connectors: local file directory (SimpleDirectoryReader). Can support parsing a wide range of file types: .pdf, .jpg, .png, .docx, etc. Notion (NotionPageReader) Google Docs (GoogleDocsReader) Slack (SlackReader) Discord (DiscordReader) Each data loader contains a \u201cUsage\u201d section showing how that loader can be used. At the core of using each loader is a download_loader function, whichdownloads the loader file into a module that you can use within your application. Example usage: \u00a9 Copyright 2022, Jerry Liu.      Revision 66db86d2.      ", "url": "https://gpt-index.readthedocs.io/en/latest/how_to/data_connectors.html"}